{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicko\\anaconda3\\envs\\futures\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from manager import ResearchManager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Superpixel Based Graph Laplacian Regularization for Sparse Hyperspectral Unmixing to ./papers/2007.14033v2.Superpixel_Based_Graph_Laplacian_Regularization_for_Sparse_Hyperspectral_Unmixing.pdf ....\n",
      "Downloading Image Segmentation using Sparse Subset Selection to ./papers/1804.02721v1.Image_Segmentation_using_Sparse_Subset_Selection.pdf ....\n",
      "Downloading Hierarchical Homogeneity-Based Superpixel Segmentation: Application to Hyperspectral Image Analysis to ./papers/2407.15321v1.Hierarchical_Homogeneity_Based_Superpixel_Segmentation__Application_to_Hyperspectral_Image_Analysis.pdf ....\n",
      "Downloading A Dynamically Weighted Loss Function for Unsupervised Image Segmentation to ./papers/2403.11266v1.A_Dynamically_Weighted_Loss_Function_for_Unsupervised_Image_Segmentation.pdf ....\n",
      "Downloading Contrastive Registration for Unsupervised Medical Image Segmentation to ./papers/2011.08894v3.Contrastive_Registration_for_Unsupervised_Medical_Image_Segmentation.pdf ....\n",
      "Downloading Unsupervised Contrastive Learning with Simple Transformation for 3D Point Cloud Data to ./papers/2110.06632v2.Unsupervised_Contrastive_Learning_with_Simple_Transformation_for_3D_Point_Cloud_Data.pdf ....\n",
      "Downloading MGTUNet: An new UNet for colon nuclei instance segmentation and quantification to ./papers/2210.10981v2.MGTUNet__An_new_UNet_for_colon_nuclei_instance_segmentation_and_quantification.pdf ....\n",
      "Downloading HoVer-UNet: Accelerating HoVerNet with UNet-based multi-class nuclei segmentation via knowledge distillation to ./papers/2311.12553v3.HoVer_UNet__Accelerating_HoVerNet_with_UNet_based_multi_class_nuclei_segmentation_via_knowledge_distillation.pdf ....\n",
      "Downloading UNet#: A UNet-like Redesigning Skip Connections for Medical Image Segmentation to ./papers/2205.11759v1.UNet___A_UNet_like_Redesigning_Skip_Connections_for_Medical_Image_Segmentation.pdf ....\n",
      "Downloading Weakly-Supervised 3D Medical Image Segmentation using Geometric Prior and Contrastive Similarity to ./papers/2302.02125v1.Weakly_Supervised_3D_Medical_Image_Segmentation_using_Geometric_Prior_and_Contrastive_Similarity.pdf ....\n",
      "Downloading Adaptive Superpixel for Active Learning in Semantic Segmentation to ./papers/2303.16817v2.Adaptive_Superpixel_for_Active_Learning_in_Semantic_Segmentation.pdf ....\n",
      "Downloading Fully and Weakly Supervised Referring Expression Segmentation with End-to-End Learning to ./papers/2212.10278v1.Fully_and_Weakly_Supervised_Referring_Expression_Segmentation_with_End_to_End_Learning.pdf ....\n",
      "Downloading LRNNet: A Light-Weighted Network with Efficient Reduced Non-Local Operation for Real-Time Semantic Segmentation to ./papers/2006.02706v1.LRNNet__A_Light_Weighted_Network_with_Efficient_Reduced_Non_Local_Operation_for_Real_Time_Semantic_Segmentation.pdf ....\n",
      "Downloading Realtime Global Attention Network for Semantic Segmentation to ./papers/2112.12939v1.Realtime_Global_Attention_Network_for_Semantic_Segmentation.pdf ....\n",
      "Downloading BiSeNet V2: Bilateral Network with Guided Aggregation for Real-time Semantic Segmentation to ./papers/2004.02147v1.BiSeNet_V2__Bilateral_Network_with_Guided_Aggregation_for_Real_time_Semantic_Segmentation.pdf ....\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llm_name = \"CLAUDE\"\n",
    "\n",
    "research_assistant = ResearchManager(llm_name)\n",
    "\n",
    "research_topics = research_assistant.analyze_research(\"What are some new and novel ways to segment images that don't use tranformer architecture?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResearchTopics(topic='Graph based image segmentation using superpixel clustering and spectral methods', priority=3, query='superpixel spectral segmentation', timestamp='2024-11-23 18:21:07', research_papers=[ResearchPaper(title='Superpixel Based Graph Laplacian Regularization for Sparse Hyperspectral Unmixing', authors=[arxiv.Result.Author('Taner Ince')], abstract='An efficient spatial regularization method using superpixel segmentation and\\ngraph Laplacian regularization is proposed for sparse hyperspectral unmixing\\nmethod. Since it is likely to find spectrally similar pixels in a homogeneous\\nregion, we use a superpixel segmentation algorithm to extract the homogeneous\\nregions by considering the image boundaries. We first extract the homogeneous\\nregions, which are called superpixels, then a weighted graph in each superpixel\\nis constructed by selecting $K$-nearest pixels in each superpixel. Each node in\\nthe graph represents the spectrum of a pixel and edges connect the similar\\npixels inside the superpixel. The spatial similarity is investigated using\\ngraph Laplacian regularization. Sparsity regularization for abundance matrix is\\nprovided using a weighted sparsity promoting norm. Experimental results on\\nsimulated and real data sets show the superiority of the proposed algorithm\\nover the well-known algorithms in the literature.', url='http://arxiv.org/abs/2007.14033v2', pdf_path='./papers/2007.14033v2.Superpixel_Based_Graph_Laplacian_Regularization_for_Sparse_Hyperspectral_Unmixing.pdf', content='arXiv:2007.14033v2  [cs.CV]  12 Sep 2020\\nIEEE GEOSCIENCE AND REMOTE SENSING LETTERS, VOL. XX, NO. X, JULY 2020\\n1\\nSuperpixel Based Graph Laplacian Regularization\\nfor Sparse Hyperspectral Unmixing\\nTaner Ince, Member, IEEE\\nAbstract—An efﬁcient spatial regularization method using\\nsuperpixel segmentation and graph Laplacian regularization is\\nproposed for sparse hyperspectral unmixing method. Since it is\\nlikely to ﬁnd spectrally similar pixels in a homogeneous region,\\nwe use a superpixel segmentation algorithm to extract the homo-\\ngeneous regions by considering the image boundaries. We ﬁrst\\nextract the homogeneous regions, which are called superpixels,\\nthen a weighted graph in each superpixel is constructed by\\nselecting K-nearest pixels in each superpixel. Each node in the\\ngraph represents the spectrum of a pixel and edges connect\\nthe similar pixels inside the superpixel. The spatial similarity\\nis investigated using graph Laplacian regularization. Sparsity\\nregularization for abundance matrix is provided using a weighted\\nsparsity promoting norm. Experimental results on simulated and\\nreal data sets show the superiority of the proposed algorithm over\\nthe well-known algorithms in the literature.\\nIndex Terms—Sparse unmixing, graph Laplacian, abundance\\nestimation, superpixel.\\nI. INTRODUCTION\\nH\\nYPERSPECTRAL imaging used in remote sensing al-\\nlows the identiﬁcation of the substances in the scene\\nby measuring the light spectrum over hundreds of contiguous\\nbands. However, low spatial resolution of hyperspectral sensor\\nand combination of different materials in the homogeneous\\nmixtures cause mixed pixels. Decomposition of a mixed pixel\\ninto spectral signatures (endmembers) with corresponding\\nfractions (abundances) is known as spectral unmixing [1].\\nSpectral unmixing methods mainly use linear mixture model\\n(LMM) in which the observed spectra is a linear combination\\nof the endmembers with corresponding abundances. Linear\\nspectral unmixing (LSU) methods are simple and has tractable\\nsolutions; however, nonlinearity and spectral variability ef-\\nfects the performance of spectral unmixing [2]. In LSU, an\\nendmember extraction step is applied and then abundance\\nvalue for each pixel is estimated. There are many algorithms\\nfor endmember extraction such as N-FINDR [3], pixel purity\\nindex (PPI) [4] and vertex component analysis (VCA) [5].\\nThese algorithms require pure pixel assumption and it is not\\nalways satisﬁed due to the spatial resolution. One way of\\nsolving this problem is to use ground spectral libraries and\\nthen obtaining the abundance value of each pixel using this\\nlarge spectral library. Generally, the number of endmembers in\\nthe scene are small compared to the number of endmembers\\nThe author is with the Department of Electrical and Electronics En-\\ngineering, Gaziantep University, 27310 Gaziantep, Turkey (e-mail: taner-\\nince@gantep.edu.tr\\nThis work has been submitted to the IEEE for possible publication.\\nCopyright may be transferred without notice, after which this version may\\nno longer be accessible.\\nin the spectral library. This means that only small number of\\nendmembers contribute the mixed pixel. Therefore, abundance\\nvector of mixed pixel is expected to be sparse. Estimating\\nthe sparse abundance vector using a priori available spectral\\nlibrary is known as sparse unmixing (SU) [6]. Sparse unmixing\\nby variable splitting and augmented Lagrangian (SUnSAL) [6]\\nsolves an l1 minimization problem satisfying abundance non-\\nnegativity constraint (ANC) and abundance sum constraint\\n(ASC). Collaborative SUnSAL (CLSUnSAL) [7] solves an\\nl2,1 norm optimization problem to promote the row-sparse\\nstructure of the abundance matrix. Local collaborative sparse\\nunmixing (LCSU) [8] estimates the sparse abundance matrix\\nby solving CLSUnSAL in a neighborhood of pixels to obtain\\nmore accurate abundance values. Iterative reweighted sparse\\nunmixing (IRWSU) [9] use a weighting strategy in the formu-\\nlation which has a better abundance estimation compared to\\nCLSUnSAL. Furthermore, nonconvex sparsity based methods\\nare developed for hyperspectral unmixing [10].\\nFurthermore, spatial-contextual information of the abun-\\ndance map is exploited in many works [11]–[14] by consider-\\ning the piecewise smoothness of the abundance map. Total\\nvariation (TV) regularization [15] is used in SUnSAL-TV\\n[11] which minimizes the fractional abundance of neighboring\\npixels. It provides smooth abundance map, however it does\\nnot take into account the local changes in the abundance\\nmap. A spatial discontinuity weight strategy is developed to\\npreserve the details in the abundance map better [12] using\\nthe idea that smooth abundance map condition is not hold\\nin real scenarios. A double reweighted sparse unmixing and\\nTV (DRSU-TV) [13] improves the sparsity of the abundance\\nmatrix by using a double reweighting strategy which is ap-\\nplied in both spectral and spatial domains. Spectral-spatial\\nweighted sparse unmixing (S2WSU) method is presented in\\n[14] which includes a single regularizer with spectral and\\nspatial weighting matrices in the proposed formulation to\\nimprove the abundance estimation.\\nIt is known that similar pixels in a local region are likely\\nto have similar abundances, graph based approaches are de-\\nveloped for hyperspectral unmixing [16]–[18]. A hypergraph-\\nregularized sparse nonnegative matrix factorization (NMF)\\n[16] based unmixing approach employs a hypergraph structure\\nwhere each pixel is taken as a vertex and the pixels in the\\nneighborhood of that pixel form a hypergraph. Therefore,\\nsimilar pixels having similar abundances are found which leads\\nto obtain better unmixing results. In a same manner, graph\\nLaplacian regularization is used in [17] to promote the smooth-\\nness of the abundance map in sparse regression framework.\\nRecently, spatial-contextual information is exploited using\\nhypergraph learning [18] to extract the similarity between the\\nIEEE GEOSCIENCE AND REMOTE SENSING LETTERS, VOL. XX, NO. X, JULY 2020\\n2\\npixels in a small spatial neighborhood. [18] uses K-nearest-\\nneighbors algorithm to ﬁnd the spectrally similar pixels in a\\nlocal neighborhood. However, as the noise level increases it is\\ndifﬁcult to ﬁnd the spectrally similar pixels in a local region.\\nFurthermore, it is likely to ﬁnd similar pixels with different\\nregions of data separated by edges in the image.\\nRecently, superpixel segmentation is investigated in several\\nworks [19]–[23] in hyperspectral imaging. Fang et. al [19] pro-\\npose a hyperspectral classiﬁcation method based on superpixel\\nsegmentation. The pixels in each superpixel are jointly repre-\\nsented by a set of common atoms. [20] employs multiscale\\nsuperpixels to extract the local information for hyperspectral\\nimage (HSI) classiﬁcation. A superpixel weighting strategy is\\nused in [21] to include the spatial correlation. A fast multiscale\\nspatial regularization based on simple linear iterative clustering\\n(SLIC) [24] is proposed in [22]. Superpixel-based reweighted\\nlow-rank and total variation (SUSRLR-TV) [23] minimizes the\\nrank of the abundance matrix in each superpixel and promote\\nthe smoothness of the abundance map using TV.\\nIn this paper, we propose a superpixel based graph Lapla-\\ncian for sparse unmixing (SBGLSU). Superpixel segmentation\\ntakes into account the image boundaries when segmenting the\\nHSI into homogeneous regions. Therefore, we ﬁrst segment\\nHSI into many superpixels using SLIC. Then, a weighted\\ngraph for each superpixel is constructed, where each node rep-\\nresents the neighboring pixels in the superpixel. Although, su-\\nperpixels are homogeneous shape adaptive spatial-neighboring\\npixels, we include a weighted graph regularization to measure\\nthe similarity of the K-nearest pixels in each superpixel. In this\\nmanner, spatial correlation among the K-nearest neighboring\\npixels inside the superpixel is better extracted. The sparsity of\\nthe abundance matrix is satisﬁed using an l1 norm regularizer\\nwith a weighting strategy that promotes the joint-sparsity of\\nthe abundance matrix.\\nThe rest of the paper is organized as follows. Section II\\nexplains the proposed method. The simulated and real data\\nexperiments are given in Section III. Finally, Section IV\\nconcludes the paper.\\nII. SUPERPIXEL BASED GRAPH LAPLACIAN\\nREGULARIZATION FOR SPARSE UNMIXING (SBGLSU)\\nLMM assumes that endmembers are linearly combined to\\nform the measured spectrum of a pixel. It can be modeled as\\nY = AS + N\\nwhere Y = [y1, . . . , yn] ∈RL×n is the L-band spectrum\\nof n pixels where each yi (i = 1, 2, . . ., n) represents the\\nspectrum of ith pixel in the HSI, A ∈RL×m is the mixing\\nmatrix containing m endmembers, S = [s1, . . . , sn] ∈Rm×n\\nfractional abundance matrix where each si (i = 1, 2, . . ., n)\\nrepresents the fractional abundance vector of ith pixel and\\nN ∈RL×n models the error in the measurements. If the\\nnumber of active endmembers is much lower than the number\\nof endmembers in spectral library A, then abundance matrix\\nS is expected to be sparse.\\nFurthermore, a spatial similarity exists between neighbor-\\ning pixels in a HSI which leads to abundance similarity of\\nneighboring pixels. Therefore, we ﬁrst construct a weighted\\ngraph G\\n=\\n(V, E) [25] where V\\n=\\n{v1, . . . , vn} and\\nE = {e1, . . . , en} denote the vertex set and weighted edge\\nset, respectively. A weighted adjacency matrix W ∈Rn×n\\nis constructed where each entry Wij deﬁnes the degree of\\nsimilarity between the spectrum of pixels yi and yj. If yi\\nand yj are similar then a large positive weight is assigned to\\nWij. If they are not similar, a small positive value is assigned.\\nThere are different choice of selecting adjacency matrix to\\nconstruct similarity graphs. We use Gaussian heat kernel which\\nis deﬁned as\\nWij = exp\\n\\x12\\n−∥yi −yj∥2\\n2\\n2σ2\\n\\x13\\n(1)\\nwhere σ controls the width of neighborhood.\\nWhen constructing a similarity graph for hyperspectral data\\nusing weighted adjacency matrix, it is likely to ﬁnd two similar\\npixels in different regions of the HSI. However, local regions\\ntend to have similar pixels leading to similar abundances.\\nTherefore, we extract the homogeneous regions using a seg-\\nmentation algorithm. Generally, K-means algorithm is used to\\nextract the local regions, however K-means search the whole\\nimage to ﬁnd the similar pixels. For this reason, our purpose is\\nto search a local area which have spatially similar regions. We\\nresort the SLIC to segment the HSI into homogeneous regions.\\nSLIC is a variant of K-means clustering but it searches a\\nlimited region and it takes into account the image boundaries.\\nIt is also easy to use, fast and memory efﬁcient and it requires\\nlittle number of parameters.\\nTherefore, we ﬁrst segment the HSI into superpixels and\\nthen construct graph Laplacian for each superpixel. We can\\nexpress the abundance similarity in all superpixels as\\n1\\n2\\nng\\nX\\ng=1\\nX\\n(i,j)∈εg\\nWgij∥si −sj∥2\\n2 =\\nng\\nX\\ng=1\\nTr(SgLgST\\ng )\\n(2)\\nHere, ng denotes the number of superpixels in the image, εg\\nis the neighborhood of each superpixel, Tr(·) denotes the trace\\nof a matrix, Sg is the abundance matrix of the gth superpixel,\\nLg = Dg−Wg is the graph Laplacian matrix of gth superpixel\\nwhere Wg is the adjacency matrix of gth superpixel, Dg is\\na diagonal matrix which is calculated as Dgii = Pn\\nj=1 Wgij\\nwhere Wgij denotes the each entry of Wg .\\nAfter deﬁning the abundance similarity measure in each\\nsuperpixel, SBGLSU is proposed as\\nmin\\nS\\n1\\n2∥Y −AS∥2\\nF + λs∥Ws ⊙S∥1 + λg\\nng\\nX\\ng=1\\nTr(SgLgST\\ng )\\n+ ιR+(S)\\n(3)\\nwhere ∥·∥F and ∥·∥1 denote the Frobenius norm and l1 norm,\\nrespectively. λs and λg are regularization parameters, Ws is\\nthe weight matrix to promote the sparsity of S, ⊙denotes\\nHadamard product. ιR+(S) is indicator function that is equal\\nto zero if s ≥0 and +∞otherwise.\\nWe split the optimization problem into subproblems using\\nalternating direction method of multipliers (ADMM) [26] to\\nIEEE GEOSCIENCE AND REMOTE SENSING LETTERS, VOL. XX, NO. X, JULY 2020\\n3\\nsolve alternately. The optimization problem (5) can be written\\nin a compact form as\\nmin\\nS,V g(V)\\nsubject to\\nGS + BV = 0\\n(4)\\nwhere\\ng(V) = 1\\n2∥Y −V1∥2\\nF + λs∥Ws ⊙V2∥1\\n+ λg\\nng\\nX\\ng=1\\nTr(V3gLgV3\\nT\\ng ) + ιR+(V4)\\n(5)\\nV = (V1, V2, V3, V4), G = [A, I, I, I]T and B = diag[−I].\\nThe augmented lagrangian formulation of (4) is\\nL(V, S, Λ) = g(V) + µ\\n2 ∥GS + BV −Λ∥2\\nF\\n(6)\\nwhere µ > 0 is a penalty parameter and Λ/µ denotes the\\nLagrange multipliers.\\nThe algorithm of SBGLSU is shown in Algorithm 1.\\nSBGLSU includes a weighting strategy to promote the row-\\nsparsity of the abundance matrix. However, ADMM requires\\nthat all functions should be closed, proper and convex to\\nguarantee convergence. Therefore, we use inner and outer\\nloops in Algorithm 1 to make the convergency of the algorithm\\nbetter. In simulation section, the maximum iteration number of\\nouter and inner loops are set to l = 60 and t = 8, respectively.\\nFor the complexity analysis, the most computationally ex-\\npensive parts are step 6 and step 10 of Algorithm 1. In Step\\n6, the term (AT A + 3I)−1 is ﬁxed and can be precomputed\\nto reduce the complexity. Therefore, calculation of S(t+1)\\nhas a computational complexity of O(mnL). Similarly, the\\nterm (2λgLg + µI)−1 in step 10 can be precomputed so\\nthat updating V(t+1)\\n3\\nhas a computational complexity of\\nO(mng|ns|2) where |ns| denotes the number of pixels in\\neach superpixel. Other terms have computational complexity\\nof O(n). Therefore, the overall complexity of SBGLSU is\\nO(mnL) + O(mng|ns|2) + O(n).\\nIII. SIMULATED AND REAL DATA EXPERIMENTS\\nIn this section, we demonstrate the performance of the\\nproposed method by using synthetic hyperspectral data sets.\\nWe perform two synthetic data experiment to demonstrate\\nthe effectiveness of SBGLSU. Signal to reconstruction error\\n(SRE) is used to measure the quality of the unmixing results.\\nIt is deﬁned as SRE = 10 log10(∥S∥2\\nF /∥S −ˆS∥2\\nF ) where\\nS represents the ground truth abundance map and ˆS is the\\nestimated abundance map.\\nA. Simulated Data Sets\\nIn the synthetic data experiments, we create a spectral\\nlibrary A by selecting 240 signatures randomly from dig-\\nital spectral library (splib06) [27] obtained from the U.S.\\nGeological Survey (USGS) which contains the spectra of\\n498 materials measured in 224 spectral bands distributed\\nuniformly in the interval 0.4 and 2.5 µm. We generate two\\nsimulated data sets satisfying ASC and ANC. Simulated data\\ncube 1 (DC1) is created by selecting ﬁve spectral signatures\\nAlgorithm 1 Pseudocode of the proposed SBGLSU\\nInput: Y, A, λs, λg, µ > 0, ǫ, SLIC parameters\\nInitialization: l = 0, t = 0, S(0), V(0)\\n1 , V(0)\\n2 , V(0)\\n3 , V(0)\\n4\\nΛ(0)\\n1 , Λ(0)\\n2 , Λ(0)\\n3 , Λ(0)\\n4\\n1: for g = 1 to ng\\n2:\\nLg = Dg −Wg\\n3: end for\\n4: repeat\\nWs(:, i)(l) =\\n\\x14\\n1\\n∥(S(l) −Λ(l)\\n2 )(1, :)∥2 + ǫ\\n; ...\\n;\\n1\\n∥(S(l) −Λ(l)\\n2 )(m, :)∥2 + ǫ\\n\\x15\\ni = 1, 2, . . ., n\\n5: repeat\\n6: S(t+1) = (AT A + 3I)\\n−1h\\nAT (V(t)\\n1\\n+ Λ(t)\\n1 )\\n+(V(t)\\n2\\n+ Λ(t)\\n2 ) + (V(t)\\n3\\n+ Λ(t)\\n3 ) + (V(t)\\n4\\n+ Λ(t)\\n4 )\\ni\\n7: V(t+1)\\n1\\n=\\n1\\n1+µ(Y + µ(AS(t+1) −Λ(t)\\n1 ))\\n8: V(t+1)\\n2\\n= soft(S(t+1) −Λ(t)\\n2 , (λs/µ)Ws\\n(l))\\n9: for g = 1 to ng\\n10:\\nV(t+1)\\n3g\\n= µ(S(t+1)\\ng\\n−Λ(t)\\n3g )(2λgLg + µI)−1\\n11: end for\\n12: V(t+1)\\n4\\n= max(S(t+1) −Λ(t)\\n4 , 0)\\n13: Λ(t+1)\\n1\\n= Λ(t)\\n1 −AS(t+1) + V(t+1)\\n1\\n14: Λ(t+1)\\n2\\n= Λ(t)\\n2 −S(t+1) + V(t+1)\\n2\\n15: Λ(t+1)\\n3\\n= Λ(t)\\n3 −S(t+1) + V(t+1)\\n3\\n16: Λ(t+1)\\n4\\n= Λ(t)\\n4 −S(t+1) + V(t+1)\\n4\\n17: Update iteration: t ←t + 1\\n18: S(l+1) ←S(t+1)\\n19: Λ(l+1)\\n2\\n←Λ(t+1)\\n2\\n20: Update iteration: l ←l + 1\\n21: until some stopping criteria is satisﬁed.\\nrandomly from library A as active endmembers and using\\nthe corresponding fractional abundance maps having size of\\n75 × 75. For simulated data cube 2 (DC2), we select nine\\nspectral signatures randomly from A as active endmembers\\nand using the corresponding fractional abundance maps having\\nsize of 100 × 100. Simulated data sets DC1 and DC2 is\\nthen contaminated with Gaussian noise of signal-to-noise ratio\\n(SNR) with SNR=20, 30 and 40 dB, respectively.\\nB. Comparison to Other Unmixing Methods\\nWe compare the unmixing results of SBGLSU with\\nSUnSAL-TV [11], S2WSU [14], MUASLIC [22] and SUSRLR-\\nTV [23]. Optimal regularization parameters of all algorithms\\nare found by varying the regularization parameters in a suitable\\nrange. The superpixel size and regularization parameter of\\nSLIC algorithm for SBGLSU are set to 8 and 2e-3 for DC1 and\\nDC2, respectively. We run all algorithms under comparison\\nand report the SRE values in Table I along with optimal\\nregularization parameters obtained by different algorithms for\\nDC1 and DC2 for SNR values 20, 30 and 40 dB. We can see\\nclearly that the SBGLSU performs best in all noise levels for\\nIEEE GEOSCIENCE AND REMOTE SENSING LETTERS, VOL. XX, NO. X, JULY 2020\\n4\\nDC1 and DC2. SUnSAL-TV has lowest SRE values in all SNR\\nvalues so it is not reported in Table I. Furthermore, we compare\\nTABLE I\\nSRE VALUES OF DIFFERENT ALGORITHMS\\nDC1\\nSNR\\nS2WSU\\nMUASLIC\\nSUSRLR-TV\\nSBGLSU\\n20\\n7.68\\nλ = 1e −1\\n11.34\\nλ1 = 3e −2\\nλ2 = 1e −1\\n14.38\\nρ = 1e −1\\nλT V = 5e −2\\n19.99\\nλs = 5e −2\\nλg = 1e3\\n30\\n15.48\\nλ = 5e −3\\n15.73\\nλ1 = 7e −3\\nλ2 = 5e −2\\n25.29\\nρ = 5e −2\\nλT V = 1e −2\\n34.49\\nλs = 1e −2\\nλg = 1e3\\n40\\n28.23\\nλ = 1e −3\\n22.34\\nλ1 = 1e −3\\nλ2 = 1e −2\\n38.72\\nρ = 1e −2\\nλT V = 5e −4\\n45.33\\nλs = 5e −3\\nλg = 1e3\\nDC2\\nSNR\\nS2WSU\\nMUASLIC\\nSUSRLR-TV\\nSBGLSU\\n20\\n9.33\\nλ = 1e −1\\n14.75\\nλ1 = 3e −2\\nλ2 = 1e −1\\n16.08\\nρ = 1e −1\\nλT V = 5e −2\\n18.13\\nλs = 2e −2\\nλg = 1e3\\n30\\n21.66\\nλ = 5e −3\\n18.33\\nλ1 = 7e −3\\nλ2 = 5e −2\\n22.25\\nρ = 5e −2\\nλT V = 1e −2\\n23.51\\nλs = 7e −2\\nλg = 5e −2\\n40\\n27.79\\nλ = 1e −3\\n20.92\\nλ1 = 1e −3\\nλ2 = 5e −3\\n25.97\\nρ = 5e −3\\nλT V = 1e −3\\n29.52\\nλs = 2e −2\\nλg = 7e −3\\nthe unmixing results visually for individual endmember. Fig.\\n1 shows the estimated abundance map obtained by different\\nunmixing algorithms for endmember #5 in DC1 with SNR =\\n20 dB. It can be seen clearly that SBGLSU is able to recover\\nthe details much better than the other algorithms. Similar\\nconclusions can be made for DC2. Fig. 2 shows the estimated\\nabundance map obtained by different unmixing algorithms for\\nendmember #1 in DC2 with SNR = 20 dB.\\n \\n \\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nReference\\n \\n \\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nSUnSAL-TV\\n \\n \\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nS2WSU\\n \\n \\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nMUASLIC\\n \\n \\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nSUSRLR-TV\\n \\n \\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nSBGLSU\\nFig. 1. Estimated abundance maps for endmember #5 in DC1 with SNR =\\n20 dB.\\nFor convergency analysis, we record the root mean square\\nerror (RMSE) at each outer iteration of SBGLSU. RMSE is\\ndeﬁned as RMSE =\\nq\\n1\\nmn\\nPn\\ni=1 ∥si −ˆsi∥2 where si and ˆsi\\nare the actual and estimated abundance vectors, respectively.\\nFig. 3 shows the convergency curve for SBGLSU for DC1\\nand DC2 at all noise levels. We can see that after 60 outer\\niterations, SBGLSU is able to obtain stable solution.\\n \\n \\n20 \\n40 \\n60 \\n80 \\n100\\n20\\n40\\n60\\n80\\n100\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nReference\\n \\n \\n20 \\n40 \\n60 \\n80 \\n100\\n20\\n40\\n60\\n80\\n100\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nSUnSAL-TV\\n \\n \\n20 \\n40 \\n60 \\n80 \\n100\\n20\\n40\\n60\\n80\\n100\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nS2WSU\\n \\n \\n20 \\n40 \\n60 \\n80 \\n100\\n20\\n40\\n60\\n80\\n100\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nMUASLIC\\n \\n \\n20 \\n40 \\n60 \\n80 \\n100\\n20\\n40\\n60\\n80\\n100\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nSUSRLR-TV\\n \\n \\n20 \\n40 \\n60 \\n80 \\n100\\n20\\n40\\n60\\n80\\n100\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nSBGLSU\\nFig. 2. Estimated abundance maps for endmember #1 in DC2 with SNR =\\n20 dB.\\n10\\n20\\n30\\n40\\n50\\n60\\n0    \\n0.005\\n0.01 \\n0.015\\n0.02 \\n0.025\\nIterations\\nRMSE\\n \\n \\nSNR=20\\nSNR=30\\nSNR=40\\n(a) DC1\\n10\\n20\\n30\\n40\\n50\\n60\\n0   \\n0.01\\n0.02\\n0.03\\n0.04\\n0.05\\nIterations\\nRMSE\\n \\n \\nSNR=20\\nSNR=30\\nSNR=40\\n(b) DC2\\nFig. 3. Convergence curves of SBGLSU.\\nC. Real Data Experiment\\nIn this section, we present the performance of the proposed\\nmethod on the real data set. The real data set used in the\\nexperiment is Cuprite dataset1 which contains 14 kinds of\\nminerals [5]. This data set is used frequently to compare\\nthe performance of the unmixing algorithms. It contains 224\\nbands with a wavelength range of 0.4-2.5 µm. However,\\nsome bands of the Cuprite data set have low-SNR and water\\nabsorption. Therefore, we removed the bands 1-2, 105-115,\\n150-170, and 223-224 prior to analysis. The spatial size of\\nthe data used in the experiment is 250 × 191. We use the\\nspectral library of 498 minerals from the USGS library. Since\\nwe do not have exact abundance maps for Cuprite data, we\\nuse Tetracorder 4.4 [28] classiﬁcation algorithm in order to\\ncompare the unmixing results qualitatively. We compare the\\nunmixing results of SBGLSU with SUnSAL-TV, S2WSU,\\nMUASLIC, SUSRLR-TV for Chalcedony mineral in the Cuprite\\ndata. The regularization parameters of the algorithms under\\ncomparison are set to as: λ = 10−3 and λT V = 10−3 for\\nSUnSAL-TV, λ = 7 × 10−1 for S2WSU, λ1 = 10−3 and\\nλ2 = 10−3 for MUASLIC, ρ = 10−3 and λT V = 10−3 for\\nSUSRLR-TV and λs = 10−3 and λg = 10−3 for SBGLSU.\\nFig. 4 shows a qualitative comparison among the clas-\\nsiﬁcation maps obtained by Tetracorder 4.4 [28] algorithm\\nand abundance maps obtained by SBGLSU, SUnSAL-TV,\\nS2WSU, MUASLIC and SUSRLR-TV. It can be concluded that\\n1http://aviris.jpl.nasa.gov/html/aviris.freedata.html\\nIEEE GEOSCIENCE AND REMOTE SENSING LETTERS, VOL. XX, NO. X, JULY 2020\\n5\\nSBGLSU is a valid unmixing algorithm for real hyperspectral\\ndata.\\nThe computation times of all algorithms under comparison\\nare reported in Table II. MUASLIC is the fastest unmixing algo-\\nrithm under comparison. SBGLSU computation time is com-\\nparably much better than other algorithms except MUASLIC.\\n20\\n40\\n60\\n80\\n100\\n120\\n140\\n20\\n40\\n60\\n80\\n100\\n120\\n140\\n160\\n180\\nTetracorder 4.4\\n \\n \\n20 40 60 80 100 120 140 160 180\\n50\\n100\\n150\\n200\\n250\\n0\\n0.05\\n0.1\\n0.15\\n0.2\\n0.25\\n0.3\\n0.35\\n0.4\\nSUnSAL-TV\\n \\n \\n20 40 60 80 100 120 140 160 180\\n50\\n100\\n150\\n200\\n250\\n0\\n0.05\\n0.1\\n0.15\\n0.2\\n0.25\\n0.3\\n0.35\\n0.4\\nS2WSU\\n \\n \\n20 40 60 80 100 120 140 160 180\\n50\\n100\\n150\\n200\\n250\\n0\\n0.05\\n0.1\\n0.15\\n0.2\\n0.25\\n0.3\\n0.35\\n0.4\\nMUASLIC\\n \\n \\n20 40 60 80 100 120 140 160 180\\n50\\n100\\n150\\n200\\n250\\n0\\n0.05\\n0.1\\n0.15\\n0.2\\n0.25\\n0.3\\n0.35\\n0.4\\nSUSRLR-TV\\n \\n \\n20 40 60 80 100 120 140 160 180\\n50\\n100\\n150\\n200\\n250\\n0\\n0.05\\n0.1\\n0.15\\n0.2\\n0.25\\n0.3\\n0.35\\n0.4\\nSBGLSU\\nFig. 4.\\nAbundance maps obtained by different algorithms corresponding\\nChalcedony.\\nTABLE II\\nCOMPUTATION TIMES OF DIFFERENT ALGORITHMS ON REAL\\nDATA (IN MINUTES).\\nSUnSAL-TV\\nS2WSU\\nMUASLIC\\nSUSRLR-TV\\nSBGLSU\\n28.27\\n19.36\\n2.44\\n50.68\\n13.95\\nIV. CONCLUSION\\nIn this paper, we have developed a novel graph Laplacian\\nregularized sparse hyperspectral unmixing method based on\\nsuperpixel segmentation. Superpixel segmentation extracts the\\nspatially homogeneous regions and graph Laplacian regular-\\nization minimizes the abundance similarity of each superpixel.\\nA sparsity inducing norm with a weighting strategy is included\\nin the formulation to promote the sparsity of the abundance\\nmatrix better. The proposed method is solved using a variable\\nsplitting approach which includes inner and outer loops to\\nconverge better. Experimental results on both simulated and\\nreal data sets have shown that the proposed method is a very\\neffective sparse unmixing method compared to other state-of-\\nthe-art sparse unmixing methods in the literature.\\nREFERENCES\\n[1] N. Keshava and J. F. Mustard, “Spectral unmixing,” IEEE Signal\\nProcess. Mag., vol. 19, no. 1, pp. 44–57, Jan 2002.\\n[2] D. Hong, N. Yokoya, J. Chanussot, and X. X. Zhu, “An augmented linear\\nmixing model to address spectral variability for hyperspectral unmixing,”\\nIEEE Trans. Image Process., vol. 28, no. 4, pp. 1923–1938, 2019.\\n[3] M. E. Winter, “N-ﬁndr: an algorithm for fast autonomous spectral end-\\nmember determination in hyperspectral data,” vol. 3753, 1999, pp. 266–\\n275.\\n[4] J. W. Boardman, F. A. Kruse, and R. O. Green, “Mapping target\\nsignatures via partial unmixing of AVIRIS data,” in Fifth JPL Airborne\\nEarth Science Workshop, vol. 95.\\nJPL Publication, 1995, pp. 23–26.\\n[5] J. M. P. Nascimento and J. M. B. Dias, “Vertex component analysis: a\\nfast algorithm to unmix hyperspectral data,” IEEE Trans. Geosci. Remote\\nSens., vol. 43, no. 4, pp. 898–910, April 2005.\\n[6] J. M. Bioucas-Dias and M. A. T. Figueiredo, “Alternating direction al-\\ngorithms for constrained sparse regression: Application to hyperspectral\\nunmixing,” in Proc. 2nd Workshop Hyperspectral Image Signal Process.,\\nEvol. Remote Sens. (WHISPERS), June 2010, pp. 1–4.\\n[7] M. D. Iordache, J. M. Bioucas-Dias, and A. Plaza, “Collaborative sparse\\nregression for hyperspectral unmixing,” IEEE Trans. Geosci. Remote\\nSens., vol. 52, no. 1, pp. 341–354, Jan 2014.\\n[8] S. Zhang, J. Li, K. Liu, C. Deng, L. Liu, and A. Plaza, “Hyperspectral\\nunmixing based on local collaborative sparse regression,” IEEE Geosci.\\nRemote Sens. Lett., vol. 13, no. 5, pp. 631–635, May 2016.\\n[9] C. Y. Zheng, H. Li, Q. Wang, and C. L. Philip Chen, “Reweighted sparse\\nregression for hyperspectral unmixing,” IEEE Trans. Geosci. Remote\\nSens., vol. 54, no. 1, pp. 479–488, Jan 2016.\\n[10] J. Yao, D. Meng, Q. Zhao, W. Cao, and Z. Xu, “Nonconvex-sparsity and\\nnonlocal-smoothness-based blind hyperspectral unmixing,” IEEE Trans.\\nImage Process., vol. 28, no. 6, pp. 2991–3006, 2019.\\n[11] M. D. Iordache, J. M. Bioucas-Dias, and A. Plaza, “Total variation\\nspatial regularization for sparse hyperspectral unmixing,” IEEE Trans.\\nGeosci. Remote Sens., vol. 50, no. 11, pp. 4484–4502, Nov 2012.\\n[12] S. Zhang, J. Li, Z. Wu, and A. Plaza, “Spatial discontinuity-weighted\\nsparse unmixing of hyperspectral images,” IEEE Trans. Geosci. Remote\\nSens., vol. 56, no. 10, pp. 5767–5779, Oct 2018.\\n[13] R. Wang, H. Li, A. Pizurica, J. Li, A. Plaza, and W. J. Emery,\\n“Hyperspectral unmixing using double reweighted sparse regression and\\ntotal variation,” IEEE Geosci. Remote Sens. Lett., vol. 14, no. 7, pp.\\n1146–1150, July 2017.\\n[14] S. Zhang, J. Li, H. Li, C. Deng, and A. Plaza, “Spectral-spatial\\nweighted sparse regression for hyperspectral image unmixing,” IEEE\\nTrans. Geosci. Remote Sens., vol. 56, no. 6, pp. 3265–3276, June 2018.\\n[15] L. I. Rudin, S. Osher, and E. Fatemi, “Nonlinear total variation based\\nnoise removal algorithms,” Phys. D, vol. 60, no. 1-4, pp. 259–268, Nov.\\n1992.\\n[16] W. Wang, Y. Qian, and Y. Y. Tang, “Hypergraph-regularized sparse nmf\\nfor hyperspectral unmixing,” IEEE J. Sel. Topics Appl. Earth Observ.\\nRemote Sens., vol. 9, no. 2, pp. 681–694, 2016.\\n[17] R. Ammanouil, A. Ferrari, and C. Richard, “A graph laplacian regu-\\nlarization for hyperspectral data unmixing,” in Proc. IEEE Int. Conf.\\nAcoust., Speech Signal Process. (ICASSP), Apr. 2015, pp. 1637–1641.\\n[18] P. Jia, M. Zhang, and Y. Shen, “Hypergraph learning and reweighted\\nℓ1-norm minimization for hyperspectral unmixing,” IEEE J. Sel. Topics\\nAppl. Earth Observ. Remote Sens., vol. 12, no. 6, pp. 1898–1904, 2019.\\n[19] L. Fang, S. Li, X. Kang, and J. A. Benediktsson, “Spectral-spatial clas-\\nsiﬁcation of hyperspectral images with a superpixel-based discriminative\\nsparse model,” IEEE Trans. Geosci. Remote Sens., vol. 53, no. 8, pp.\\n4186–4201, Aug 2015.\\n[20] T. Dundar and T. Ince, “Sparse representation-based hyperspectral image\\nclassiﬁcation using multiscale superpixels and guided ﬁlter,” IEEE\\nGeosci. Remote Sens. Lett., vol. 16, no. 2, pp. 246–250, Feb 2019.\\n[21] S. Zhang, C. Deng, J. Li, S. Wang, F. Li, C. Xu, and A. Plaza,\\n“Superpixel-guided sparse unmixing for remotely sensed hyperspectral\\nimagery,” in Proc. IEEE Int. Geosci. Remote Sens. Symp., July 2019,\\npp. 2155–2158.\\n[22] R. A. Borsoi, T. Imbiriba, J. C. M. Bermudez, and C. Richard, “A\\nfast multiscale spatial regularization for sparse hyperspectral unmixing,”\\nIEEE Geosci. Remote Sens. Lett., vol. 16, no. 4, pp. 598–602, 2019.\\n[23] H. Li, R. Feng, L. Wang, Y. Zhong, and L. Zhang, “Superpixel-\\nbased reweighted low-rank and total variation sparse unmixing for\\nhyperspectral remote sensing imagery,” IEEE Trans. Geosci. Remote\\nSens., pp. 1–19, 2020.\\n[24] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. S¨usstrunk,\\n“Slic superpixels compared to state-of-the-art superpixel methods,” IEEE\\nTrans. Pattern Anal. Mach. Intell., vol. 34, no. 11, pp. 2274–2282, 2012.\\n[25] U. Von Luxburg, “A tutorial on spectral clustering,” Statistics and\\ncomputing, vol. 17, no. 4, pp. 395–416, 2007.\\n[26] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, “Distributed\\noptimization and statistical learning via the alternating direction method\\nof multipliers,” Found. Trends Mach. Learn., vol. 3, no. 1, pp. 1–122,\\n2011.\\n[27] R. N. Clark et al., USGS digital spectral library splib06a.\\nU.S.\\nGeological Survey Denver, CO, 2007.\\n[28] ——, “Imaging spectroscopy: Earth and planetary remote sensing with\\nthe USGS Tetracorder and expert systems,” J. Geophys. Res., vol. 108,\\nno. E12, p. 5131, Dec. 2003.\\n'), ResearchPaper(title='Image Segmentation using Sparse Subset Selection', authors=[arxiv.Result.Author('Fariba Zohrizadeh'), arxiv.Result.Author('Mohsen Kheirandishfard'), arxiv.Result.Author('Farhad Kamangar')], abstract='In this paper, we present a new image segmentation method based on the\\nconcept of sparse subset selection. Starting with an over-segmentation, we\\nadopt local spectral histogram features to encode the visual information of the\\nsmall segments into high-dimensional vectors, called superpixel features. Then,\\nthe superpixel features are fed into a novel convex model which efficiently\\nleverages the features to group the superpixels into a proper number of\\ncoherent regions. Our model automatically determines the optimal number of\\ncoherent regions and superpixels assignment to shape final segments. To solve\\nour model, we propose a numerical algorithm based on the alternating direction\\nmethod of multipliers (ADMM), whose iterations consist of two highly\\nparallelizable sub-problems. We show each sub-problem enjoys closed-form\\nsolution which makes the ADMM iterations computationally very efficient.\\nExtensive experiments on benchmark image segmentation datasets demonstrate that\\nour proposed method in combination with an over-segmentation can provide high\\nquality and competitive results compared to the existing state-of-the-art\\nmethods.', url='http://arxiv.org/abs/1804.02721v1', pdf_path='./papers/1804.02721v1.Image_Segmentation_using_Sparse_Subset_Selection.pdf', content='Image Segmentation using Sparse Subset Selection\\nFariba Zohrizadeh\\nMohsen Kheirandishfard\\nFarhad Kamangar\\nDepartment of Computer Science and Engineering,\\nUniversity of Texas at Arlington, USA\\n{fariba.zohrizadeh,mohsen.kheirandishfard,farhad.kamangar}@uta.edu\\nAbstract\\nIn this paper, we present a new image segmentation\\nmethod based on the concept of sparse subset selection.\\nStarting with an over-segmentation, we adopt local spec-\\ntral histogram features to encode the visual information of\\nthe small segments into high-dimensional vectors, called\\nsuperpixel features. Then, the superpixel features are fed\\ninto a novel convex model which efﬁciently leverages the\\nfeatures to group the superpixels into a proper number of\\ncoherent regions. Our model automatically determines the\\noptimal number of coherent regions and superpixels as-\\nsignment to shape ﬁnal segments.\\nTo solve our model,\\nwe propose a numerical algorithm based on the alternat-\\ning direction method of multipliers (ADMM), whose itera-\\ntions consist of two highly parallelizable sub-problems. We\\nshow each sub-problem enjoys closed-form solution which\\nmakes the ADMM iterations computationally very efﬁcient.\\nExtensive experiments on benchmark image segmentation\\ndatasets demonstrate that our proposed method in combi-\\nnation with an over-segmentation can provide high quality\\nand competitive results compared to the existing state-of-\\nthe-art methods.\\n1. Introduction\\nImage segmentation is a fundamental and challenging\\ntask in computer vision with diverse applications in various\\nareas, such as video segmentation [26, 27], object segmen-\\ntation [11, 15, 20, 25], and semantic segmentation [9, 31].\\nThe primary challenges of image segmentation are rooted\\nin the diversity and ambiguity of visual textures encoun-\\ntered in input images. The solution to these challenges has\\nbeen the subject of some research studies in the recent years\\n[1, 17, 54].\\nOne of the major challenges in image segmentation is\\nto determine the optimal number of coherent regions. This\\nparameter can be calculated based on the distribution of im-\\nage features [54], given as a prior knowledge [29, 32, 56] or\\nset to a constant value [1], depending on the segmentation\\n(a)\\n(b)\\n(c)\\n(d)\\nFigure 1: Segmenting image pixels into multiple coherent\\nregions. (a) Input image. (b-d) Segmentation results when\\nthe number of coherent regions is overestimated (b), under-\\nestimated (c), and properly determined (our method) (d).\\nmethodology. The performance of segmentation methods\\nheavily depends on the right choice of this parameter, de-\\nnoted by K. Figure 1 illustrates the segmentation results ob-\\ntained for various choices of K. In the case that K is over-\\nestimated (shown in Figure 1b), each coherent region may\\nbe divided into many separate segments. To merge these\\nsegments, many time-consuming and complicated steps are\\nrequired which in turn increase the computational complex-\\nity and decrease the performance of the algorithm. On the\\nother hand, when K is underestimated (shown in Figure\\n1c), some coherent regions are forced to be merged together\\nwhich in turn leads to a poor segmentation quality. Figure\\n1d shows our method has achieved a high quality segmen-\\ntation by properly determining parameter K.\\nGenerally, determination of the number of coherent re-\\ngions in image segmentation is nearly similar to the prob-\\nlem of ﬁnding the optimal number of clusters in other areas\\n[3, 46, 49]. These problems are similar in a sense that they\\nboth seek to ﬁnd the optimal number of groups. However,\\nthey may be different depending on the nature and intrin-\\nsic properties of their input data. For instance, in image\\nsegmentation, the spatial relationship among the image fea-\\ntures can be perceived as an important property of data. Fea-\\ntures may also have more speciﬁc properties depending on\\nthe feature extraction procedure. These properties need to\\nbe taken into account in determining the optimal number of\\narXiv:1804.02721v1  [cs.CV]  8 Apr 2018\\ncoherent regions.\\nIn this paper, we adopt local spectral histogram (LSH)\\nfeatures [34] to model the input image. These features are\\ncomputed by averaging the distribution of visual properties\\n(such as color, texture, etc.) over a local patch centered at\\neach pixel. Therefore, they can be considered as power-\\nful tools to encode the local texture information. As LSH\\nfeatures are computed by averaging distributions in a local\\nneighborhood, they are always nonnegative and the features\\nbelonging to the same coherent region are linearly depen-\\ndent on each other. Our method leverages these properties\\nto develop a convex model based on the concept of sparse\\nsubset selection. The main contributions of this work can\\nbe summarized as follows:\\nI: We design an effective convex model based on the prop-\\nerties of LSH features which automatically determines the\\noptimal number of coherent regions and pixels assignment.\\nII: We develop a parallel numerical algorithm based on the\\nalternating direction method of multiplier [4, 18] whose it-\\nerations consists of two sub-problems with closed-form so-\\nlutions.\\nWe show the proposed algorithm can solve our\\nmodel signiﬁcantly faster than the standard convex solvers\\n[40, 48, 50] while maintaining a high accuracy.\\nIII: We conduct extensive experiments on three commonly\\nused datasets, BSD300 [38], BSD500 [1], and MSRC [45]\\nto show our results are competitive comparing to the results\\nof state-of-the-arts methods.\\nThe remainder of this paper is structured as follows: Sec-\\ntion 2, shortly reviews related works; Section 3, explains\\nour method in detail; Section 4, provides experimental re-\\nsults; Section 5, draws a conclusion about this paper.\\nNotation: Throughout this paper, matrices, vectors, and\\nscalars are denoted by boldface uppercase, boldface low-\\nercase, and italic lowercase letters, respectively.\\nFor a\\ngiven matrix A, symbol Ai,j denotes the element at ith row\\nand jth column, ∥A∥F indicates the Frobenius norm, and\\n∥A∥p,q is the ℓp norm of the ℓq norm of the rows in A. For a\\ngiven vector a, symbols ∥a∥p, diag(a), and ai denote stan-\\ndard ℓp norm, a diagonal matrix formed by the elements\\nof a, and the ith element of a, respectively. Symbol tr(.)\\nstands for the trace operator, R+ indicates the set of posi-\\ntive real numbers, and 1 is a column vector of all ones of\\nappropriate dimension.\\n2. Related works\\nOver-segmentation is obtained by partitioning the input\\nimage into multiple small homogeneous regions, called su-\\nperpixels. Recent segmentation algorithms usually utilize\\nan over-segmentation and merge the similar superpixels to\\nshape ﬁnal segments [17, 32, 56]. Fu et al. [17] proposed\\na pipeline of three effective post-processing steps which are\\napplied on an over-segmentation to shape ﬁnal segments. Li\\net al. [32] suggested to construct a bipartite graph over mul-\\ntiple over-segmentations provided by [7] and [16]. Then,\\nthe spectral clustering is applied on the graph to form ﬁ-\\nnal segments. Ren et al. [43] presented a method which\\nconstructs a cascade of boundary classiﬁers and iteratively\\nmerges the superpixels of an over-segmentation to build ﬁ-\\nnal segments.\\nOne notable segmentation method is presented by Ar-\\nbelaez et al. in [1], which reduces the problem of image\\nsegmentation to a contour detection. The method combines\\nmultiple contour cues of different image layers to create a\\ncontour map, called gPb. The contour map and its corre-\\nsponding hierarchical segmentation further utilized by some\\nalgorithms as an initial over-segmentation [10, 19, 28, 33].\\nLiu et al. [33] trained a classiﬁer over the gPb results to\\nconstruct a hierarchical region merging tree. Then, the clas-\\nsiﬁer iteratively merges the most similar regions to shape ﬁ-\\nnal segments. Gao et al. [19] proposed to construct a graph\\nover the gPb results based on the spatial and visual infor-\\nmation of superpixels. Then, a model is proposed to parti-\\ntion the graph into multiple components where each one is\\ncorresponding to a ﬁnal segment. Recently, a widely-used\\nextension of gPb is proposed by Arbelaez et al. [2], called\\nMultiscale Combinatorial Grouping (MCG). The method\\ncombines the information obtained from multiple image\\nscales to generate a hierarchical segmentation. Yu et al.\\n[53] proposed a nonlinear embedding method based on a ℓ1-\\nregularized objective which is integrated into MCG frame-\\nwork to provide better local distances among the superpix-\\nels. Chen et al. [6] realigned the MCG results by modifying\\nthe depth of segments in its hierarchical structure. There are\\nsome recently-developed methods based on deep learning\\nin many related tasks such as contour detection [30, 37] and\\nsemantic image segmentation [24, 35, 41, 55]. Although\\nthese methods are able to exploit more sophisticated and\\ncomplex representative features, they are often highly de-\\nmanding in terms of training data and training time. There-\\nfore, these methods may not be the most appropriate choice\\nin some applications.\\nTo illustrate, consider natural im-\\nage segmentation in which many unknown and diverse pat-\\nterns are likely to be presented in each single image. This\\nimplies that we may have insufﬁcient number of training\\nsamples per each pattern. Motivated by this, we propose a\\nnew image segmentation method based on the concept of\\nsparse subset selection [12, 14]. The method starts with an\\nover-segmentation (e.g., MCG) and uses an effective con-\\nvex model to group the superpixels into a proper number\\nof coherent regions. Our work is roughly similar to the\\nfactorization-based segmentation (Fact) algorithm [54] in\\nthe sense that both use local spectral histogram (LSH) fea-\\ntures to model the input image and seek to estimate the op-\\ntimal number of coherent regions. However, our method\\ndiffers from Fact in two major ways: (1) Fact determines\\nthe optimal number of coherent regions and pixels assign-\\nInput Image\\nOver-\\nSegmentation \\nDictionary\\nSelected \\nDictionary\\nSuperpixels\\nAssignment\\nFinal \\nSegmentation\\nFigure 2: The pipeline of our proposed algorithm. Given an input image, we adopt an algorithm to generate a super-pixel\\nsegmentation layer. Then, we compute the superpixels features and learn a dictionary of words over all superpixels. Our\\nconvex model efﬁciently selects a small subset of informative words and softly assigns superpixels to the selected words. The\\nneighboring superpixels which are assigned to the same selected words are merged to shape ﬁnal segmentation.\\nment in two consecutive steps which may lead to error prop-\\nagation, but our model simultaneously determines the opti-\\nmal number of coherent regions and pixels assignment in\\nan effective manner. (2) Fact does not take advantage of\\nthe spatial information among pixels, but we incorporate\\nthis information as a Laplacian regularization term in our\\nconvex model. Moreover, we propose a parallel numeri-\\ncal algorithm based on the alternating direction method of\\nmultipliers [4, 18] to solve our model and obtain ﬁnal seg-\\nments. Note that the model can be easily utilized in some\\nother applications such as video summarization [12, 13] and\\ndimensionality reduction [14].\\n3. Proposed method\\nThis section describes our segmentation method in two\\nphases: problem formulation and numerical algorithm. The\\nﬁrst phase formulates a convex model based on the proper-\\nties of local spectral histogram (LSH) features and the sec-\\nond phase presents our solution to the model in details.\\n3.1. Problem formulation\\nGiven an input image I, we start with an over-\\nsegmentation consisting n superpixels.\\nWe form feature\\nmatrix X = [x1|x2| . . . |xn] ∈R\\nd×n\\n+\\nby averaging the LSH\\nfeatures of pixels within each superpixel. Hence, each xi is\\nconsidered as a d-dimensional feature corresponding to the\\nith superpixel. Under the assumption of linear dependence\\namong the LSH features, we model the feature matrix X as,\\nX = DU + E,\\n(1)\\nwhere D = [d1|d2| . . . |dl] ∈R\\nd×l\\n+\\nis a dictionary of\\nl words inferred from the superpixels features, U =\\n[u1|u2| . . . |un] ∈Rl×n denotes a coefﬁcient matrix whose\\nrows indicate the contribution of each word in reconstruct-\\ning X, and E ∈Rd×n indicates the model error. The goal\\nis to design a model which takes into account the linear\\ndependence and spatial relationship among the features to\\ncompute an optimal matrix U.\\nIn order to incorporate the linear dependence among the\\nfeatures into the model, we adopt a non-negative matrix fac-\\ntorization framework to construct D over the feature ma-\\ntrix X. Consider the dissimilarity matrix R ∈Rl×n where\\nRj,i indicates the dissimilarity between dj and xi. We use\\nRj,i = ∥dj −xi∥\\n2\\n2 to compute R. In the case of normalized\\nfeatures and visual words, Rj,i only depends on the inner\\nproduct between dj and xi. More precisely, it shows how\\nwell xi is expressible by dj which is a reasonable dissim-\\nilarity measure according to the linear dependence among\\nthe features. Since the superpixels are not necessarily of\\nthe same size, we deﬁne a diagonal regularization matrix\\nP ∈Rn×n whose diagonal elements are proportional to the\\nnumber of pixels in each superpixel. The elements of P\\nscale each Rj,i by the size of the ith superpixel.\\nIn order to embed the spatial relationship among the su-\\nperpixels into the model, we construct a graph over the ini-\\ntial over-segmentation. Let G = (V, E, W) be the graph\\nwhere nodes are superpixels and edges connect every pairs\\nof adjacent superpixels with a weight speciﬁed by W ∈\\nRn×n. The edge weight between the adjacent superpixels i\\nand j indicates their similarity and is deﬁned as:\\nWi,j = e−∥xi−xj∥2\\n2\\nσx\\n−b,\\n(2)\\nwhere b is the average strength of their common boundary\\nand σx controls the effect of feature distances on their sim-\\nilarity weight. Given such graph G, we deﬁne Laplacian\\nmatrix L ∈Rn×n as L = diag(W1) −W.\\nOnce the Laplacian matrix L, the dissimilarity matrix R,\\nand the regularization matrix P are computed, we seek to\\nﬁnd a small subset of the dictionary words that well repre-\\nsents feature matrix X. To do so, a model is required which\\nsatisﬁes the following requirements:\\n• minimizes the number of selected words. In the ideal\\ncase, we are interested to have a single word corre-\\nsponding to each coherent region.\\n• ensures each feature in {xi}\\nn\\ni=1 is well expressible as a\\nnonnegative linear combination of the selected words.\\nThe coefﬁcients of such linear combination indicate\\nthe contribution of each selected word in reconstruct-\\ning the feature.\\n• ensures each feature in {xi}\\nn\\ni=1 is expressed by at least\\none selected word. To do so, we impose a constraint\\non the sum of the linear combination coefﬁcients.\\n• takes advantage of the spatial relationship and linear\\ndependence of the features.\\nMotivated by [12], we formulate the following convex\\nmodel which fulﬁlls the requirements.\\nminimize\\nU∈Rl×n\\ntr(PR⊤U)+γ tr(ULU⊤)+λ∥U∥1,∞\\n(3a)\\nsubject to U ≥0,\\n(3b)\\n1⊤U = 1⊤,\\n(3c)\\nwhere γ > 0 and λ > 0 are regularization parameters. The\\nﬁrst term in (3) is corresponding to the cost of representing\\nfeature matrix X using dictionary D proportional to the size\\nof superpixels. The Laplacian regularization term incorpo-\\nrates the spatial relation of superpixels into the objective and\\nthe last term is a row sparsity regularization term which pe-\\nnalizes the objective in proportion to the number of selected\\nwords. Note that although D does not directly appear in (3),\\nthe rows of U are constructed based on the contribution of\\nthe dictionary words, {dj}\\nl\\nj=1, in reconstructing X.\\nThe optimal solution of problem (3) is U∗∈[0, 1]\\nl×n\\nwhose nonzero rows are corresponding to the selected\\nwords. Note that U∗not only determines the selected words\\nbut also shows the contribution of selected words in recon-\\nstructing the superpixel features {xi}\\nn\\ni=1. Hence, the ele-\\nments of U∗can be interpreted as a soft assignment of the\\nsuperpixels to the selected words. In this case, the ith super-\\npixel is assigned to the selected word which has the largest\\ncontribution in the reconstruction of xi. Final segmentation\\nis obtained by merging the neighboring superpixels which\\nare assigned to the same selected word. Figure 2 illustrates\\nour segmentation pipeline in details.\\n3.2. Numerical algorithm\\nTo solve the model, we propose an efﬁcient numerical\\nalgorithm based on alternating direction method of mul-\\ntipliers (ADMM) which is a powerful technique to solve\\nconvex optimization problems. The basic idea of ADMM\\nis to introduce auxiliary variables to break down a com-\\nplicated convex problem into smaller sub-problems where\\neach one is efﬁciently solvable via an explicit formulas.\\nThe ADMM iteratively solves the sub-problems until con-\\nvergence. To formulate the ADMM, let deﬁne m ∈Rl such\\nthat mj = argmaxi |Uj,i| and reformulate (3) as follows:\\nminimize\\nU∈Rl×n\\nm∈Rl\\ntr(PR⊤U)+γ tr(ULU⊤)+λ1⊤m\\n(4a)\\nsubject to U ≥0,\\n(4b)\\n1⊤U = 1⊤,\\n(4c)\\nm1⊤≥U,\\n(4d)\\nwhere (4d) is imposed to guarantee the equivalence of (3)\\nand (4). Using slack variable V = [v1| . . . |vn] ∈Rl×n, (4)\\ncan be written in more convenient form as,\\nminimize\\nU,V∈Rl×n\\nm∈Rl\\ntr(PR⊤U)+γ tr(ULU⊤)+λ1⊤m\\n(5a)\\nsubject to U ≥0,\\n(5b)\\n1⊤U = 1⊤,\\n(5c)\\nm1⊤= V + U,\\n(5d)\\nV ≥0.\\n(5e)\\nAs 1⊤m1⊤1=1⊤(V + U)1, the third term of (5a) can\\nbe equivalently written as λ\\nn1⊤(V + U)1. Hence, (5) can\\nbe reformulated independent of m as:\\nminimize\\nU,V∈Rl×n tr(PR⊤U)+γ tr(ULU⊤)+ λ\\nn1⊤(V+U)1(6a)\\nsubject to U ≥0,\\n(6b)\\n1⊤U = 1⊤,\\n(6c)\\nvi−1 + ui−1 = vi + ui,\\ni = 2, . . . , n,\\n(6d)\\nV ≥0,\\n(6e)\\nwhere (6d) is obtained by removing m from (5d). In or-\\nder to derive an ADMM formulation with subproblems pos-\\nsessing explicit formulas, we introduce auxiliary matrices\\nˆU ∈Rl×n, ˆV ∈Rl×n and reformulate (6) as:\\nminimize\\nU,V, ˆU,ˆV∈Rl×n tr(PR⊤ˆU)+γ tr(ULU⊤)+ λ\\nn1⊤(V+U)1\\n+ µ1\\n2\\n\\r\\r\\rU −ˆU\\n\\r\\r\\r\\n2\\nF + µ2\\n2\\n\\r\\r\\rV −ˆV\\n\\r\\r\\r\\n2\\nF\\n(7a)\\nsubject to ˆU ≥0,\\n(7b)\\n1⊤ˆU = 1⊤,\\n(7c)\\nvi−1 + ui−1 = vi + ui,\\ni = 2, . . . , n,\\n(7d)\\nˆV ≥0,\\n(7e)\\nU = ˆU,\\n(7f)\\nV = ˆV,\\n(7g)\\nwhere µ1 > 0 and µ2 > 0 are the augmented Lagrangian\\nparameters. As it is suggested in [4], we can set µ1 = µ2 =\\nµ. Note that (7) is equivalent to (6), because the additional\\nterms in (7a) vanish for any feasible solution. To solve (7),\\naugmented Lagrangian function is formed as:\\nLµ(U,V, ˆU, ˆV,Λ1,Λ2)=tr(PR⊤ˆU)+γ tr(ULU⊤)\\n+ λ\\nn1⊤(V+U)1+ µ\\n2\\n\\r\\r\\r\\rU−ˆU+ Λ1\\nµ\\n\\r\\r\\r\\r\\n2\\nF\\n+ µ\\n2\\n\\r\\r\\r\\rV−ˆV+ Λ2\\nµ\\n\\r\\r\\r\\r\\n2\\nF\\n(8)\\nwhere Λ1 ∈Rl×n and Λ2 ∈Rl×n are Lagrange multipliers\\nassociated with the equality constraints (7f) and (7g).\\nGiven initial values for ˆU, ˆV, Λ1, and Λ2, the ADMM\\niterations to solve (7) are summarized as follow:\\n(Uk+1,Vk+1):= argmin\\nU,V∈Rl×nLµ(U,V, ˆUk, ˆVk, Λk\\n1, Λk\\n2)\\nsubject tovi−1+ui−1 =vi+ui, i = 2, . . . , n,\\n(9)\\n( ˆUk+1,ˆVk+1):= argmin\\nˆU, ˆV∈Rl×nLµ(Uk+1,Vk+1, ˆU, ˆV, Λk\\n1, Λk\\n2)\\nsubject to ˆU ≥0, 1⊤ˆU = 1⊤,\\nˆV ≥0.\\n(10)\\n(\\nΛk+1\\n1\\n= Λk\\n1 + µ(Uk+1 −ˆUk+1)\\nΛk+1\\n2\\n= Λk\\n2 + µ(Vk+1 −ˆVk+1)\\n(11)\\nNote that the variables in each of the above iterations can be\\nstacked together to form a single matrix variable. Therefore,\\nthe numerical algorithm is not considered as a multi-block\\nADMM. To solve (9), let form yj ∈R2n by concatenating\\nthe jth rows of U and V. Then, (9) can be divided into l\\nequality constrained quadratic programs as follows:\\nminimize\\nyj∈R2n\\n1\\n2yj\\n⊤Byj + yj\\n⊤bj\\n(12a)\\nsubject to Ayj = c,\\n(12b)\\nwhere B ∈R2n×2n is a block diagonal positive semi-\\ndeﬁnite matrix, A ∈Rn×2n is a sparse matrix correspond-\\ning to the constraint (7d), and c∈Rn is a vector of all zeros.\\nProblem (10) can be split into two separate sub-problems\\nwith closed-form solutions as follows:\\nminimize\\nˆU∈Rl×n\\n\\r\\r\\r\\r ˆU −(U + Λ1 + RP⊤\\nµ\\n)\\n\\r\\r\\r\\r\\n2\\nF\\n(13a)\\nsubject to\\nˆU ≥0, 1⊤ˆU = 1⊤,\\n(13b)\\nminimize\\nˆV∈Rl×n\\n\\r\\r\\r\\r ˆV −(V + Λ2\\nµ )\\n\\r\\r\\r\\r\\n2\\nF\\n(14a)\\nsubject to\\nˆV ≥0,\\n(14b)\\n0\\n100 200 300 400\\n10−6\\n10−5\\n10−4\\n10−3\\n10−2\\n10−1\\nIteration Number\\nϵ\\nµ = 0.1\\nµ = 1\\nµ = 10\\n0\\n100 200 300 400\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\nIteration Number\\nCost Value\\nµ = 0.1\\nµ = 1\\nµ = 10\\n1\\nFigure 3: Our convergence behavior for different choices of\\nµ. Left: Combined residual. Right: Cost function.\\nwhere each one consists of n computationally cheap par-\\nallel programs. Sub-problem (13) can be divided into n\\nparallel programs over the columns of ˆU where each one\\nis a Euclidean norm projection onto the probability simplex\\nconstraints. These programs enjoy closed-form solutions as\\npresented in [51]. Sub-problem (14) consists of n small par-\\nallel programs over the columns of ˆV, where each program\\nis a minimization of the Euclidean norm projection onto the\\nnonnegative orthant and admits closed-form solution.\\nProblem (11) can be split into two sub-problems over\\nΛ1 and Λ2 where each sub-problem consists of n parallel\\nupdates over the columns of corresponding matrix.\\nOur numerical algorithm consists of two sub-problems\\nwith closed-form solutions, which makes the iterations\\ncomputationally efﬁcient. To evaluate the convergence be-\\nhavior of the proposed algorithm, we adopt combined resid-\\nual presented in [21] as:\\nϵk+1 = 1\\nµ\\n\\r\\rΛk+1\\n1\\n−Λk\\n1\\n\\r\\r2\\nF + µ\\n\\r\\rUk+1 −Uk\\r\\r2\\nF\\n+ 1\\nµ\\n\\r\\rΛk+1\\n2\\n−Λk\\n2\\n\\r\\r2\\nF + µ\\n\\r\\rVk+1 −Vk\\r\\r2\\nF .\\n(15)\\nFigure 3 demonstrates the convergence behavior of our al-\\ngorithm in terms of combined residual and cost function.\\nWe solve (3) for three choices of µ to show the sensitivity\\nof our numerical algorithm with respect to µ. Clearly seen,\\nthe numerical algorithm converges in a reasonable number\\nof iterations for a wide range of µ.\\n4. Experiments\\nWe perform multiple experiments on benchmark image\\nsegmentation datasets to evaluate the performance of our\\nmethod (termed IS4). The ﬁrst part of this section gives\\ninformation about the benchmarking datasets, evaluation\\nmeasures, and parameter settings. The second part com-\\npares our results with state-of-the-art methods to demon-\\nstrate the effectiveness of IS4.\\n4.1. Settings\\nDatasets: We adopt three commonly used datasets in image\\nsegmentation: (1) BSD300 [38] containing 300 images (200\\ntraining and 100 validation) of size 321 × 481, where each\\nimage has in average 5 ground-truths manually drawn by\\nhuman; (2) BSD500 [1] is an extension of BSD300 with 200\\nnew testing images; (3) MSRC [45] containing 591 images\\nof size 320 × 213, where each image has a single ground-\\ntruth. It should be mentioned that we use the cleaned ver-\\nsion of MSRC [36] in our experiments.\\nMeasures: We adopt three widely accepted measures in im-\\nage segmentation: (1) segmentation Covering (Cov) [36],\\nwhich measures the overlapping between two segmenta-\\ntions; (2) probability Rand Index (PRI) [42], which mea-\\nsures the probability that a pair of pixels is consistently\\nlabeled in two segmentations; (3) variation of Information\\n(VoI) [39], which measures the distance between two seg-\\nmentations as the average of their conditional entropy.\\nParameters: Given an over-segmentation, IS4 computes\\nthe superpixels features {xi}\\nn\\ni=1 by averaging the local spec-\\ntral histogram (LSH) [34] features of pixels within each su-\\nperpixel. We use the algorithm and parameters presented\\nin [54] to extract features and build a dictionary of size l.\\nParameter l should be chosen sufﬁciently large (larger than\\nthe number of coherent regions) to ensure each superpixel\\nfeature is well expressible as a nonnegative linear combi-\\nnation of the dictionary words. We set l = 20 which is\\nnormally much larger than the number of coherent regions\\nin BSD and MSRC images. Our proposed model in (3)\\nhas two parameters γ and λ, where γ controls the effect\\nof spatial relationship among superpixels and λ controls the\\nnumber of selected words. As γ increases, the neighbor-\\ning regions are more likely to be merged together and as λ\\nincreases, the number of selected words reduces. We opti-\\nmize γ on the training set of BSD by applying grid search\\nand use the optimized γ in our experiments on BSD300,\\nBSD500, and MSRC datasets. Parameter λ is set to αλmax,\\nwhere α ∈[0, 1] and λmax is a constant computed based on\\nPR⊤, γ, and L using [12]. If α is greater than 1 (which\\nmeans λ>λmax), only a single word is selected to represent\\nthe whole features. We follow [1, 2, 19, 43] to present our\\nresults as a family of segmentations which share the same\\nparameter settings except for α that varies from 0 to 1. The\\nevaluation measures are also reported at Optimal Dataset\\nScale (ODS) and Optimal Image Scale (OIS).\\n4.2. Results\\nSegmentation quality:\\nWe run IS4 on the benchmark\\ndatasets and report the results in tables 1, 2, and 3, to make\\na comparison with recent methods such as, Normalized cut\\n(Ncut) [44], Multi-scale Normalized cut (MNcut)[8], gPb-\\nUltametric contour map (gPb) [1], Image Segmentation by\\nCascade Region Agglomeration (ISCRA) [43], Reverse Im-\\nCov (↑)\\nPRI (↑)\\nVoI (↓)\\nMethods\\nODS\\nOIS\\nODS\\nOIS\\nODS\\nOIS\\nMNcut[8]\\n0.44\\n0.53\\n0.75\\n0.79\\n2.18\\n1.84\\ngPb-UCM [1]\\n0.59\\n0.65\\n0.81\\n0.85\\n1.65\\n1.47\\nISCRA [43]\\n0.60\\n0.67\\n0.81\\n0.86\\n1.61\\n1.40\\nRIS+HL[52]\\n0.59\\n0.65\\n0.82\\n0.86\\n1.71\\n1.53\\nMCG [2]\\n0.61\\n0.67\\n0.81\\n0.86\\n1.55\\n1.37\\nGWC [19]\\n0.61\\n0.68\\n0.82\\n0.86\\n1.60\\n1.42\\nIS4(MCG)\\n0.61\\n0.65\\n0.81\\n0.83\\n1.54\\n1.40\\nTable 1: Quantitative comparisons on BSD300 val set.\\nCov (↑)\\nPRI (↑)\\nVoI (↓)\\nMethods\\nODS\\nOIS\\nODS\\nOIS\\nODS\\nOIS\\nNcut [44]\\n0.45\\n0.53\\n0.78\\n0.80\\n2.23\\n1.89\\ngPb-UCM [1]\\n0.59\\n0.65\\n0.83\\n0.86\\n1.69\\n1.48\\nDC-Seg [10]\\n0.59\\n0.64\\n0.82\\n0.85\\n1.68\\n1.54\\nISCRA [43]\\n0.59\\n0.66\\n0.82\\n0.85\\n1.60\\n1.42\\nRIS+HL[52]\\n0.57\\n0.66\\n0.84\\n0.86\\n1.73\\n1.55\\nMCG [2]\\n0.61\\n0.66\\n0.83\\n0.86\\n1.57\\n1.39\\nPFE+MCG [53]\\n0.62\\n0.68\\n0.84\\n0.87\\n1.56\\n1.36\\nMCG-Aligned [6]\\n0.63\\n0.68\\n0.83\\n0.86\\n1.53\\n1.38\\nGWC [19]\\n0.61\\n0.66\\n0.83\\n0.87\\n1.62\\n1.41\\nIS4(MCG)\\n0.63\\n0.66\\n0.83\\n0.85\\n1.55\\n1.35\\nTable 2: Quantitative comparisons on BSD500 test set.\\nCov (↑)\\nPRI (↑)\\nVoI (↓)\\nMethods\\nODS\\nOIS\\nODS\\nOIS\\nODS\\nOIS\\ngPb-UCM [1]\\n0.65\\n0.75\\n0.78\\n0.85\\n1.28\\n0.99\\nISCRA [43]\\n0.67\\n0.75\\n0.77\\n0.85\\n1.18\\n1.02\\nGWC [19]\\n0.68\\n0.76\\n0.78\\n0.85\\n1.24\\n0.98\\nMCG [2]\\n0.66\\n0.72\\n0.78\\n0.83\\n1.23\\n1.14\\nIS4(MCG)\\n0.69\\n0.77\\n0.80\\n0.86\\n1.15\\n0.91\\nTable 3: Quantitative comparisons on MSRC dataset.\\nage Segmentation with High/Low-level pairwise potentials\\n(RIS-HL) [52], Multiscale Combinatorial Grouping (MCG)\\n[2], Contour-guided Color Palletes (CCP-2) [17], Piecewise\\nFlat Embedding (PFE) [53], Discrete-Continuous Gradi-\\nent Orientation Estimation for Segmentation (DC-Seg) [10],\\nGraph Without Cut (GWC) [19], and Aligned hierarchical\\nsegmentation (MCG-Aligned) [6]. All scores are collected\\nfrom [1, 2, 6, 10, 19] except the MCG on MSRC and CCP-2\\non BSD500 which are obtained by running the implemen-\\ntations provided by the respective authors.\\nParameter sensitivity: To evaluate the role played by an\\ninitial over-segmentation, we run IS4 in combination with\\nthree segmentation methods CCP-2, ISRA, and MCG. In\\nCCP-2, we use the same parameter settings as suggested\\nby the respective author. In MCG and ISRA we respec-\\nCov (↑)\\nPRI (↑)\\nVoI (↓)\\nMethods\\nODS\\nOIS\\nODS\\nOIS\\nODS\\nOIS\\nISCRA [43]\\n0.59\\n0.66\\n0.82\\n0.85\\n1.60\\n1.42\\nIS4 (ISCRA) [7]\\n0.61\\n0.65\\n0.82\\n0.85\\n1.58\\n1.38\\nCCP-2 [17]\\n0.45\\n−\\n0.79\\n−\\n3.1\\n−\\nIS4 (CCP-2)\\n0.58\\n0.64\\n0.81\\n0.85\\n1.78\\n1.52\\nMCG [2]\\n0.61\\n0.66\\n0.83\\n0.86\\n1.57\\n1.39\\nIS4(MCG)\\n0.63\\n0.66\\n0.83\\n0.85\\n1.55\\n1.35\\nTable 4: Sensitivity of our method with respect to the dif-\\nferent initial over-segmentations on BSD500 test set.\\nCov (↑)\\nPRI (↑)\\nVoI (↓)\\nγ\\nODS\\nOIS\\nODS\\nOIS\\nODS\\nOIS\\n10−2\\n0.62\\n0.65\\n0.83\\n0.85\\n1.56\\n1.39\\n10−1\\n0.62\\n0.65\\n0.83\\n0.85\\n1.55\\n1.39\\n100\\n0.62\\n0.66\\n0.83\\n0.85\\n1.54\\n1.38\\n101\\n0.63\\n0.66\\n0.83\\n0.85\\n1.54\\n1.37\\n102\\n0.62\\n0.64\\n0.81\\n0.83\\n1.53\\n1.41\\nTable 5: Sensitivity of our method with respect to the pa-\\nrameter variations on BSD500 test set.\\ntively adopted the segmentations at scale 0.39 and 44 as the\\nover-segmentations. In average, the over-segmentation lay-\\ners provided by CCP-2, ISRA, and MCG have 120, 45, and\\n37 superpixels, respectively. Figure 4a and table 4 respec-\\ntively show the qualitative and quantitative results of these\\ncombinations. Moreover, we run IS4 for different γ to as-\\nsess our robustness with respect to the variations of γ. The\\nresults are reported in table 5 in terms of segmentation mea-\\nsures. As tables 4 and 5 indicate, IS4 not only achieves\\nsatisfactory results for a wide range of γ but also improves\\nthe quality of initial over-segmentations on most of the seg-\\nmentation measures. It is worth pointing out that IS4 can\\nbe applied on the result of any segmentation method. The\\nresult may either be directly generated by a segmentation\\nalgorithm (e.g., CCP-2) or obtained from a speciﬁc level of\\na hierarchical segmentation (e.g., MCG).\\nTables 1, 2, and 3 show that IS4 in combination with\\nMCG generates a high quality segmentation. The scores\\nindicate that IS4(MCG) outperforms all competitor meth-\\nods on BSD300 (ODS: VoI) and MSCRC (ODS: Cov, PRI,\\nVoI and OIS: Cov, PRI, VoI). Other scores obtained by\\nIS4(MCG) are also on par or in close proximity of the\\nbest competitors except for BSD300 (OIS: Cov, PRI) and\\nBSD500 (OIS: PRI). In comparison with MCG, IS4(MCG)\\nachieves better results on BSD300 (ODS: 0.01 on VoI),\\nBSD500 (ODS: 0.02 on Cov, 0.02 on VoI and OIS: 0.04\\non VoI), and MSRC (ODS: 0.03 on Cov, 0.02 on PRI, 0.08\\non VoI and OIS: 0.05 on Cov, 0.03 on PRI, 0.23 on VoI).\\nOur method is also on par with MCG on BSD300 (ODS:\\nCov, PRI) and BSD500 (ODS: PRI). Moreover, the tables\\nindicate that IS4(MCG) achieves lower score than MCG in\\nBSD300 (OIS: Cov, PRI, VoI) and BSD500 (OIS: PRI). It\\nmay seem reasonable to state that IS4(MCG) should con-\\nsistently improve the MCG measures. However, this is not\\na fairly accurate statement. The reason is that IS4 does not\\ndirectly apply on the MCG hierarchical segmentation to im-\\nprove or degrade the MCG results. It just adopts an over-\\nsegmentation by simply thresholding the MCG hierarchi-\\ncal segmentation and generates a family of segmentations.\\nThese segmentations differ from the ones which may be ob-\\ntained at different thresholds of MCG.\\nMCG usually provides a high-quality hierarchical seg-\\nmentation, but its performance is sometimes unsatisfactory,\\nespecially in textured images. Our method in combination\\nwith MCG improves the segmentation quality of these im-\\nages (shown in Figure 4b) by adopting superpixels features\\nas an informative representation of small regions. Despite\\nthe advantages of IS4, it may fail to correctly segment the\\npixels belonging to an elongated coherent region. The rea-\\nson is that the local neighborhood around these pixels con-\\ntains visual information of neighboring coherent regions.\\nHence, their LSH features are inaccurate, which may cause\\na wrong assignment to the neighboring coherent regions.\\nAlgorithm complexity: We decomposed model (3) into\\nthree sub-problems (9), (10), and (11) which are considered\\nas the steps of the alternating direction method of multipli-\\ners (ADMM). To investigate the complexity of solving these\\nsub-problems, we discuss each one separately. Subproblem\\n(9) is cast as l parallel equality constrained quadratic pro-\\ngrams of form (12) where each can be efﬁciently solved\\nby solving a system of linear equations [5] in O(n2.8) [47].\\nSince A and B are the same in all programs, the left-hand\\nside of such systems are similar. Hence, one program just\\nneeds to be solved and then back-solves can be carried out\\nfor the right-hand sides of other programs. Therefore, the\\ncomplexity of solving (9) is O(n2.8) + O((l−1)n2). It is\\nworth mentioning that in practice the complexity would be\\nmuch lower, because A and B are highly sparse and struc-\\ntured. Subproblem (10) is split into two parallel problems\\n(13) and (14).\\nProblem (13) consists of n parallel pro-\\ngrams where each is solvable in O(l log(l)) [51]. Problem\\n(14) consists of n parallel programs where each is solved\\nin O(l) [4]. Therefore, (10) can be efﬁciently solved in\\nO(nl log(l)). Subproblem (11) consists of n parallel up-\\ndates over the columns of Λ1 and Λ2 which can be per-\\nformed in O(nl). In total, the complexity of our numeri-\\ncal solution is O(n2.8) + O((l−1)n2) in the ﬁrst iteration\\nand O(ln2) in subsequent iterations. Since all steps of our\\nADMM are highly parallelizable, the processing time can\\nbe signiﬁcantly reduced using parallel computation. In the\\ncase of having p parallel processing resources (assumption\\n(a)\\nImage\\nISCRA\\nMCG\\nPFE+MCG IS4(MCG)\\nImage\\nISCRA\\nMCG\\nPFE+MCG IS4(MCG)\\n(b)\\nFigure 4: Qualitative comparison of segmentations. (a) From top to bottom, left column: image, CCP-2 [17], ISCRA [43],\\nand MCG [2]; right column: groundtruth, IS4(CCP-2), IS4(ISCRA), and IS4(MCG); Note that the left column shows the\\ninitial over-segmentation provided by these methods and the right column shows our ﬁnal segmentation with these initial\\nover-segmentations. (b) All segmentation results are shown at Optimal Dataset Scale (ODS).\\nSeDuMi\\nSDPT3\\nMOSEK\\nIS4(MCG)\\nRun-Time (sec.)\\n1.4×102\\n9.8×100\\n2.7×100\\n6.1×10−1\\n(a)\\nSeDuMi\\nSDPT3\\nMOSEK\\nRelative Error\\n1.7×10−2\\n1.1×10−2\\n9.0×10−3\\n(b)\\nTable 6: Performance comparison with convex solvers, Se-\\nDuMi [48], SDPT3 [50], and MOSEK [40].\\nn > l), the complexity of (9), (10), and (11) are O(n2.8),\\nO(⌈n\\np ⌉l log(l)), and O(⌈n\\np ⌉l), respectively.\\nTo compare our numerical algorithm with other solvers,\\nwe solve (3) for 20 randomly chosen images in BSD500 us-\\ning three standard convex solvers [22, 23]. The results are\\nreported in tables 6a and 6b in terms of the average run-\\nning times and relative errors, respectively. In this case, our\\nrelative error is computed as\\n\\r\\r\\rU∗\\nsolver−U∗\\nIS4\\n\\r\\r\\r\\nF\\n\\r\\r\\rU∗\\nsolver\\n\\r\\r\\r\\nF\\n. It should be\\nnoted that our average running time is computed without\\nconsidering any parallelization.\\nTable 6 indicates that our numerical algorithm is not only\\nsigniﬁcantly faster than SeDuMi [48], SDPT3 [50], and\\nMOSEK [40] in solving (3), but also offers an optimal solu-\\ntion extremely close to their solutions. The running time\\nof IS4 heavily depends on the initial over-segmentation.\\nIS4(MCG) takes 5.8 seconds per image in average, 2.7 sec-\\nonds for feature extraction, 2.5 seconds for learning the dic-\\ntionary, and the remaining 0.6 seconds is taken by our nu-\\nmerical algorithm. All the experiments are performed on an\\nIntel Core i5 quad-core 3.20GHz CPU and 16 GB RAM.\\n5. Conclusions\\nThis paper presented a novel segmentation model based\\non the concept of sparse subset selection. The model auto-\\nmatically estimates the optimal number of coherent regions\\nand pixel assignments to form ﬁnal segments. Moreover,\\nwe presented a parallel numerical algorithm based on the al-\\nternating direction method of multipliers (ADMM) to solve\\nour model. The main advantages of this work are as fol-\\nlow: (1) does not require time for training over different\\ndatasets and works well in combination with various seg-\\nmentation methods; (2) consists of three steps: extracting\\nfeatures, learning dictionary, and solving model where each\\none can be implemented in parallel; (3) contains ADMM\\nsteps with closed-form solutions which make the iterations\\ncomputationally very efﬁcient; (4) is not restricted to the\\nsegmentation problem and can be easily extended to other\\napplications, such as video summarization, dimensionality\\nreduction, etc.\\nReferences\\n[1] P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik. Contour de-\\ntection and hierarchical image segmentation. IEEE TPAMI,\\n33(5):898–916, 2011. 1, 2, 6\\n[2] P. Arbelaez, J. Pont-Tuset, J. T. Barron, F. Marques, and\\nJ. Malik.\\nMultiscale combinatorial grouping.\\nIn CVPR,\\n2014. 2, 6, 7, 8\\n[3] A. Ben-Hur, A. Elisseeff, and I. Guyon. A stability based\\nmethod for discovering structure in clustered data. In PSB,\\n2001. 1\\n[4] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Dis-\\ntributed optimization and statistical learning via the alternat-\\ning direction method of multipliers. Foundations and Trends\\nin Machine Learning, 2011. 2, 3, 4, 7\\n[5] S. Boyd and L. Vandenberghe. Convex optimization. Cam-\\nbridge University Press, 2004. 7\\n[6] Y. Chen, D. Dai, J. Pont-Tuset, and L. Van Gool. Scale-aware\\nalignment of hierarchical image segmentation.\\nIn CVPR,\\n2016. 2, 6\\n[7] D. Comaniciu and P. Meer. Mean shift: A robust approach\\ntoward feature space analysis. IEEE TPAMI, 24(5):603–619,\\n2002. 2, 7\\n[8] T. Cour, F. Benezit, and J. Shi. Spectral segmentation with\\nmultiscale graph decomposition. In CVPR, 2005. 6\\n[9] R. D´ıaz, M. Lee, J. Schubert, and C. C. Fowlkes. Lifting gis\\nmaps into strong geometric context for scene understanding.\\nIn WACV, 2016. 1\\n[10] M. Donoser and D. Schmalstieg. Discrete-continuous gradi-\\nent orientation estimation for faster image segmentation. In\\nCVPR, 2014. 2, 6\\n[11] S. Dutt Jain and K. Grauman. Active image segmentation\\npropagation. In CVPR, 2016. 1\\n[12] E. Elhamifar, G. Sapiro, and S. S. Sastry. Dissimilarity-based\\nsparse subset selection.\\nIEEE TPAMI, 38(11):2182–2197,\\nNov 2016. 2, 3, 4, 6\\n[13] E. Elhamifar, G. Sapiro, and R. Vidal. See all by looking at\\na few: Sparse modeling for ﬁnding representative objects. In\\nCVPR, 2012. 3\\n[14] E. Esser, M. Moller, S. Osher, G. Sapiro, and J. Xin.\\nA convex model for nonnegative matrix factorization and\\ndimensionality reduction on physical space.\\nIEEE TIP,\\n21(7):3239–3252, 2012. 2, 3\\n[15] P. Faridi, H. Danyali, M. S. Helfroush, and M. A. Jahromi.\\nAn automatic system for cell nuclei pleomorphism segmen-\\ntation in histopathological images of breast cancer. In SPMB,\\n2016. 1\\n[16] P. F. Felzenszwalb and D. P. Huttenlocher. Efﬁcient graph-\\nbased image segmentation. IJCV, 59(2):167–181, 2004. 2\\n[17] X. Fu, C.-Y. Wang, C. Chen, C. Wang, and C.-C. Jay Kuo.\\nRobust image segmentation using contour-guided color\\npalettes. In ICCV, 2015. 1, 2, 6, 7, 8\\n[18] D. Gabay and B. Mercier. A dual algorithm for the solu-\\ntion of nonlinear variational problems via ﬁnite element ap-\\nproximation. Computers & Mathematics with Applications,\\n2(1):17–40, 1976. 2, 3\\n[19] L. Gao, J. Song, F. Nie, F. Zou, N. Sebe, and H. T. Shen.\\nGraph-without-cut: An ideal graph learning for image seg-\\nmentation. In AAAI, 2016. 2, 6\\n[20] G. Ghiasi and C. C. Fowlkes. Laplacian pyramid reconstruc-\\ntion and reﬁnement for semantic segmentation. In ECCV,\\n2016. 1\\n[21] T. Goldstein, B. O’Donoghue, S. Setzer, and R. Baraniuk.\\nFast alternating direction optimization methods. SIAM Jour-\\nnal on Imaging Sciences, 7(3):1588–1623, 2014. 5\\n[22] M. Grant and S. Boyd.\\nGraph implementations for non-\\nsmooth convex programs.\\nIn V. Blondel, S. Boyd, and\\nH. Kimura, editors, Recent Advances in Learning and Con-\\ntrol, Lecture Notes in Control and Information Sciences,\\npages 95–110. Springer-Verlag Limited, 2008. 8\\n[23] M. Grant and S. Boyd. CVX: Matlab software for disciplined\\nconvex programming, 2014. 8\\n[24] B. Hariharan, P. Arbel´aez, R. Girshick, and J. Malik. Simul-\\ntaneous detection and segmentation. In ECCV, 2014. 2\\n[25] S. D. Jain, B. Xiong, and K. Grauman. Fusionseg: Learn-\\ning to combine motion and appearance for fully automatic\\nsegmention of generic objects in videos.\\narXiv preprint\\narXiv:1701.05384, 2017. 1\\n[26] A. Khoreva, F. Galasso, M. Hein, and B. Schiele. Learning\\nmust-link constraints for video segmentation based on spec-\\ntral clustering. In GCPR, 2014. 1\\n[27] A. Khoreva, F. Galasso, M. Hein, and B. Schiele. Classiﬁer\\nbased graph construction for video segmentation. In CVPR,\\n2015. 1\\n[28] J. Kim and K. Grauman. Boundary preserving dense local\\nregions. In CVPR, 2011. 2\\n[29] T. H. Kim, K. M. Lee, and S. U. Lee. Learning full pair-\\nwise afﬁnities for spectral segmentation.\\nIEEE TPAMI,\\n35(7):1690–1703, 2013. 1\\n[30] I. Kokkinos. Pushing the boundaries of boundary detection\\nusing deep learning. arXiv preprint arXiv:1511.07386, 2015.\\n2\\n[31] P. Kr¨ahenb¨uhl and V. Koltun.\\nEfﬁcient inference in fully\\nconnected crfs with gaussian edge potentials. In NIPS, 2011.\\n1\\n[32] Z. Li, X.-M. Wu, and S.-F. Chang. Segmentation using su-\\nperpixels: A bipartite graph partitioning approach. In CVPR,\\n2012. 1, 2\\n[33] T. Liu, M. Seyedhosseini, and T. Tasdizen. Image segmenta-\\ntion using hierarchical merge tree. IEEE TIP, 25(10):4596–\\n4607, 2016. 2\\n[34] X. Liu and D. Wang. Image and texture segmentation us-\\ning local spectral histograms. IEEE TIP, 15(10):3066–3077,\\n2006. 2, 6\\n[35] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional\\nnetworks for semantic segmentation. In CVPR, 2015. 2\\n[36] T. Malisiewicz and A. A. Efros. Improving spatial support\\nfor objects via multiple segmentations. In BMVC, 2007. 6\\n[37] K. K. Maninis, J. Pont-Tuset, P. Arbelez, and L. Van Gool.\\nConvolutional oriented boundaries: From image segmenta-\\ntion to high-level tasks. IEEE TPAMI, PP(99):1–1, 2017. 2\\n[38] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database\\nof human segmented natural images and its application to\\nevaluating segmentation algorithms and measuring ecologi-\\ncal statistics. In ICCV, 2001. 2, 6\\n[39] M. Meil.\\nComparing clusterings: an axiomatic view.\\nIn\\nICML, 2005. 6\\n[40] A. Mosek. The mosek optimization toolbox for matlab man-\\nual. Version 7.1 (Revision 28), 2015. 2, 8\\n[41] H. Noh, S. Hong, and B. Han. Learning deconvolution net-\\nwork for semantic segmentation. In ICCV, 2015. 2\\n[42] W. M. Rand. Objective criteria for the evaluation of cluster-\\ning methods. JASA, 66(336):846–850, 1971. 6\\n[43] Z. Ren and G. Shakhnarovich. Image segmentation by cas-\\ncaded region agglomeration. In CVPR, 2013. 2, 6, 7, 8\\n[44] J. Shi and J. Malik. Normalized cuts and image segmenta-\\ntion. IEEE TPAMI, 22(8):888–905, 2000. 6\\n[45] J. Shotton, J. Winn, C. Rother, and A. Criminisi. Textonboost\\nfor image understanding: Multi-class object recognition and\\nsegmentation by jointly modeling texture, layout, and con-\\ntext. IJCV, 81(1):2–23, 2009. 2, 6\\n[46] S. Still and W. Bialek. How many clusters? an information-\\ntheoretic perspective.\\nNeural computation, 16(12):2483–\\n2506, 2004. 1\\n[47] V. Strassen. Gaussian elimination is not optimal. Numerische\\nMathematik, 13(4):354–356, 1969. 7\\n[48] J. F. Sturm. Using sedumi 1.02, a matlab toolbox for opti-\\nmization over symmetric cones. Optimization methods and\\nsoftware, 11(1-4):625–653, 1999. 2, 8\\n[49] R. Tibshirani, G. Walther, and T. Hastie. Estimating the num-\\nber of clusters in a data set via the gap statistic. Journal of the\\nRoyal Statistical Society: Series B (Statistical Methodology),\\n63(2):411–423, 2001. 1\\n[50] K.-C. Toh, M. J. Todd, and R. H. T¨ut¨unc¨u. Sdpt3a matlab\\nsoftware package for semideﬁnite programming, version 1.3.\\nOptimization methods and software, 11(1-4):545–581, 1999.\\n2, 8\\n[51] W. Wang and M. A. Carreira-Perpin´an. Projection onto the\\nprobability simplex: An efﬁcient algorithm with a simple\\nproof, and an application. arXiv preprint arXiv:1309.1541,\\n2013. 5, 7\\n[52] J. Wu, J. Zhu, and Z. Tu. Reverse image segmentation: A\\nhigh-level solution to a low-level task. In BMVC, 2014. 6\\n[53] Y. Yu, C. Fang, and Z. Liao. Piecewise ﬂat embedding for\\nimage segmentation. In ICCV, 2015. 2, 6\\n[54] J. Yuan, D. Wang, and A. M. Cheriyadat. Factorization-based\\ntexture segmentation. IEEE TIP, 24(11):3488–3497, 2015.\\n1, 2, 6\\n[55] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet,\\nZ. Su, D. Du, C. Huang, and P. H. Torr. Conditional ran-\\ndom ﬁelds as recurrent neural networks. In ICCV, 2015. 2\\n[56] F. Zohrizadeh, M. Kheirandishfard, K. Ghasedidizaji, and\\nF. Kamangar.\\nReliability-based local features aggregation\\nfor image segmentation. In ISVC, 2016. 1, 2\\n'), ResearchPaper(title='Hierarchical Homogeneity-Based Superpixel Segmentation: Application to Hyperspectral Image Analysis', authors=[arxiv.Result.Author('Luciano Carvalho Ayres'), arxiv.Result.Author('Sérgio José Melo de Almeida'), arxiv.Result.Author('José Carlos Moreira Bermudez'), arxiv.Result.Author('Ricardo Augusto Borsoi')], abstract='Hyperspectral image (HI) analysis approaches have recently become\\nincreasingly complex and sophisticated. Recently, the combination of\\nspectral-spatial information and superpixel techniques have addressed some\\nhyperspectral data issues, such as the higher spatial variability of spectral\\nsignatures and dimensionality of the data. However, most existing superpixel\\napproaches do not account for specific HI characteristics resulting from its\\nhigh spectral dimension. In this work, we propose a multiscale superpixel\\nmethod that is computationally efficient for processing hyperspectral data. The\\nSimple Linear Iterative Clustering (SLIC) oversegmentation algorithm, on which\\nthe technique is based, has been extended hierarchically. Using a novel robust\\nhomogeneity testing, the proposed hierarchical approach leads to superpixels of\\nvariable sizes but with higher spectral homogeneity when compared to the\\nclassical SLIC segmentation. For validation, the proposed homogeneity-based\\nhierarchical method was applied as a preprocessing step in the spectral\\nunmixing and classification tasks carried out using, respectively, the\\nMultiscale sparse Unmixing Algorithm (MUA) and the CNN-Enhanced Graph\\nConvolutional Network (CEGCN) methods. Simulation results with both synthetic\\nand real data show that the technique is competitive with state-of-the-art\\nsolutions.', url='http://arxiv.org/abs/2407.15321v1', pdf_path='./papers/2407.15321v1.Hierarchical_Homogeneity_Based_Superpixel_Segmentation__Application_to_Hyperspectral_Image_Analysis.pdf', content='Hierarchical Homogeneity-Based Superpixel Segmentation:\\nApplication to Hyperspectral Image Analysis\\nL. C. Ayresa, S. J. M. de Almeidab, J. C. M. Bermudeza,b and R. A. Borsoic\\naFederal University of Santa Catarina, SC Florian´opolis, Brazil;\\nbCatholic University of Pelotas, RS Pelotas, Brazil;\\ncCentre de Recherche en Automatique de Nancy (CRAN), Universit´e de Lorraine, CNRS,\\nVandoeuvre-l`es-Nancy, France\\nARTICLE HISTORY\\nCompiled July 23, 2024\\nABSTRACT\\nHyperspectral image (HI) analysis approaches have recently become increasingly\\ncomplex and sophisticated. Recently, the combination of spectral-spatial informa-\\ntion and superpixel techniques have addressed some hyperspectral data issues, such\\nas the higher spatial variability of spectral signatures and dimensionality of the\\ndata. However, most existing superpixel approaches do not account for specific HI\\ncharacteristics resulting from its high spectral dimension. In this work, we propose\\na multiscale superpixel method that is computationally efficient for processing hy-\\nperspectral data. The Simple Linear Iterative Clustering (SLIC) oversegmentation\\nalgorithm, on which the technique is based, has been extended hierarchically. Us-\\ning a novel robust homogeneity testing, the proposed hierarchical approach leads to\\nsuperpixels of variable sizes but with higher spectral homogeneity when compared\\nto the classical SLIC segmentation. For validation, the proposed homogeneity-based\\nhierarchical method was applied as a preprocessing step in the spectral unmixing\\nand classification tasks carried out using, respectively, the Multiscale sparse Un-\\nmixing Algorithm (MUA) and the CNN-Enhanced Graph Convolutional Network\\n(CEGCN) methods. Simulation results with both synthetic and real data show that\\nthe technique is competitive with state-of-the-art solutions.\\nKEYWORDS\\nHyperspectral data; superpixels; homogeneity; hierarchical; unmixing;\\nclassification.\\n1.\\nIntroduction\\nSeveral remote sensing applications have successfully used hyperspectral imaging\\n(Schowengerdt 2006). This technology has recently been expanded to a number of\\nfields, including military surveillance and reconnaissance (Shimoni, Haelterman, and\\nPerneel 2019), food quality control (Liu, Pu, and Sun 2017; Pu, Wei, and Sun 2023)\\nand medicine (Halicek et al. 2019; Cihan, Ceylan, and Ornek 2022). Various aspects,\\nsuch as spectral mixing, atmospheric interference and acquisition noise, often make hy-\\nCONTACT: Luciano Carvalho Ayres. Email: lucayress@gmail.com\\nPublished in International Journal of Remote Sensing, DOI: 10.1080/01431161.2024.2384098. Copyright be-\\nlongs to Taylor and Francis.\\n– A preliminary version of this work was presented by Ayres et al. (2021).\\narXiv:2407.15321v1  [eess.IV]  22 Jul 2024\\nperspectral data analysis a complex task that requires sophisticated algorithms. The\\ntypically high spectral resolution of hyperspectral sensors, and the increasing tem-\\nporal resolution required from some applications, lead to hyperspectral data of very\\nhigh dimensionality and size. This requires computationally efficient solutions that can\\nfacilitate the interpretation and exploitation of hyperspectral datasets in multiple ap-\\nplications, such as data fusion, unmixing, classification, target detection, and physical\\nparameter retrieval (Bioucas-dias et al. 2013).\\nRecently, superpixel segmentation has been frequently employed to address some\\nof these problems in HI analysis. Superpixel structures can be adapted in size and\\nshape based on the spatial and spectral information of adjacent pixels in the observed\\nscene. Although originally designed to work with conventional images, the superpixel\\nconcept has been applied to hyperspectral data, and has gained much popularity in HI\\nprocessing research. There is a wide variety of superpixel generation algorithms, and\\nsome works that compare their performances (Achanta et al. 2012; Machairas et al.\\n2015; Wang et al. 2017a; Stutz, Hermans, and Leibe 2018). A survey on the use of\\nsuperpixels as a preprocessing step in HI analysis describes how these segmentation\\nalgorithms have been successfully applied to tasks such as classification, unmixing,\\ndimensionality reduction, and band selection, among others (Subudhi et al. 2021).\\nApproximately 80% of the proposed techniques focused on classification and unmixing\\nproblems, which are briefly reviewed in the following.\\n1.1.\\nHyperspectral unmixing\\nLarge sensor-to-target distances and the low spatial resolution of hyperspectral sensors\\ncause the observed reflectance spectrum of a given HI pixel to frequently represent a\\ncombination of different materials (Bioucas-Dias et al. 2012). The technique of Spectral\\nunmixing involves breaking down the spectrum of a mixed pixel from a HI into a set of\\nendmembers, or spectral signatures of pure materials, and a set of fractional abundances\\nthat show what proportion of the pixel each endmember represents (Keshava and\\nMustard 2002). The majority of approaches to the unmixing problem are based on\\na linear mixing model (LMM) (Keshava and Mustard 2002; Dobigeon et al. 2014),\\nwhich assumes that each HI pixel is constituted by the linear combination of a few\\nendmembers, weighted by their fractional abundances.\\nWhen the endmembers are correctly estimated, the LMM produces fast and accurate\\nunmixing solutions (Bioucas-Dias et al. 2012). However, most algorithms that extract\\nthe endmembers directly from a given HI rely on the presence of pure pixels (i.e.,\\npixels containing only a single material), or on the data not being heavily mixed Ma\\net al. (2013). Sparse regression-based linear unmixing, which gets over this limitation,\\nmakes the assumption that the reflectance of the observed pixels in an HI can be\\ndescribed as a linear combination of a few endmember signatures from a large, a\\npriori known spectral library (Iordache, Bioucas-Dias, and Plaza 2011). The unmixing\\nproblem consists of finding the subset of signatures in the library and their abundances\\nto best represent each observed pixel. Employing spectral libraries avoids the need for\\nestimating the number of endmembers and their spectral signatures. However, using\\nlarge libraries makes the unmixing problem ill-posed, and the solution highly sensitive\\nto noise (Iordache, Plaza, and Bioucas-Dias 2010).\\nTraditional unmixing algorithms process the spectrum of each observed image pixel\\nindependently, therefore ignoring the spatial organisation and the relationship between\\nadjacent pixels (Iordache, Plaza, and Bioucas-Dias 2010; Iordache, Bioucas-Dias, and\\n2\\nPlaza 2014; Wang et al. 2016; Zheng et al. 2016). Integrating spatial-contextual in-\\nformation through regularisers can significantly enhance the performance of sparse\\nunmixing (Iordache, Bioucas-Dias, and Plaza 2012; Zhang et al. 2017; Wang et al.\\n2017b; Zhang et al. 2018; Qi et al. 2019). Moreover, the use of spatial regularisa-\\ntions also brings significant performance improvements to unmixing algorithms that\\nconsider nonidealities such as nonlinear interactions between light and the materi-\\nals in the scene (Dobigeon et al. 2014; Borsoi et al. 2020), and the variability of the\\nendmembers spectra across different pixels due to acquisition or physico-chemical vari-\\nations (Borsoi et al. 2021). In particular, recent works have proposed regularisation\\napproaches that leverage the superpixels decomposition of the HI as a representation\\nof its spatial information. A data-dependent multiscale regularisation model was pro-\\nposed taking into account the spectral variability of the endmembers, where both the\\nabundances and the endmember signatures were constrained to vary smoothly inside\\neach superpixel (Borsoi, Imbiriba, and Bermudez 2020). Superpixels were used in a\\nmultiscale regularisation strategy for nonlinear spectral unmixing based on kernels\\n(Borsoi et al. 2020). As for the sparsity-based regularisation methods using superpix-\\nels, a spatial group-sparsity-regularised non-negative matrix factorisation (NMF) was\\npresented (Wang et al. 2017c), which comprises a spatial group sparsity regularisation\\nconstraint into the NMF-based unmixing problem. A robust generalised bilinear model\\n(GBM) unmixing solution based on superpixels and low-rank representation was pro-\\nposed by Mei et al. (2018). Also, a spatial regularisation technique using superpixels\\nand graph Laplacian regularisation was presented for the sparse unmixing problem\\n(Ince 2020, 2021).\\nA concern about the use of spatial information in the unmixing problem is that it\\nusually increases the computational complexity of the algorithms. To address this lim-\\nitation, a fast sparse unmixing algorithm called Multiscale sparse Unmixing Algorithm\\n(MUA) was proposed to efficiently introduce spatial context in the unmixing problem\\n(Borsoi et al. 2019). Applying the Simple Linear Iterative Clustering (SLIC) (Achanta\\net al. 2012) oversegmentation algorithm, MUA demonstrated its capacity to estimate\\nabundances with a quality that is comparable to the state-of-the-art S2WSU (Zhang\\net al. 2018) (or even superior in noisy circumstances), with significantly lower execu-\\ntion time. A variation of MUA based on an ℓ1/2 norm regularisation has also been\\nproposed by Zou and Lan (2019). A superpixel-based collaborative sparse and low-\\nrank regularisation algorithm was proposed to solve the sparse unmixing (Chen et al.\\n2022). Also, a reweighted low-rank and joint-sparse unmixing method (Zhang, Yuan,\\nand Li 2022) divided the problem into two stages: a superpixel-based rough unmix-\\ning stage, and a fine-tuning unmixing stage. More recently, Ye et al. (2022) combined\\nlow-rank constraints for similar superpixels and total variation sparse unmixing for\\nHIs. A spectral-spatial-sparse unmixing with superpixel oriented graph Laplacian was\\nalso presented by Li et al. (2023b). A superpixel-based multiscale approach was also\\nproposed in (Ayres et al. 2024) for unmixing with endmember bundles and structured\\nsparsity penalties.\\nSuperpixel-based algorithms such as MUA have become a well-established approach\\nto perform unmixing while taking spatial information into account. To obtain signifi-\\ncant performance improvements, these methods, however, heavily rely on two aspects\\nof the oversegmentation results: superpixels must 1) aggregate a large number of pix-\\nels, and 2) have spectrally uniform pixels inside of them, with the exception of noise.\\nHowever, conventional superpixel (or image segmentation) methods (MacQueen 1967;\\nVeganzones et al. 2014; Achanta et al. 2012; Beucher and Meyer 1993; Shi and Ma-\\nlik 2000; Felzenszwalb and Huttenlocher 2004; Levinshtein et al. 2009) aren’t made to\\n3\\nmaximise these requirements. Using superpixel-based unmixing algorithms to HIs with\\nirregular spatial arrangements thus indicated a higher sensitivity to the image content.\\nNevertheless, these algorithms rely strongly on two characteristics of the oversegmen-\\ntation results to achieve meaningful performance improvements: 1) the superpixels\\nmust group a large number of pixels, and 2) the pixels within a superpixel must be\\nspectrally homogeneous, except for the influence of noise. However, traditional super-\\npixel (or image segmentation) algorithms (MacQueen 1967; Veganzones et al. 2014;\\nAchanta et al. 2012; Beucher and Meyer 1993; Shi and Malik 2000; Felzenszwalb and\\nHuttenlocher 2004; Levinshtein et al. 2009) are not designed to optimise these crite-\\nria. Thus, superpixel-based unmixing algorithms revealed a greater sensitivity to the\\nimage content when applied to images with irregular spatial compositions.\\n1.2.\\nHyperspectral image classification\\nThe extensive spectral information available from the many narrow bands recorded\\nby hyperspectral instruments makes it feasible to accurately discriminate between\\ndifferent materials. This information embedded in hyperspectral data allows the char-\\nacterisation and identification of elements on the surface of the observed scene with\\ngreater accuracy and robustness. In HI analysis, the classification task tries to allocate\\na unique class label to each pixel of the image, generating a classification map of the\\nscene as a result (Landgrebe 2002). Nevertheless, classification is not a simple task\\ndue to several factors, such as the existence of redundant features, the quality and\\navailability of training samples, and the high dimensionality of the data.\\nClassification approaches for HIs can be organised according to several criteria (P.\\nGhamisi et al. 2017), such as whether or not labelled training data is required. Su-\\npervised methods classify the input image using training data, while unsupervised\\napproaches do not consider the labels of the training samples. Semi-supervised meth-\\nods, on the other hand, combine a small amount of labelled data with a large amount of\\nunlabelled data to generate a classification map. Various supervised and unsupervised\\ntechniques have been proposed to handle the problem of classifying HIs (P. Ghamisi\\net al. 2017), such as artificial neural networks (Civco 1993; Bischof and Leonardis\\n1998), support vector machines (SVMs) and other kernel-based approaches (Melgani\\nand Bruzzone 2004). The major limitation of supervised strategies is that the training\\nprocess relies strongly on the quality of the labelled dataset. Moreover, the training\\nset is often limited or not available due to the high cost of accurate sample labelling.\\nUnsupervised classification techniques, on the other hand, are based on clustering\\nmethods, such as spectral clustering (Zhao, Yuan, and Wang 2019). In addition, prior\\nknowledge of the scene is needed, as a preliminary preprocessing step is typically\\nperformed to reduce the high dimensionality of the input, such as, e.g., band selec-\\ntion (Yang et al. 2018) or feature extraction using sparse autoencoders (Tao et al.\\n2015). Still, unsupervised methods have shown satisfactory results in HI classification\\ntasks (Zhao, Yuan, and Wang 2019). Semi-supervised classification strategies offer the\\npossibility of exploiting effectively both labelled and unlabelled data, improving per-\\nformance in small sample problems. Several semi-supervised approaches have been\\nused for HSI classification, ranging from graph-based (Kotzagiannidis and Sch¨onlieb\\n2022) to neural-network-based approaches (Cao et al. 2023b,a).\\nTo relieve the shortage of labelled samples and increase the classification accuracy\\nof HIs, Sellars, Aviles-Rivero, and Sch¨onlieb (2020) use SLIC superpixels to generate\\nlocal regions that have a high probability of sharing the same classification label. Then,\\n4\\nspectral and spatial features are extracted from these regions to be used to produce\\na weighted graph representation, where each vertex represents a region (superpixel)\\nrather than a pixel. The graph is then fed into a graph-based semisupervised classifier\\nthat provides the final classification map. In a similar more recent work (Kotzagian-\\nnidis and Sch¨onlieb 2022), graphs are also used to model the semi-supervised classifi-\\ncation problem by propagating labels among nearest neighbours. A superpixel-based\\naugmentation technique was further presented to address the issue of limited labelled\\nsamples, with the purpose of expanding the number of training instances (Li et al.\\n2023a).\\nLike in unmixing, the use of spatial information in addition to spectral properties\\ncan lead to improved HI classification performance (He et al. 2018; Mookambiga and\\nGomathi 2021). Several deep learning algorithms have achieved promising results in\\nHI classification (Audebert, Le Saux, and Lef`evre 2019; Kumar, Singh, and Dua 2022).\\nFor instance, convolutional neural networks (CNNs) have been widely used to extract\\nHI spectral-spatial features. Recently, it has been verified through semi-supervised\\napproaches which represent hyperspectral data in graphs that the spatial contextual\\nstructure of the HI can be better modelled by graph convolutional networks (GCNs)\\n(Qin et al. 2018). However, GCNs are complex structures which require a significant\\ntraining time. The segmentation of an HI using superpixels has been proposed to\\nsignificantly reduce the graph size and thus make GCNs more feasible for HI classifi-\\ncation (Wan et al. 2019; Ding et al. 2022). Despite the fact that such approaches can\\nachieve state-of-the-art performance (Liu et al. 2021), the classification quality is, as\\nin unmixing, directly linked to the quality of the oversegmentation maps.\\nMoreover, we note that superpixels algorithms have also been used to improve\\nother aspects of hyperspectral or multispectral image classification. For instance, in\\n(Dieste, Arg¨uello, and Heras 2023) and (Arg¨uello et al. 2021), the waterpixels algo-\\nrithm (Machairas et al. 2015) was used to reduce the computational load of a classifica-\\ntion process for multispectral images. In the case of (Acci´on, Arg¨uello, and Heras 2020),\\nSLIC was used for data augmentation before the classification of HIs. Note that when\\nsuperpixel segmentation algorithms are used as preprocessing to pixel-wise unmixing\\nor classification tasks (so that only one spectral vector related to each superpixel has\\nto be unmixed/classified), the computational complexity of these downstream tasks\\ncan be reduced by the ratio between the number of superpixels in the decomposition\\nand the total number of pixels in the image.\\n1.3.\\nChallenges, contributions and organisation\\nThe trend towards incorporating spatial information into HI analysis by means of su-\\nperpixels is well-established in the literature. Besides HI classification and unmixing,\\nthis approach has also been applied to other problems in hyperspectral and mul-\\ntispectral imaging such as change detection (Naik et al. 2023). Moreover, different\\nsuperpixel segmentation strategies have been used for HI processing in the literature,\\nincluding SLIC, which is clustering-based, the waterpixels (Machairas et al. 2015),\\nwhich is a gradient based algorithm and presents high adherence to image edges (Di-\\neste, Arg¨uello, and Heras 2023; Arg¨uello et al. 2021), and other algorithms that are\\nbased on the optimisation of carefully constructed energy functions (Yao et al. 2015;\\nVan den Bergh et al. 2012). However, most of these segmentation techniques were not\\noriginally intended for the hyperspectral case. This shows the need and opportunity\\nto develop multiscale representations that meet the specific requirements of problems\\n5\\nsuch as spectral unmixing and classification.\\nIn the HI segmentation process, measures of pixel similarity are usually based on\\nEuclidean distance measurements and spectral angles (Keshava and Mustard 2002)\\nbetween the pixels and the representative vector of its segment. Although such met-\\nrics can be used to identify uniform regions, their effectiveness depends on factors such\\nas parameter initialisation, target application and the nature of the algorithm, e.g.,\\ngraph-based or gradient-ascent-based (Achanta et al. 2012). Most image segmentation\\nalgorithms that have been applied to HIs do not provide guarantees of spectral uni-\\nformity within the regions. This points to the need for an oversegmented image, i.e.,\\nto reduce the area size of the regions attempting a higher level of homogeneity. On\\nthe other hand, oversegmentation algorithms produce superpixels of relatively similar\\nsizes. This can lead to the grouping of different materials within the same region when\\nthe size of the superpixels is large; or to a large number of superpixels of very small\\naverage area. An unnecessary number of superpixels can compromise the benefits of\\ndimensionality reduction provided by image segmentation. Therefore, an important\\nadditional step is to assess the level of homogeneity of the resulting superpixel regions\\nto determine whether the oversegmentation step is satisfactory or if improvements\\nshould be made. Note that although hierarchical superpixel decomposition algorithms\\nhave been proposed, these were aimed at conventional images and are not equipped to\\ndeal with the challenges in segmenting HIs. As proposed by Vasquez and Scharcanski\\n(2018), the representation of digital images is done not only at two scales (original\\nand over-segmented), but at different levels of over-segmentation, in a hierarchical\\nstructure based on graphs. Lower resolution scales are generated by contractions of\\nthe graphs until a desired number of superpixels is obtained. However, for a higher\\nnumber of bands, as in HIs, this technique can become excessively complex compu-\\ntationally, or may not generate good results. Likewise, a hierarchical tree algorithm\\nwas proposed that is capable of generating superpixels of multiple scales but with\\none to two orders of magnitude of speed-up (Wei et al. 2018). An image superpixel\\nsegmentation based on hierarchical multi-level local information was also proposed by\\nDi et al. (2021). However, these methods were aimed at segmenting RGB images.\\nThe homogeneity level of the regions formed in HI segmentation remains relatively\\nunexplored in the literature. A similarity threshold based on the maximum spectral\\nangular distance (SAD) between pixel spectra was used to determine whether they\\nbelong to the same class of material (endmember) (Saranathan and Parente 2016).\\nThe homogeneous regions were then represented by the average spectral signature of\\nthe superpixel for spectral unmixing. The technique can improve the performance of\\nthe original graph-based Felzenszwalb and Huttenlocher (FH) superpixel generation\\nalgorithm (Felzenszwalb and Huttenlocher 2004) in the preprocessing of tasks such\\nas unmixing. However, the clusters formed by this method are not necessarily con-\\ntiguous, and classes that do not correspond to a real endmember can be produced\\nif the similarity threshold is too restrictive. Moreover, the modified FH algorithm is\\ncomputationally costly and does not offer explicit parameters for controlling the size\\nand compactness of the superpixels.\\nThe homogeneity of superpixels generated using the SLIC algorithm for spectral un-\\nmixing was analysed by Yi and Velez-Reyes (2018). After the HI segmentation, each\\nsuperpixel was expressed as a matrix composed of the spectra of their inner pixels.\\nSuperpixels of nearly rank-1 were then considered homogeneous. The average spectral\\nsignature only represents homogeneous superpixels, in contrast to Saranathan and\\nParente (2016). The heterogeneous ones are characterised by the most representative\\npixels according to algorithms that solve a column-subset-selection (CSS) problem. De-\\n6\\nspite providing an enhanced lower-dimensional representation of an HI, this procedure\\ndoes not increase the level of homogeneity of the superpixels defined using SLIC. A hi-\\nerarchical segmentation algorithm called Binary Partition Tree (BPT) was modified to\\nhandle the high spectral information contained in an HI aiming at spectral unmixing\\nquality in terms of spectral reconstruction error (Veganzones et al. 2014). Although\\nadapted for unmixing, experiments reported by Borsoi et al. (2019) showed that BPT\\noften creates clusters of pixels of considerably different sizes, corresponding to both\\nsmall and large objects in the scene. While this makes it possible to represent large\\nregions with homogeneous abundance characteristics without compromising smaller\\nobjects, it can lead to the clustering of pixels that share different abundance char-\\nacteristics. When applied to the unmixing process, this segmentation map generated\\nresults that were very sensitive to the image content. Despite generating regions with\\nsimilar sizes, the SLIC algorithm was able to mitigate these limitations by generating\\na larger number of regions, what improved the performance of unmixing. Furthermore,\\nthe lack of homogeneity guarantees in existing superpixels algorithms also constitutes\\na challenge for superpixel-based HI classification methods (Tu et al. 2018). Though\\na few works have proposed superpixel methods that are trainable (Jampani et al.\\n2018) or adapted to ignore textured image content (Yuan et al. 2021), learning-based\\napproaches require training data and existing methods are not adequate to process\\nHIs.\\nThis paper proposes a novel hierarchical homogeneity-based oversegmentation\\nmethod adapted for hyperspectral image analysis problems such as unmixing and\\nclassification. The presented approach is based on a reliable superpixel homogene-\\nity test and the SLIC oversegmentation algorithm. The Euclidean distance between\\na superpixel’s median vector and each of its pixels is used in this novel superpixel\\nhomogeneity assessment to mitigate the effect of potential outliers on the test result.\\nThose superpixels classified as non-homogeneous are subdivided into smaller regions\\nby successive rounds of oversegmentation. This leads to superpixels that are less likely\\nto group pixels from different material compositions or classes, while mitigating the\\ninfluence of noise and outliers. The proposed oversegmentation method is then applied\\nto HI unmixing and to HI classification problems. The proposed method is referred to\\nas Hierarchical Homogeneity-Based Oversegmentation (H2BO).\\nIn unmixing, the proposed method is used in the first abundance estimation step\\n(coarse domain) of MUA (Borsoi et al. 2019). This results in the so-called MUAH2BO\\nalgorithm, which leads to a multiscale representation of an HI that improves the effi-\\nciency of sparse unmixing. The technique enhances the sparse unmixing performance of\\nthe previous MUASLIC for HIs with irregular spatial compositions by more accurately\\ndescribing the spatial organisation of the abundances in an HI with content spread\\nin regions of different sizes and shapes. For the HI classification problem, we propose\\nto replace the application of the linear discriminant analysis (LDA) technique in the\\ndimensionality reduction step of the CEGCN algorithm (Liu et al. 2021). Our hierarchi-\\ncal oversegmentation method is then applied in place of SLIC in the superpixel-based\\ngraph constructor (SGC) that defines the graph encoder and decoder of the CEGCN\\nalgorithm. The resulting method is termed CEGCNH2BO.\\nIt is important to notice that the proposed hierarchical representation based on\\nsuperpixel homogeneity assessment can be used as a preprocessing step in other\\nsuperpixel-based algorithms besides MUA and CEGCN.\\nThe main contributions of the paper are:\\n• We propose a new, computationally efficient hierarchical superpixel method\\n7\\nadapted to hyperspectral imaging tasks such as classification and unmixing.\\n• To account for peculiarities of hyperspectral data, such as noise and outliers, a\\nnew robust homogeneity test is devised.\\n• The performance of the method is demonstrated with two hyperspectral image\\nanalysis tasks (unmixing and classification).\\nThe paper is organised as follows. Section 2 introduces the design of the proposed\\nmultiscale representation of the hyperspectral data. Section 3 presents a brief expla-\\nnation for the sparse unmixing problem and its use as a MUA, as well as experimental\\nresults and discussion with synthetic and real data. Section 4 brings the application\\nof the proposed technique to the classification task by adapting the CEGCN method.\\nThe conclusions are presented in Section 5.\\n2.\\nProposed Hierarchical Homogeneity-Based Oversegmentation\\nThe results of using image segmentation to introduce spatial information in hyper-\\nspectral image analysis strongly depend on the image pixels being adequately grouped\\ninto spectrally homogeneous regions. Existing oversegmentation algorithms usually do\\nnot guarantee a decomposition with an appropriate homogeneity level, particularly in\\nlow signal-to-noise ratio (SNR) scenarios. To address this limitation, we propose a new\\nmultiscale image segmentation algorithm specifically designed for hyperspectral image\\nanalysis problems, such as sparse unmixing and classification.\\nNote that the content in an HI can be distributed in regions of irregular sizes and\\nshapes. Hence, characterising the spatial organisation of the endmembers in a scene\\nmay not be suitable when adopting an oversegmentation approach with a single av-\\nerage superpixel size for the entire region. Large (small) superpixels should be used\\nto segment large (small) patterns. For this, we suggest gradually oversegmenting the\\nHI in a number of scales. Initially, use large superpixels and an appropriate metric\\nto evaluate the homogeneity of these regions following each oversegmentation stage.\\nThereafter, more cycles of oversegmentation with gradually smaller average super-\\npixel sizes are applied to the regions designated as non-homogeneous. In this manner,\\na sufficient amount of homogeneous superpixels that can adapt to the various pattern\\nshapes in the HI can be produced. Beginning from the observed HI Y ∈RL×N with L\\nbands and N pixels, to reach a predetermined level of homogeneity, we consider grad-\\nually decomposing its non-homogeneous superpixels (up to R representation scales).\\nMoreover, to adequately account for the high dimensionality of hyperspectral data, we\\npropose a new robust homogeneity test.\\nFigure 1 shows the SLIC method to decompose an HI on multiple scales for R = 2.\\nThe superpixels identified as non-homogeneous (homogeneous) are represented by the\\ngrey regions with red dots (white regions with blue dots). Non-homogeneous super-\\npixels in scale r are divided into homogeneous (non-homogeneous) superpixels in scale\\nr +1 indicated by blue (red) lines. The steps of the proposed hierarchical oversegmen-\\ntation algorithm are detailed in the following.\\nStep 1 – Initial image oversegmentation\\nThe SLIC algorithm forms a segmentation map that subdivides the image into K0\\nsuperpixels with average region size determined by parameter σ0 =\\np\\nN/K0 and\\n8\\nFigure 1.\\nScheme of proposed method procedure with SLIC for R = 2. The superpixels identified as non-\\nhomogeneous (homogeneous) are represented by the grey regions with red dots (white regions with blue dots).\\nNon-homogeneous superpixels in scale r are divided into homogeneous (non-homogeneous) superpixels in scale\\nr + 1 indicated by blue (red) lines.\\ncompactness controlled by parameter γ (Achanta et al. 2012)1. Consider the matrix\\nSr,k ∈RL×|Br,k| whose columns constitute the pixels contained in superpixel k ∈\\n{1, 2, . . . , Kr} in a scale of representation r ∈{0, 1, . . . , R}. The set Br,k contains the\\nindexes In, n = 1, 2, . . . , |Br,k| (| · | denoting set cardinality) of each pixel in the kth\\nsuperpixel at the rth representation scale. Then,\\nSr,k =\\n\\x02\\nyI1, yI2, . . . , yI|Br,k|\\n\\x03\\n,\\n(1)\\nwhere yi is the ith column of the original image Y and {I1, I2, . . . , I|Br,k|}\\n= Br,k.\\nWe also define by Wr ∈RN×Kr an image coarsening transformation promoted by the\\nsegmentation map Sr,k at the rth representation level, for r ∈{0, . . . , R}.\\nThe transformation Wr computes the average of all HI pixels inside each of the\\nsuperpixels at the scale r. At the final decomposition scale (i.e., the smallest r ≤R such\\nthat all superpixels are homogeneous) this transformation is denoted by WF ∈RN×KF\\nand contains KF < N superpixels. It defines a coarse-scale spatial decomposition which\\n1σ0 and γ correspond to parameters S and m, in the original reference, respectively.\\n9\\ncan be applied to the HI as:\\nYC = YWF\\n(2)\\nwhere YC ∈RL×KF , and the C subscript refers to the new approximate (coarse) domain\\nof the image. The relation KF < N means that there are lesser superpixels in YC than\\npixels in Y. Columns of YC are the average of all pixels in each superpixel.\\nThe coarse image YC can also be represented in the original image domain (i.e.,\\na regular spatial grid with N pixels), denoted by D. We consider a representation in\\nwhich all pixels inside each superpixel share the same spectrum, which can be useful to\\nhyperspectral image analysis tasks. To this end, consider a conjugate transformation\\nby a matrix W∗\\nF ∈RKF ×N, represented by YD = YCW∗\\nF ∈RL×N. Operation YCW∗\\nF\\napplies the average reflectance value of each superpixel in YC to every pixel in the\\ncorresponding superpixel region in the original domain. An example of the computation\\nof W∗\\nF for an image with 5 pixels and 3 superpixels (N = 5 and K = 3), where pixels\\n1 and 2 belong to superpixel 1, pixel 3 belongs to superpixel 3 and pixels 4 and 5\\nbelong to superpixel 2 is shown in (3).\\n\\uf8ee\\n\\uf8f0\\n|\\n|\\n|\\n|\\n|\\nyC1\\nyC1\\nyC3\\nyC2\\nyC2\\n|\\n|\\n|\\n|\\n|\\n\\uf8f9\\n\\uf8fb\\n|\\n{z\\n}\\nYD\\n=\\n\\uf8ee\\n\\uf8f0\\n|\\n|\\n|\\nyC1\\nyC2\\nyC3\\n|\\n|\\n|\\n\\uf8f9\\n\\uf8fb\\n|\\n{z\\n}\\nYC\\n·\\n\\uf8ee\\n\\uf8f0\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n0\\n0\\n1\\n0\\n0\\n\\uf8f9\\n\\uf8fb\\n|\\n{z\\n}\\nW∗\\nF\\n(3)\\nStep 2 – Superpixels homogeneity test\\nA crucial step of the proposed strategy is deciding whether or not a superpixel is\\nhomogeneous. This is not straightforward, since HIs can be contaminated by noise\\nand outliers. To address this challenge, we propose a new robust homogeneity test.\\nLet mk be the median of the pixels in Sr,k, which belong to the kth superpixel at a\\ngiven representation scale r:\\nmk =\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nm1\\n...\\nmL\\n\\uf8f9\\n\\uf8fa\\uf8fb=\\n\\uf8ee\\n\\uf8ef\\uf8f0\\nmed\\n\\x00[y1,I1, . . . , y1,I|Br,k|]\\n\\x01\\n...\\nmed\\n\\x00[yL,I1, . . . , yL,I|Br,k|]\\n\\x01\\n\\uf8f9\\n\\uf8fa\\uf8fb,\\n(4)\\nwhere yℓ,i is the ℓth band of yi. Define dk as the vector of Euclidean distances between\\neach pixel yIn and mk:\\ndk =\\n\\x02\\nd1, d2, . . . , d|Br,k|\\n\\x03⊤,\\ndn = ∥mk −yIn∥2 .\\n(5)\\nWhile evaluating homogeneity, it is important to keep in mind that even for fully\\nhomogeneous superpixels, the values in dk are typically constrained away from zero\\ndue to the influence of measurement perturbations in Y. Also, a small number of\\noutliers may distort the estimate significantly. To solve these issues, we first clear out\\npotential outliers by eliminating a part of the highest values from dk as τoutliers. This\\nresults in a reduced distance vector d′\\nk ∈R⌊D⌋with D = (1 −τoutliers)|Br,k|, ⌊·⌋being\\nthe floor function. The homogeneity measure δk is then established as the deviation\\n10\\nbetween the maximum distance max(d′\\nk) found in the kth superpixel after removing\\nthe outliers, with respect to the average d′\\nk of its distances:\\nδk = max(d′\\nk) −d′\\nk\\nd′\\nk\\n,\\nHomogeneous: δk ≤τhomog .\\n(6)\\nIf δk is less than a reasonable threshold, superpixels are considered homogeneous\\n(τhomog). The percentage of homogeneous superpixels in the rth scale with Kr su-\\nperpixels is defined by ηr = (Khomog/Kr) × 100%, where Khomog is the amount of\\nsuperpixels classified as homogeneous.\\nStep 3 – Subdivision of non-homogeneous superpixels\\nAreas identified in Step 2 as non-homogeneous are subjected to a second oversegmen-\\ntation phase with a reduced average region size parameter σr < σr−1, ∀r > 0. This\\nprocess is repeated for r = 1, . . . , R, or until ηr = 100% is attained. The segmen-\\ntation scale for which the algorithm executes the subdivision of non-homogeneous\\nsuperpixels for the final time will be denoted by r′ ≤R. This produces a series\\nof segmentation maps with increasing spatial definition and superpixel homogeneity:\\nS1,k, S2,k, . . . , Sr′,k. The hierarchical oversegmentation related to Sr′,k yields the final\\n(F subscript) spatial transformation operator WF . A pseudocode for the hierarchical\\nhomogeneity-based oversegmentation method is presented in Algorithm 1.\\nAlgorithm 1: Hierarchical Homogeneity-Based Oversegmentation (H2BO)\\nInput: hyperspectral image Y, parameters γ, σ0, σ1, . . ., σR, τoutliers, τhomog.\\n1 W0, {S0,k}k ←initial oversegmentation of Y;\\n2 WF ←W0;\\n3 η0 ←homogeneity test of {S0,k}k;\\n4 for r = 1 to R do\\n5\\nif ηr−1 < 100% then\\n6\\nWr, {Sr,k}k ←oversegmentation of non-homogeneous superpixels of\\n{Sr−1,k}k with σr < σr−1;\\n7\\nηr ←homogeneity test of {Sr,k}k ;\\n8\\nWF ←Wr;\\n9\\nend\\n10 end\\n11 return the final transformation matrix WF ;\\n2.1.\\nParameter selection\\nThe proposed H2BO algorithm requires several parameters to be selected. However,\\nchoosing most of them is straightforward. The number of scales (R) determines the\\nnumber of hierarchical levels used in the segmentation. While a higher R value leads\\nto more refined segmentations, it also increases the computation time. In practice, due\\nto the rapid homogenization of superpixels within a few iterations, small values such\\n11\\nas R = 3 are often sufficient. The σ0, . . . , σR values represent the size of superpix-\\nels at each decomposition level, with σi > σi+1. These values are not overly critical\\nto the overall performance. However, it’s important to avoid excessively small initial\\nsuperpixel sizes (σ0). The rationale behind this is that the algorithm relies on the re-\\nsegmentation of large, non-homogeneous superpixels. The outlier threshold (τoutliers)\\nis used to remove outliers from the computation of the metrics employed during seg-\\nmentation. This parameter is typically set between 0 and 100%, with values around\\n10% being sufficient in most cases. τoutliers exhibits low sensitivity to specific settings.\\nThe homogeneity threshold (τhomog) plays a crucial role in determining whether a\\nsuperpixel is deemed homogeneous. It is based on the relative one-sided dispersion\\nof the values in d′\\nk relative to the median. Due to its dependence on image content,\\nthe optimal value for τhomog can vary. For homogeneous HSIs, values between 20 and\\n50% are often adequate. Conversely, highly textured HSIs, such as those containing\\nvegetation or significant noise (common in many real-world HSIs), may require higher\\nthresholds ranging from 100 to 200%.\\n3.\\nApplication to spectral unmixing\\nIn this section, we illustrate the performance of the proposed homogeneity-based over-\\nsegmentation method with the spectral unmixing problem. To this end, we consider\\nthe MUA unmixing algorithm (Borsoi et al. 2019) due to its good trade-off between\\nlow computational complexity and good unmixing performance. Moreover, MUA only\\ntakes spatial contextual information into account through the HI oversegmentation\\nresults, which makes evaluating the quality of H2BO’s results easier.\\nThe MUA algorithm solves the spectral unmixing problem by sparse linear regres-\\nsion. Consider the LMM (Bioucas-Dias et al. 2012) of an observed HI Y ∈RL×N with\\nL bands and N pixels as Y = AX + N, where abundances X ∈RP×N are subject to\\nnonnegativity constraints. Matrix A ∈RL×P denotes a spectral library containing P\\nendmember signatures and N ∈RL×N represents the modelling errors and additive\\nnoise. In this case, the problem is to estimate the abundance matrix bX that best rep-\\nresents each pixel in Y as a linear combination of a small subset of the signatures in\\nthe library A. Using image segmentation and superpixel techniques (MacQueen 1967;\\nVeganzones et al. 2014; Achanta et al. 2012), MUA divides the sparse unmixing opti-\\nmisation problem into two spatial domains or scales: one with the actual image (D)\\nand another with its coarse (C) representation composed by the average of the pixels\\nin each superpixel. Prior to solving the optimisation issue at the original scale, spectral\\nunmixing is carried out in the coarse domain to produce initial abundance estimates\\nbXC, which are then utilised to regularise the optimisation problem to be solved in the\\noriginal scale, resulting in the final estimated abundance matrix bX. MUA uses a spatial\\ntransformation represented by W, which is defined analogously to Wr in Section 2.\\nIn Borsoi et al. (2019), this transformation was constructed from a modification of the\\nBPT (Veganzones et al. 2014), k-means (MacQueen 1967) or SLIC (Achanta et al.\\n2012) algorithms.\\n3.1.\\nExperimental setup\\nWe substituted the spatial transformation operator W of the original MUA with the\\nWF operator resulting from the homogeneity-based hierarchical method proposed in\\nSection 2, obtaining the MUAH2BO algorithm. We compared the proposed MUAH2BO\\n12\\nto MUASLIC, since the segmentation generated by the SLIC algorithm led to the best\\nexperimental results of the original MUA (Borsoi et al. 2019). As an alternative to\\nSLIC, the waterpixels (Machairas et al. 2015) algorithm (referred to in this paper\\nwith the subscript “WPX”) for generating superpixels was also used with MUA in\\nthe experiments, denoted by MUAWPX. We used also the results from the S2WSU\\nalgorithm (Zhang et al. 2018) as a baseline. We evaluated the algorithms in terms\\nof abundance estimation quality and computational complexity using both synthetic\\nand real hyperspectral datasets. Note that the choice of the MUASLIC and S2WSU\\nalgorithms is also justified by the fact that they have already shown better perfor-\\nmances compared to others of the same class, e.g., SUnSAL (Iordache, Plaza, and\\nBioucas-Dias 2010), SUnSAL-TV (Iordache, Bioucas-Dias, and Plaza 2012), DRSU\\n(Wang et al. 2016) and DRSU-TV (Wang et al. 2017b).\\nIn the simulations, the SLIC oversegmentation was implemented in the VLFeat\\ntoolbox (Vedaldi and Soatto 2008), which permits its use for multichannel data. The\\nSLIC method uses two parameters: σ, which defines the average size of the superpixels\\n(\\np\\nN/K), and γ, which establishes the importance of the spatial component in the\\npixel measure of similarity (Achanta et al. 2012). For the waterpixels technique, the two\\nrelevant parameters are σ, which denotes an initial grid step that in turn determines\\nthe number of superpixels; and the spatial regulariser κ, which operates in a similar\\nway to SLIC’s γ parameter. The watepixels algorithm also requires the computation\\nof the morphological gradient of the image, which we computed by averaging the\\nmorphological gradient at each band of the HI as described in (Noyel, Angulo, and\\nJeulin 2020).\\nThe algorithms were conducted in MATLABTM, on a workstation equipped with a\\nIntel Core i7 3537U @ 2.00GHz processor and 8GB RAM. The codes developed for the\\nproposed H2BO technique are available at https://github.com/lucayress/H2BO.\\n3.2.\\nSimulation results with synthetic data\\nThree synthetic HIs (DC1, DC2 and DC3), represented in Figure 2 with 100 × 100\\npixels, were created using nine endmembers chosen from a library A ∈R224×240\\nformed from a subset of 240 endmember signatures from the USGS splib06 2 library.\\nThis allowed the results to be compared to a known reference (ground-truth). The\\nHyperspectral Imagery Synthesis3 tool was used to produce several spatially corre-\\nlated abundance patterns for the data cubes in order to evaluate the methodolo-\\ngies in various scenarios. DC1 consists of medium and large, uniform areas; DC2\\nconsists of regions with irregular shapes and sizes; and DC3 is a combination of\\nfour images, each 50 × 50 pixels, with various arrangements. In order to get SNRs\\nof 20 and 30 dB, white Gaussian noise was incorporated to the generated images\\n(SNR = 10 log10(E{∥AX∥2\\nF }/E{∥N∥2\\nF })). As a quantitative criterion of the unmixing\\nperformance, we used the signal-to-reconstruction error (Iordache, Bioucas-Dias, and\\nPlaza 2012), which assesses the estimation of the abundances,\\nSRE (dB) = 10 log10\\n \\n∥X∥2\\nF\\n∥X −bX∥2\\nF\\n!\\n.\\n(7)\\nAll results are based on optimal parameter values obtained for each HI and shown\\n2Available online at https://www.usgs.gov/labs/spec-lab/capabilities/spectral-library.\\n3Available online at http://www.ehu.eus/ccwintco.\\n13\\nin Table 1. A grid search was carried out in the following ranges to determine them:\\nMultiscale representation – rounds R = 3, regulariser γ ∈{0.00025, 0.00125, . . . , 0.1},\\nsuperpixels size σ0, σ1, σ2, σ3, ∈{5, 6, . . . , 14}, considering σ0 > σ1 > σ2 > σ3 and\\nthresholds τoutliers ∈{10%, 20%, 30%}, τhomog ∈{10%, 20%, . . . , 60%}; Waterpixels\\nrepresentation – σ ∈{6, . . . , 20} and κ ∈{0.0001, 0.001 . . . , 1}; Sparse unmixing –\\nregularisation parameters of MUA and S2WSU, λC, λ and λS2WSU, were varied ac-\\ncording to the values 1, 3, 5, 7, 9 × 10i, for i ∈{−3, −2, −1, 0} and β in 1, 3, 5 × 10j, for\\nj ∈{−1, 0, 1, 2}. In all cases removing only 10% of the highest values from dk in (5)\\n(τoutliers = 0.1) enough to minimise outliers in homogeneity tests. Moreover, Table 2\\nshows that each scene’s homogeneous superpixel percentage increased from the first\\n(η0) to the last (ηF ) oversegmentation round, especially for DC2 and DC3.\\n(a) DC1\\n(b) DC2\\n(c) DC3\\nFigure 2. RGB representation of the synthetic HIs.\\nTable 1.\\nParameters of the proposed algorithm for each synthetic data.\\nAlgorithm\\nParameters\\nDC1\\nDC2\\nDC3\\n30 dB\\n20 dB\\n30 dB\\n20 dB\\n30 dB\\n20 dB\\nH2BO\\nγ\\n0.00425\\n0.00425\\n0.00025\\n0.00025\\n0.00225\\n0.00225\\nσ0\\n8\\n12\\n6\\n7\\n7\\n8\\nσ1\\n7\\n6\\n5\\n6\\n6\\n7\\nσ2\\n3\\n3\\n4\\n4\\n4\\n4\\nσ3\\n2\\n2\\n2\\n2\\n2\\n3\\nτ outliers\\n10%\\n10%\\n10%\\n10%\\n10%\\n10%\\nτ homog\\n50%\\n20%\\n20%\\n20%\\n30%\\n20%\\nMUA\\nλC\\n0.003\\n0.007\\n0.003\\n0.007\\n0.005\\n0.01\\nλ\\n0.03\\n0.1\\n0.03\\n0.1\\n0.05\\n0.1\\nβ\\n3\\n10\\n3\\n3\\n1\\n1\\nMUAWPX\\nk\\n0.01\\n0.01\\n0.1\\n0.1\\n0.1\\n0.1\\nσ\\n10\\n10\\n8\\n8\\n8\\n8\\nλC\\n0.003\\n0.001\\n0.001\\n0.001\\n0.003\\n0.001\\nλ\\n0.1\\n0.07\\n0.1\\n0.1\\n0.1\\n0.1\\nβ\\n1\\n1\\n1\\n1\\n1\\n1\\nFigure 3 illustrates the comparison between the H2BO results for the initial and\\nfinal oversegmentation scales, as well as their respective homogeneity maps and the\\nhistogram related to the superpixels δk deviation in DC3, 30 dB. The decrease in grey\\nregions (non-homogeneous superpixels) on the final homogeneity map clearly shows\\nthe improvement in the overall spectral homogeneity of the HI regions produced by\\nhierarchical oversegmentation with different superpixel sizes. The histograms explain\\nthis evolution in terms of δk values. In order to get a high ηF index and a suitable\\nunmixing outcome without compromising the method’s reduced computational cost,\\n14\\n0\\n50\\n100\\n150\\n200\\n0 (%)\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\nsuperpixels\\n(d) Superpixels.\\n(e) Homogeneity map.\\n0\\n50\\n100\\n150\\n200\\nF (%)\\n0\\n50\\n100\\n150\\n200\\n250\\n300\\nsuperpixels\\n(f) Superpixels δk deviation.\\nFigure 3.\\nComparison between initial (top row) and final (bottom row) oversegmentation and superpixels\\nhomogeneity results for H2BO in DC3, SNR 30 dB.\\njust three rounds of oversegmentation (R = 3) were required. Low values of η0 result\\nin longer execution durations since more superpixel homogeneity evaluations, subdi-\\nvisions, and unmixing are necessary.\\nThe amount of superpixels used with the original MUASLIC, MUAWPX and the\\nMUAH2BO are displayed in Table 2. In more uniform circumstances, like DC1, the\\nproposed technique was effective at lowering the required number of superpixels. Also,\\nthe presented approach’s final oversegmentation resulted in images that contained a\\nsignificantly higher percentage of homogeneous superpixels. Although MUAWPX gen-\\nerated the smallest number of superpixels, it was the method that showed the lowest\\nhomogeneity percentages, according to the criteria proposed in this work. However, we\\nemphasize that the parameters of the waterpixels algorithm were adjusted maximize\\nthe unmixing performance, and not the homogeneity of its superpixels. The SRE per-\\nformance of the algorithms is displayed in Table 3 in comparison to the optimal value.\\nIn a noisy environment, MUAH2BO produced the best quantitative results (SNR 20\\ndB). When compared to MUASLIC, DC3 showed a considerable improvement, with an\\napproximately 5% boost in SRE for conditions with 20 and 30 dB SNR. These results\\nsuggest that MUAH2BO tends to work better when the spatial content of abundances\\nvaries throughout the scene. The MUAWPX showed the lowest SRE results, particu-\\nlarly for DC3, which illustrates its limitation in segmenting regions with heterogeneous\\nspatial sizes. Notwithstanding the additional stages of homogeneity assessment and\\nfurther oversegmentations, the execution time of the suggested MUAH2BO, as shown\\nin Table 4, was comparable to that of MUASLIC and much slower than that of S2WSU.\\nThe execution times of MUASLIC and MUAWPX were virtually the same. The maps\\nof endmember 3 of DC2’s true and reconstructed abundances are shown in Figure 4.\\nWith a high SNR (30 dB), S2WSU’s abundance estimates are visually the most accu-\\nrate, but MUAH2BO and MUASLIC’s results were more comparable. The outcomes by\\n15\\nTable 2.\\nNumber of generated superpixels.\\nData\\nSNR\\nMUASLIC\\nMUAWPX\\nMUAH2BO\\nsuperpixels\\nη ≡η0\\nsuperpixels\\nη ≡ηF\\nsuperpixels\\nη ≡ηF\\nDC1\\n30 dB\\n225\\n89%\\n42\\n14%\\n218\\n99%\\n20 dB\\n169\\n97%\\n42\\n76%\\n218\\n99%\\nDC2\\n30 dB\\n625\\n69%\\n60\\n18%\\n1018\\n90%\\n20 dB\\n289\\n81%\\n60\\n8%\\n444\\n94%\\nDC3\\n30 dB\\n624\\n55%\\n60\\n13%\\n1566\\n84%\\n20 dB\\n625\\n80%\\n60\\n36%\\n610\\n84%\\nTable 3.\\nSRE results.\\nData\\nSNR\\nS2WSU\\nMUASLIC\\nMUAWPX\\nMUAH2BO\\nDC1\\n30 dB\\n21.668 dB\\n18.117 dB\\n14.41 dB\\n18.339 dB\\n20 dB\\n9.332 dB\\n14.854 dB\\n11.93 dB\\n15.104 dB\\nDC2\\n30 dB\\n18.741 dB\\n11.737 dB\\n6.18 dB\\n11.780 dB\\n20 dB\\n5.689 dB\\n8.416 dB\\n5.30 dB\\n8.561 dB\\nDC3\\n30 dB\\n19.798 dB\\n10.841 dB\\n5.15 dB\\n11.398 dB\\n20 dB\\n6.899 dB\\n7.776 dB\\n4.36 dB\\n8.185 dB\\nTable 4.\\nAverage execution times.\\nData\\nSNR\\nS2WSU\\nMUASLIC\\nMUAWPX\\nMUAH2BO\\nDC1\\n30 dB\\n239 s\\n9 s\\n9 s\\n11 s\\n20 dB\\n235 s\\n15 s\\n15 s\\n16 s\\nDC2\\n30 dB\\n233 s\\n10 s\\n10 s\\n16 s\\n20 dB\\n232 s\\n10 s\\n10 s\\n16 s\\nDC3\\n30 dB\\n233 s\\n12 s\\n12 s\\n21 s\\n20 dB\\n232 s\\n12 s\\n12 s\\n15 s\\nSNR: 30 dB\\n0\\n0.5\\n1\\n(a) Ground-truth\\nSNR: 20 dB\\n(b) MUASLIC\\n(c) MUAWPX\\n(d) MUAH2BO\\n(d) S²WSU\\n0\\n0.5\\n1\\nFigure 4. Abundance estimation results for endmember 3 of DC2.\\nMUAH2BO indicate a clear improvement over MUASLIC, which illustrates its effective-\\nness for noisier images because of its robust homogeneity assessment. Nevertheless, for\\nan SNR of 20 dB, the performance of S2WSU dramatically declines.\\nSensitivity analysis – Now, in the unmixing of the DC1, DC2 and DC3 data,\\nunder the 20 dB and 30 dB SNR scenarios, the sensitivity of the SRE result to the\\nadjustment of the H2BO input parameters is analysed. The plots in Figure 5 illustrate\\n16\\nthe change of the method’s SRE as a function of a wide range of values for each\\nindividual parameter with the other parameters set to their ideal values (Table 1).\\nSince they follow a nearly uniform trend and are constrained to the order σ1 > σ2 > σ3,\\nthe optimal values of σ1, σ2 and σ3 were left unchanged. As can be seen, for a stable\\nrange of γ and σ0, the SRE value essentially remains steady. If γ > 0.1, the reductions\\nare the greatest. This results from the disproportionate preference for spatial regularity\\nover spectral regularity during superpixel generation. H2BO makes up for variations\\nin σ0 with additional rounds of oversegmentation using smaller superpixel sizes. The\\nimproved SRE produced by employing τoutliers = 10% as opposed to τoutliers = 0%, with\\ndifferences varied between 0.5 dB and 3 dB, shows how the outliers removal technique\\nin the homogeneity assessment phase improves the quality of spectral unmixing.\\n10-6\\n10-5\\n10-4\\n10-3\\n10-2\\n10-1\\n100\\n0\\n5\\n10\\n15\\n20\\nSRE (dB)\\nDC1 20dB\\nDC2 20dB\\nDC3 20dB\\nDC1 30dB\\nDC2 30dB\\nDC3 30dB\\n6\\n7\\n8\\n9\\n10 11 12 13 14 15 16 17 18 19 20\\n0\\n0\\n5\\n10\\n15\\n20\\nSRE (dB)\\n0\\n10\\n20\\n30\\n40\\n50\\noutilers (%)\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\nSRE (dB)\\n0\\n20\\n40\\n60\\n80\\n100\\nhomog (%)\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20\\nSRE (dB)\\nFigure 5. Variation of the SRE results due to changes in each parameter individually.\\nStatistical performance analysis – As the MUAH2BO has several parameters to\\nbe set, we evaluated its performance when the parameters are misspecified. Specifi-\\ncally, we evaluated how much the average performance of MUAH2BO decreased when\\ncompared to its optimal value when the parameters were selected randomly within\\nsome intervals that were empirically selected. MUAH2BO was executed 500 times\\nfor each image. Through a uniform distribution, in all executions a new random\\nvalue for each parameter was chosen according to the following intervals: regulariser\\nγ ∈[0, 001, 0, 02], superpixels size σ0 ∈[5, 20], e thresholds τoutliers ∈[10%, 20%] and\\nτhomog ∈[10%, 50%] – regularisers λC ∈[0, 001, 0, 009], λ ∈[0, 01, 0, 9], β ∈[1, 50].\\nFor σ1, σ2 and σ3 the following relationship was used: σi ∈[ σi−1\\n2 , σi−1 −1], rounded\\nup to the upper integer and σ2 and σ3 limited to 3 and 2, respectively. After spectral\\nunmixing, the deviation between the SRE value obtained in each execution and the\\noptimal SRE value in Table 3 was calculated as:\\nSRE −SREoptimal\\nSREoptimal\\n× 100% .\\n(8)\\n17\\nFigure 6 shows the result of the variations. Through these histograms, it is possible\\nto perceive an average decrease in performance between 10% and 20% and a low stan-\\ndard deviation, around 6%. This shows that the probability of obtaining good spectral\\nunmixing results is high for parameter values randomly chosen within a reasonable\\nrange.\\nSNR = 30 dB\\nminimum:  -44.22% \\nmaximum: -0.30% \\nmean:        -10.92% \\nvariance:   0.36% \\nstd. dev.:   6.01%\\n-100\\n-80\\n-60\\n-40\\n-20\\n0\\ndeviation (%)\\n0\\n50\\n100\\n150\\n200\\n250\\n300\\n350\\nexecutions\\nSNR = 30 dB\\nminimum:  -39.45% \\nmaximum: -5.64% \\nmean:        -18.21% \\nvariance:   0.38% \\nstd. dev.:   6.19%\\n-100\\n-80\\n-60\\n-40\\n-20\\n0\\ndeviation (%)\\n0\\n50\\n100\\n150\\n200\\n250\\nexecutions\\nSNR = 30 dB\\nminimum:  -41.81% \\nmaximum: -4.16% \\nmean:        -17.35% \\nvariance:   0.52% \\nstd. dev.:   7.23%\\n-100\\n-80\\n-60\\n-40\\n-20\\n0\\ndeviation (%)\\n0\\n50\\n100\\n150\\n200\\nexecutions\\nSNR = 20 dB\\nminimum:  -38.17% \\nmaximum: -0.96% \\nmean:        -9.45% \\nvariance:   0.45% \\nstd. dev.:   6.74%\\n-100\\n-80\\n-60\\n-40\\n-20\\n0\\ndeviation (%)\\n0\\n50\\n100\\n150\\n200\\n250\\nexecutions\\nSNR = 20 dB\\nminimum:  -32.99% \\nmaximum: -1.73% \\nmean:        -12.67% \\nvariance:   0.26% \\nstd. dev.:   5.05%\\n-100\\n-80\\n-60\\n-40\\n-20\\n0\\ndeviation (%)\\n0\\n50\\n100\\n150\\n200\\n250\\n300\\n350\\nexecutions\\nSNR = 20 dB\\nminimum:  -37.52% \\nmaximum: -3.77% \\nmean:        -19.28% \\nvariance:   0.49% \\nstd. dev.:   6.97%\\n-100\\n-80\\n-60\\n-40\\n-20\\n0\\ndeviation (%)\\n0\\n50\\n100\\n150\\n200\\n250\\nexecutions\\nFigure 6.\\nDeviation from the optimal SRE value of the proposed method for randomly chosen parameter\\nvalues.\\n3.3.\\nExperiments with real hyperspectral data\\nIn this section, we compare the performance of the MUAH2BO technique to the\\nMUASLIC, MUAWPX and S2WSU algorithms using well-known real hyperspectral4\\ndata, specifically the Samson and Jasper Ridge HIs. Two subimages were taken from\\nthese scenarios in to reduce the simulations’ computation times and facilitate the eval-\\nuation of the results. Samson’s subimage is a 40 × 95 pixels region with 156 bands\\nbetween 401 and 889 nm, and it is made up of three endmembers: soil, trees, and wa-\\nter. The region of interest for Jasper Ridge has 50 × 50 pixels, 198 bands between 380\\nand 2500 nm, and four key signatures: road, soil, water, and tree. The hyperspectral\\nlibraries utilised in this study are the same ones used by Borsoi et al. (2019). These\\nwere created using an extraction method that extracts A directly from the HI (Somers\\net al. 2012), yielding A ∈R156×105 for the Samson and A ∈R198×529 for the Jasper\\nRidge subimages. The additional parameters for each algorithm (included in Table 5)\\nwere determined based on a visual comparison between their estimated abundance\\nmaps and the RGB representation of the data. For H2BO, R = 3 was used.\\nThe oversegmentation results of the Samson and Jasper Ridge are shown in Figures\\n7 and 8, respectively. We find large superpixels in more regular locations, like water,\\n4Available online at http://lesun.weebly.com/hyperspectral-data-set.html.\\n18\\nTable 5.\\nParameters of the algorithms for the real HIs.\\nAlgorithm\\nParameters\\nSamson\\nJasper Ridge\\nMUAH2BO\\nH2BO\\nγ\\n0.00125\\n0.00125\\nσ0\\n15\\n15\\nσ1\\n7\\n8\\nτ outliers\\n10%\\n10%\\nτ homog\\n120%\\n100%\\nMUA\\nλC\\n0.1\\n0.003\\nλ\\n0.01\\n0.03\\nβ\\n1\\n3\\nMUASLIC\\nSLIC\\nγ\\n0.00125\\n0.00125\\nσ\\n7\\n7\\nMUA\\nλC\\n0.1\\n0.003\\nλ\\n0.01\\n0.03\\nβ\\n1\\n3\\nMUAWPX\\nWPX\\nk\\n0.01\\n0.01\\nσ\\n8\\n6\\nMUA\\nλC\\n0.1\\n0.003\\nλ\\n0.01\\n0.03\\nβ\\n1\\n3\\nS2WSU\\nS2WSU\\nλswsp\\n0.002\\n0.01\\nand smaller superpixels in more irregular places, demonstrating the effectiveness of the\\nsuggested strategy. As observed in Table 6, under these circumstances, just as with\\nthe synthetic data DC1, the proposed technique in H2BO enables the depiction of the\\nimage with a markedly reduced number of superpixels as compared to the number\\nemployed by MUASLIC. The MUAWPX performed well for Tree endmember, but failed\\nsignificantly for Soil and Water in the Samson image. In Jasper Ridge, however, the\\nresults were very consistent with those of other algorithms. The execution time of the\\nMUAH2BO remained comparable to that of the MUASLIC and far less than that of the\\nS2WSU even for simulations using real data. Still, the execution time of MUAWPX\\nand MUASLIC remain practically the same.\\nTable 6.\\nAverage execution time (in seconds) and number of superpixels for the real HIs.\\nData\\nS2WSU\\nMUASLIC\\nMUAWPX\\nMUAH2BO\\nex. time\\nex. time\\nsuperpixels\\nex. time\\nsuperpixels\\nex. time\\nsuperpixels\\nSamson\\n9 s\\n5 s\\n84\\n5 s\\n10\\n7 s\\n51\\nJasper Ridge\\n19 s\\n7 s\\n64\\n7 s\\n31\\n9 s\\n42\\nDue to the lack of ground-truth abundance maps for the widely used datasets made\\navailable online, the quantitative analysis of spectral unmixing methods based on tests\\nwith real hyperspectral data is limited. As a result, only a visual examination of the\\nabundance maps can be used to evaluate the results of abundance estimation with\\nreal data. Nonetheless, it is clear from the abundance maps calculated in Figures\\n9 and 10 that the quality of the spectral unmixing is not diminished by this more\\nstraightforward representation. Although the outcomes of the algorithms are visually\\nquite similar, if a ground-truth were available, tiny changes in the abundance maps\\n19\\n(a)\\n(b)\\n(c)\\n(d)\\nFigure 7.\\n(a) Samson subimage RGB representation. (b) Initial oversegmentation, σ0 = 15. (c) Initial ho-\\nmogeneity map (non-homogeneous regions in grey colour). (d) Result of the final oversegmentation from the\\nsubdivision of the non-homogeneous superpixels in (c), σ1 = 7.\\n(a)\\n(b)\\n(c)\\n(d)\\nFigure 8.\\n(a) Jasper Ridge subimage RGB representation. (b) Initial oversegmentation, σ0 = 15. (c) Initial\\nhomogeneity map (non-homogeneous regions in grey colour). (d) Result of the final oversegmentation from the\\nsubdivision of the non-homogeneous superpixels in (c), σ1 = 8.\\nproduced by either MUAH2BO and MUASLIC may result in discrepancies in terms of\\nquantitative metrics.\\n4.\\nApplication to hyperspectral data classification\\nIn this section, we illustrate the performance of the proposed H2BO method when\\ncombined with hyperspectral image classification strategies that leverage spatial con-\\ntextual information. More precisely, we consider the state-of-the-art semi-supervised\\ngraph convolutional neural network strategy proposed by Liu et al. (2021), which lever-\\nages the superpixel decomposition of the HI to construct a graph used in the neural\\nnetwork, which implicitly promotes pixels within a single superpixel to share the same\\nclass labels.\\nMore precisely, Liu et al. (2021) proposed an HI classification method integrating\\na superpixel-based graph convolutional network (GCN) and a convolutional neural\\nnetwork (CNN) into a single network. The basic idea is to combine the potential of a\\nGCN to model the various spatial structures of land covers in graphs with the ability\\nof a CNN to learn local spectral-spatial features at the pixel level. The technique\\nis known as CNN-enhanced GCN (CEGCN). The CEGCN algorithm performs semi-\\nsupervised classification: it learns a neural network to assign labels to each pixel of\\n20\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nSoil\\nTree\\nMUASLIC\\nWater\\nMUAWPX\\nMUAH2BO\\nS2WSU\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nFigure 9.\\nSamson – comparison between the estimated abundance maps for each endmember and algorithm.\\nan HI based on a small set of labeled training samples using information contained in\\nboth the labeled and unlabeled data.\\nTo assign each pixel a unique label, CEGCN is organised into the following stages:\\nthe superpixel-based graph constructor (SGC); the spectral transformation subnet-\\nwork (STsN); the superpixel-level graph subnetwork (SGsN); and the pixel-level con-\\nvolutional subnetwork (PCsN). The input HI is first transformed by an STsN, which\\nreduces the irrelevant information of the spectra and improves the discrimination be-\\ntween different classes. Then, SGsN is built to model the large-scale spatial structure\\nof the HI on graphs and produce superpixel-level features. In addition, a PCsN is em-\\nployed to extract local spectral-spatial features at the pixel level. Lastly, the features\\nextracted by SGsN and PCsN are merged to increase the classification performance.\\nThe SGC is the step responsible for converting the structure of the hyperspectral data\\ninto a graph. The appropriate graph structure containing spectral-spatial information\\nis built by applying the SLIC algorithm, where the similarity between each pair of\\nsuperpixels is used to construct an undirected graph. Before image segmentation, a\\ndimensionality reduction is also performed with linear discriminant analysis (LDA).\\nAlthough the use of superpixels decreases the number of nodes in a graph to be pro-\\ncessed, at the superpixel scale, the pixels in a superpixel (node) are described by a\\nsingle set of features. Hence, it is important for the superpixels to be homogeneous\\nso that the features adequately represent the pixels contained therein. Thus, we shall\\n21\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nTree\\nWater\\nDirt\\nMUASLIC\\nRoad\\nMUAWPX\\nMUAH2BO\\nS2WSU\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nFigure 10.\\nJasper Ridge – comparison between the estimated abundance maps for each endmember and\\nalgorithm.\\ncompare the performance of CEGCN using three segmentation steps, SLIC, waterpix-\\nels and H2BO, to evaluate the benefits the proposed method can bring in terms of\\nclassification performance.\\n4.1.\\nExperimental setup\\nIn this section, to validate the proposed method in the classification task, we propose\\nto replace the LDA-SLIC step in the superpixel-based graph constructor (SGC) of the\\nCEGCN algorithm with the proposed H2BO homogeneity-based multiscale method,\\nthus naming it CEGCNH2BO. We also refer to the original CEGCN algorithm using\\nSLIC by CEGCNSLIC, and CEGCNWPX by using waterpixels. As performed for the\\nunmixing task, simulations were conducted with synthetic and real datasets in order\\nto show the potential of the proposed method to improve hyperspectral image analysis\\nwhen used as preprocessing step. For the purpose of comparison between CEGCNH2BO,\\nthe original CEGCNSLIC and CEGCNWPX, the same metrics used by Liu et al. (2021)\\nwere evaluated: overall accuracy (OA), average accuracy (AA), and kappa statistics\\n(KPP) (Stehman 1997).\\n22\\n4.2.\\nSimulation results with synthetic data\\nTo validate the presented method in the classification task, the superpixel generation\\nstep of the CEGCN algorithm was replaced by the proposed hierarchical segmentation\\nstrategy. For the purpose of a controlled evaluation, it was decided to analyse the\\napplication of the algorithm on a synthetic dataset, in which the true labels of all\\npixels are known in advance and can be used for a quantitative evaluation.\\nThe DC3 image from Section 3.2 (shown in Figures 11a and 11b) was chosen for\\nthe simulations with synthetic data, since it presents a greater variety of shapes in\\nthe spatial distribution of its content and allowed for a satisfactory evaluation of the\\nunmixing task. We recall that this HI contained nine endmembers, which here will\\nbe considered as nine different classes, and their abundance proportions at each pixel\\nare available. To generate the ground-truth classification map of the DC3 image, for\\neach pixel, the class of the endmember with the highest abundance contained in it was\\nassigned. In addition to the noiseless condition, scenarios with an SNR of 30 dB and\\n20 dB were also considered. A grid search was performed to find the optimum values,\\nwhich are shown in Table 7. The parameters of CEGCN were kept with the same\\nvalues as in the original work. From the total number of labeled pixels, 10 samples\\nper class are randomly selected for training, 1% for validation, and the remaining for\\ntesting.\\nTable 7.\\nParameters of the classification CEGCN algorithm\\nfor DC3.\\nCEGCN\\nAlgorithm\\nParameters\\nDC3\\nnoiseless\\n30 dB\\n20 dB\\nH2BO\\nγ\\n0.00125\\n0.00125\\n0.00125\\nσ0\\n8\\n8\\n8\\nσ1\\n7\\n7\\n7\\nσ2\\n4\\n4\\n4\\nσ3\\n3\\n3\\n3\\nτ outliers\\n10%\\n10%\\n10%\\nτ homog\\n20%\\n30%\\n20%\\nSLIC\\nγ\\n0.01\\n0.1\\n0.1\\nσ\\n60\\n60\\n80\\nWaterpixels\\nk\\n0.001\\n0.001\\n0.01\\nσ\\n8\\n14\\n11\\nFigure 11 shows the resulting classification maps generated by the three methods.\\nIt is possible to notice that the use of the proposed H2BO oversegmentation method\\nleads to a smaller amount of misclassified pixels in CEGCNH2BO when compared to the\\noriginal method CEGCNSLIC and CEGCNWPX. This difference becomes more evident\\nwhen in the noisier condition of 20 dB SNR and in the lower left quarter of the\\nimage, where the distribution of classes is more irregular. Observing Figures 11e and\\n11h, even in the upper left region, where the arrangements are larger and uniform,\\nthe proposed method presents better classification performance, with emphasis on\\nneighbouring pixels at the boundaries between different classes. This same result can\\nbe seen from the overall and per class quantitative results shown in Table 8, as the\\nproposed CEGCNH2BO method led to improvements of up to about 2% with respect\\nto the CEGCNSLIC and 22% over CEGCNWPX in the cases of 20 dB SNR.\\nSensitivity and statistical performance analysis – Similar to the tests con-\\nducted for the unmixing task, the sensitivity of the OA, AA and KPP classification\\nmetrics to the parameter values of the proposed H2BO algorithm is analysed. The DC3\\n23\\n(a) False-colour\\n(b) Ground-truth\\n(c) CEGCNSLIC\\n(d) CEGCNSLIC, 30dB\\n(e) CEGCNSLIC, 20dB\\n(f) CEGCNWPX\\n(g) CEGCNWPX, 30dB\\n(h) CEGCNWPX, 20dB\\n(i) CEGCNH2BO\\n(j) CEGCNH2BO, 30dB\\n(k) CEGCNH2BO, 20dB\\nFigure 11. DC3 - Classification maps\\nsynthetic data and the 20 dB scenario were chosen for these experiments. In the statis-\\ntical performance analysis for randomly chosen parameters, in 200 executions, a new\\nrandom value for each parameter was sampled according uniform distributions within\\nthe following intervals: regulariser γ ∈[0, 001, 0, 02], superpixels size σ0 ∈[8, 30],\\nthresholds τoutliers ∈[10%, 50%] and τhomog ∈[20%, 200%]. For σ1, σ2 and σ3 the\\n24\\nTable 8.\\nClassification results of CEGCNSLIC, CEGCNWPX algorithms and the proposed CEGCNH2BO\\non the DC3 data set – Training samples: 10.\\nDC3 dataset\\nMetric\\nCEGCNSLIC\\nCEGCNWPX\\nCEGCNH2BO\\nNoiseless\\n30 dB\\n20 dB\\nNoiseless\\n30 dB\\n20 dB\\nNoiseless\\n30 dB\\n20 dB\\nOA(%)\\n69.5±1.6 71.8±1.6 65.4±1.6\\n74.8±1.6 66.5±2.0 54.8±2.8\\n79.4±0.3 77.4±1.7 66.7±3.7\\nAA(%)\\n69.3±1.8 71.7±1.2 64.9±2.0\\n74.6±1.8 65.8±1.7 54.5±2.6\\n79.2±0.4 77.2±1.9 66.3±3.3\\nKPP(x100) 65.5±1.8 68.1±1.7 60.9±1.8\\n71.5±1.8 62.1±2.2 49.0±3.1\\n76.7±0.4 74.4±2.0 62.4±4.2\\nfollowing relationship was used: σi ∈[ σi−1\\n2 , σi−1 −1], rounded up to the upper integer\\nand σ2 and σ3 limited to 3 and 2, respectively. We also varied the values of parameters\\nσ0, τoutliers and τhomog while keeping the remaining parameters at their optimal values,\\nand evaluated the effect on the classification performance. The results are shown in\\nFigure 12.\\nIn Figure 12a, for larger values of the initial superpixels size σ0 the score results\\ntends to improve, since it is possible that the large superpixels will be re-segmented\\nat a later scale. As the values of τoutliers increase (Figure 12b), performance worsens,\\nas relevant information is removed in addition to unwanted outliers. As expected, the\\nτhomog values in Figure 12c have the most influence on the results, since they directly\\ndetermine the degree of homogeneity for subdividing the superpixels and providing\\nthe finest representation of HI.\\nThe histogram in Figure 12d shows the percentage deviation from the reference value\\nof OA = 66.7% presented in Table 8 considering the 200 executions of CEGCNH2BO\\nalgorithm, 20 dB SNR scenario, with randomly selected parameters. It is possible\\nto perceive an average variation of approximately 3% and a low standard deviation,\\naround 5%. This shows that the probability of obtaining good results in the classifica-\\ntion task is high for parameter values randomly chosen in a reasonable range according\\nto the parameter selection methodology presented in Section 2.1.\\n4.3.\\nExperiments with real hyperspectral data\\nFor experimental analysis on real datasets, the CEGCNSLIC, CEGCNWPX and\\nCEGCNH2BO algorithms were applied to the classification of the popular Indian Pines,\\nPavia University, and Salinas scenes in the noiseless, 20 dB and 30 dB SNR scenarios.\\nFor the Indian Pines, Salinas, and Pavia University datasets, 15, 5, and 10 training\\nsamples per class were used, respectively, 1% for validation and the remaining for test-\\ning. The multiscale representation parameters for all algorithms were determined by\\ngrid search and are shown in Table 9.\\nThe Indian Pines dataset is composed mostly of cropland and forest from Indiana,\\nUSA, and has a spatial size of 145×145 pixels and 200 spectral bands (after removal\\nof noisy and water absorption bands) with wavelengths between 400 and 2500 nm\\nobserved by the AVIRIS sensor. A false-colour representation and the ground-truth of\\nthe scene with its 16 different classes are presented in Figure 13a and 13b, respectively.\\nThe resulting classification maps are shown in Figure 13c-h.\\nSalinas is a scene also collected by the AVIRIS sensor, from Salinas Valley, Califor-\\nnia, USA, with size 512×217 pixels and 16 classes annotated in its ground-truth, in-\\ncluding fields and fruit and vegetable plantations. The false-colour image, the ground-\\ntruth and the classification maps generated by the algorithms are shown in Figure\\n14.\\n25\\n8\\n12\\n16\\n20\\n24\\n28 30\\n<0\\n50\\n55\\n60\\n65\\n70\\n75\\nScore\\nOA (%)\\nAA (%)\\nKPP\\n(a)\\n10\\n15\\n20\\n25\\n30\\n35\\n40\\n45\\n50\\n=outliers (%)\\n50\\n55\\n60\\n65\\n70\\n75\\nScore\\nOA (%)\\nAA (%)\\nKPP\\n(b)\\n50\\n100\\n150\\n200\\n=homog (%)\\n50\\n55\\n60\\n65\\n70\\n75\\nScore\\nOA (%)\\nAA (%)\\nKPP\\n(c)\\nminimum: -24.3% \\nmaximum: 12.0% \\naverage:   -3.3% \\nvariance:   0.3% \\nstd. dev.:   5.1%\\n-25\\n-20\\n-15\\n-10\\n-5\\n0\\n5\\n10\\n15\\nDeviation (%)\\n0\\n5\\n10\\n15\\n20\\n25\\nNumber of executions\\n(d) Deviation from reference overall accuracy\\nFigure 12.\\nParameter sensitivity and statistical performance analysis of the classification task in DC3 using\\nCEGCNH2BO, 20 dB SNR. The top row shows the score (y-axis) of the OA(%), AA(%) and KPP metrics\\nfor the variation of the parameters (x-axis) of (a) initial superpixel size, and (b) outliers and (c) homogeneity\\nthresholds. The (d) histogram in the bottom row shows the percentage deviation (x-axis) from the reference\\nvalue OA(%) = 66.7% considering 200 executions (y-axis) of the algorithm.\\nTable 9.\\nParameters of the classification CEGCN algorithm for each HI.\\nCEGCN\\nAlgorithm\\nParameters\\nIndian Pines\\nSalinas\\nPavia University\\nnoiseless\\n30 dB\\n20 dB noiseless\\n30 dB\\n20 dB\\nnoiseless\\n30 dB\\n20 dB\\nH2BO\\nγ\\n0.00125\\n0.00125\\n0.001\\n0.00125\\n0.00125 0.00125\\n0.00125\\n0.00125 0.00125\\nσ0\\n40\\n50\\n20\\n60\\n70\\n20\\n60\\n70\\n40\\nσ1\\n15\\n15\\n20\\n15\\n15\\n15\\n15\\n15\\n15\\nσ2\\n10\\n10\\n10\\n10\\n10\\n10\\n10\\n10\\n10\\nσ3\\n8\\n8\\n8\\n8\\n8\\n8\\n8\\n8\\n8\\nτ outliers\\n10%\\n10%\\n10%\\n10%\\n10%\\n10%\\n10%\\n10%\\n10%\\nτ homog\\n50%\\n20%\\n175%\\n150%\\n150%\\n200%\\n100%\\n100%\\n100%\\nSLIC\\nγ\\n0.001\\n0.1\\n0.1\\n0.001\\n0.01\\n0.1\\n0.001\\n0.01\\n0.01\\nσ\\n60\\n60\\n80\\n30\\n80\\n80\\n60\\n70\\n60\\nWaterpixels\\nk\\n0.01\\n0.01\\n0.001\\n0.1\\n0.001\\n0.001\\n0.001\\n0.01\\n0.01\\nσ\\n20\\n20\\n11\\n15\\n14\\n10\\n12\\n12\\n10\\nThe last dataset, Pavia University, is an urban scene of Pavia, Italy, with a size of\\n610×340 pixels obtained by the ROSIS sensor, with 103 bands (after removal of noisy\\nand water absorption bands) with wavelengths between 430 and 860 nm. The false-\\ncolour image and ground-truth of the scene with the 9 annotated classes are shown\\nin Figures 15a and 15b, respectively. The resulting classification maps are given in\\n26\\nFigures 15c-h.\\nTables 10, 11 and 12 show the results of the OA, AA and KPP metrics obtained by\\napplying the classification algorithms to the Indian Pines, Salinas and Pavia Univer-\\nsity datasets, respectively. Analysing these quantitative results, the classification maps\\nand their differences for the ground-truth, it can be seen that the accuracies of both\\nCEGCNSLIC, CEGCNWPX and CEGCNH2BO were similar between the different algo-\\nrithms and SNR scenarios. The CEGCNWPX obtained better results for the Salinas\\ndataset, given that the image has large regions with a very regular shape, which could\\nbenefit from a less elaborate approach to segmentation than the one used in H2BO.\\nHowever, for datasets with more complex and irregular shapes, the CEGCNH2BO ap-\\nproach may be more suitable. When observing the classification maps of Figures 13,\\n14 and 15, it is difficult to evaluate the influence of different superpixel segmentation\\nmethods on the performance of the CEGCN algorithm, particularly due to the non-\\ncontiguous nature of the available ground truth labels. Since the annotated classes are\\nnot spatially contiguous, they do not allow for an evaluation in the challenging condi-\\ntion of classifying the pixels located at the boundary between classes, where the use\\nof the proposed hierarchical homogeneity-based strategy could provide more effective\\nand robust results in CEGCNH2BO. This highlights the importance of simulations with\\nsynthetic data.\\nThe classification methods were also compared in terms of average training time\\nand the time of the superpixel generation step. From the data presented in Table 13,\\nit is noticeable that SLIC is the fastest in the segmentation stage, but this does not\\nnecessarily result in reduced training times. On the other hand, although the proposed\\nTable 10.\\nClassification results of CEGCNSLIC, CEGCNWPX algorithms and the proposed CEGCNH2BO\\non the Indian Pines data set – Training samples: 15.\\nIndian Pines dataset\\nMetric\\nCEGCNSLIC\\nCEGCNWPX\\nCEGCNH2BO\\nNoiseless\\n30 dB\\n20 dB\\nNoiseless\\n30 dB\\n20 dB\\nNoiseless\\n30 dB\\n20 dB\\nOA(%)\\n88.6±2.0 79.1±2.5 71.2±2.3\\n89.5±1.2 80.8±1.6 71.4±2.4\\n88.7±1.5 77.3±1.8 71.6±3.9\\nAA(%)\\n94.0±0.5 88.0±1.1 83.2±1.2\\n94.7±0.4 89.1±1.2 82.5±1.1\\n93.6±0.8 86.4±1.6 81.6±2.3\\nKPP(x100) 87.0±2.2 76.2±2.8 67.5±2.3\\n88.1±1.3 78.2±1.7 67.5±2.7\\n87.2±1.7 74.1±2.0 67.5±4.3\\nTable 11.\\nClassification results of CEGCNSLIC, CEGCNWPX algorithms and the proposed CEGCNH2BO\\non the Salinas data set – Training samples: 5.\\nSalinas dataset\\nMetric\\nCEGCNSLIC\\nCEGCNWPX\\nCEGCNH2BO\\nNoiseless\\n30 dB\\n20 dB\\nNoiseless\\n30 dB\\n20 dB\\nNoiseless\\n30 dB\\n20 dB\\nOA(%)\\n96.4±1.0 90.9±4.0 84.6±3.5\\n99.1±0.5 91.3±1.5 90.0±5.0\\n95.9±1.4 84.6±3.4 85.5±3.4\\nAA(%)\\n98.0±0.7 94.2±1.8 83.6±1.4\\n98.8±0.5 94.0±1.3 92.2±3.7\\n97.5±0.7 85.5±2.0 88.0±2.0\\nKPP(x100) 96.0±1.1 89.9±4.3 83.0±3.8\\n99.0±0.5 90.3±1.6 88.8±5.6\\n95.5±1.6 90.7±2.1 83.9±3.7\\nTable 12.\\nClassification results of CEGCNSLIC, CEGCNWPX algorithms and the proposed CEGCNH2BO\\non the Pavia University data set – Training samples: 10.\\nPavia University dataset\\nMetric\\nCEGCNSLIC\\nCEGCNWPX\\nCEGCNH2BO\\nNoiseless\\n30 dB\\n20 dB\\nNoiseless\\n30 dB\\n20 dB\\nNoiseless\\n30 dB\\n20 dB\\nOA(%)\\n94.3±1.6 93.6±2.1 91.0±3.0\\n95.6±2.0 96.3±1.1 92.9±1.7\\n95.5±1.3 95.9±2.1 93.2±2.6\\nAA(%)\\n96.9±0.6 95.9±0.4 94.8±1.6\\n97.2±0.8 97.7±0.3 95.9±0.9\\n97.6±0.5 97.2±1.0 95.0±2.0\\nKPP(x100) 92.6±2.0 91.6±2.6 88.4±3.8\\n94.3±2.5 95.2±1.4 90.8±2.1\\n94.1±1.7 94.6±2.7 91.1±3.4\\n27\\n(a) False-colour\\n(b) Ground-truth\\n(c) CEGCNSLIC\\n(d) CEGCNSLIC, 30dB\\n(e) CEGCNSLIC, 20dB\\n(f) CEGCNWPX\\n(g) CEGCNWPX, 30dB\\n(h) CEGCNWPX, 20dB\\n(i) CEGCNH2BO\\n(j) CEGCNH2BO, 30dB\\n(k) CEGCNH2BO, 20dB\\nFigure 13. Indian Pines – false-colour representation, ground-truth and classification maps.\\nH2BO presents the longest time in the superpixel generation process, its ability to\\nrepresent scales with different superpixel sizes can lead to faster convergence and a\\nreduction in total training time. This effect becomes clearer when the dimensions of\\nthe images are larger, as in the case of the Salinas and Pavia University datasets.\\nAlthough the extra rounds of segmentation and homogeneity assessment can impact\\n28\\nTable 13.\\nAverage training time (in seconds) of the classification algorithms\\nand their superpixel segmentation step.\\nDataset\\nCEGCNSLIC\\nCEGCNWPX\\nCEGCNH2BO\\nTotal\\nSLIC\\nTotal\\nWPX\\nTotal\\nH2BO\\nDC3\\n57.04\\n0.04\\n54.25\\n0.25\\n73.25\\n1.25\\nIndian Pines\\n56.09\\n0.09\\n55.51\\n0.51\\n57.23\\n6.23\\nSalinas\\n277.57\\n0.50\\n328.73\\n2.73\\n169.21\\n28.21\\nPavia University\\n575.52\\n0.52\\n565.52\\n4.52\\n500.07\\n95.07\\nthe training time for small images, it still remains comparable to those of the other\\nalgorithms. As for waterpixels, their performance is in the middle ground between\\nSLIC and H2BO, still proving to be an efficient algorithm in general.\\nThe obtained results show that the proposed method leads to improved classification\\nperformance when the regions to be classified have a more homogeneous composition,\\nand when areas corresponding to different classes tend to have significantly different\\nsizes. This is the expected performance, as the proposed algorithm was designed with\\nthe aim of identifying homogeneous regions of the image, and to allow significantly\\ndifferent numbers of pixels for different superpixels. Of course, not all hyperspectral\\nimages will have these characteristics. Another interesting characteristic of the pro-\\nposed algorithm was that it has led to a smaller performance degradation as the image\\nSNR is reduced when compared with the competing algorithms. These characteristics\\nwere illustrated more clearly in the simulation results using synthetic images, where\\nwe were able to test different relative sizes of class regions and different degrees of\\nhomogeneity within each class. For the real images, the results are more difficult to\\ninterpret since the pixels belonging to single class are not necessarily spectrally homo-\\ngeneous (e.g., it is possible that a large region assigned to a given class is composed\\nof pixels with large variability). In such cases, the use of superpixels decomposition\\nalgorithms based on the spatial-spectral homogeneity property, such as SLIC and the\\nproposed H2BO, might be inadequate to aid in a classification task.\\nFinally, the performance of the proposed algorithm was still on par when compared\\nwith the competing algorithms even when the conditions for which it was designed\\nwere not satisfied. This illustrates the important robustness of the algorithm, making\\nit useful for hyperspectral imaging tasks.\\n5.\\nConclusions\\nIn this paper, we presented a novel hierarchical superpixels segmentation technique for\\nhyperspectral image analysis. We progressively split an HI into irregular superpixels\\nwith increased spectral homogeneity, which better depict the spatial information of the\\nmaterials in a scene, using the SLIC oversegmentation algorithm and an unique robust\\nhomogeneity testing approach. A multiscale methodology is then applied to the final\\nsuperpixel decomposition to incorporate spatial information into the sparse unmixing\\nand convolutional graph neural networks-based classification challenges. The results of\\nexperiments using hyperspectral data with different spatial compositions demonstrated\\nthat, in noisy situations, homogeneity-based approaches outperform state-of-the-art\\nalgorithms while preserving computational complexity.\\n29\\nReferences\\nAcci´on, ´Alvaro, Francisco Arg¨uello, and Dora B Heras. 2020. “Dual-window superpixel data\\naugmentation for hyperspectral image classification.” Applied Sciences 10 (24): 8833.\\nAchanta, Radhakrishna, A. Shaji, K. Smith, A. Lucchi, P. Fua, and Sabine S¨usstrunk.\\n2012. “SLIC Superpixels Compared to State-of-the-Art Superpixel Methods.” IEEE\\nTransactions\\non\\nPattern\\nAnalysis\\nand\\nMachine\\nIntelligence\\n34\\n(11):\\n2274–2282.\\nhttps://doi.org/10.1109/TPAMI.2012.120.\\nArg¨uello, Francisco, Dora B Heras, Alberto S Garea, and Pablo Quesada-Barriuso. 2021. “Wa-\\ntershed monitoring in galicia from UAV multispectral imagery using advanced texture meth-\\nods.” Remote Sensing 13 (14): 2687.\\nAudebert, Nicolas, Bertrand Le Saux, and S´ebastien Lef`evre. 2019. “Deep learning for classifi-\\ncation of hyperspectral data: A comparative review.” IEEE geoscience and remote sensing\\nmagazine 7 (2): 159–173.\\nAyres, L. C., Sergio J.M. De Almeida, Jose C.M. Bermudez, and Ricardo A. Borsoi. 2021.\\n“A Homogeneity-based Multiscale Hyperspectral Image Representation for Sparse Spectral\\nUnmixing.” In ICASSP, IEEE International Conference on Acoustics, Speech and Signal\\nProcessing - Proceedings, Vol. 2021-June, 1460–1464. Institute of Electrical and Electronics\\nEngineers Inc.\\nAyres, Luciano C, Ricardo A Borsoi, Jos´e CM Bermudez, and S´ergio JM De Almeida. 2024.\\n“A Generalized Multiscale Bundle-Based Hyperspectral Sparse Unmixing Algorithm.” IEEE\\nGeoscience and Remote Sensing Letters 21.\\nBeucher, S, and F Meyer. 1993. “The Morphological Approach to Segmentation: The Wa-\\ntershed Transformation.” In Mathematical Morphology in Image Processing, edited by E.R\\nDougherty, 433–481. CRC Press.\\nBioucas-dias, Jos´e M, Antonio Plaza, Gustavo Camps-valls, Paul Scheunders, Nasser M\\nNasrabadi, and Jocelyn Chanussot. 2013. “Hyperspectral Remote Sensing Data Analysis\\nand Future Challenges.” IEEE Geoscience and Remote Sensing Magazine 1 (June): 6–36.\\nhttps://doi.org/10.1109/MGRS.2013.2244672.\\nBioucas-Dias, Jos´e M., Antonio Plaza, Nicolas Dobigeon, Mario Parente, Qian Du,\\nPaul Gader, and Jocelyn Chanussot. 2012. “Hyperspectral unmixing overview: Ge-\\nometrical, statistical, and sparse regression-based approaches.” IEEE Journal of Se-\\nlected Topics in Applied Earth Observations and Remote Sensing\\n5 (2): 354–379.\\nhttps://doi.org/10.1109/JSTARS.2012.2194696.\\nBischof, Horst, and Ales Leonardis. 1998. “Finding optimal neural networks for land use clas-\\nsification.” IEEE transactions on Geoscience and Remote Sensing 36 (1): 337–341.\\nBorsoi,\\nRicardo\\nAugusto,\\nTales\\nImbiriba,\\nand\\nJose\\nCarlos\\nMoreira\\nBermudez.\\n2020.\\n“A\\nData\\nDependent\\nMultiscale\\nModel\\nfor\\nHyperspectral\\nUnmixing\\nwith\\nSpectral\\nVariability.”\\nIEEE\\nTransactions\\non\\nImage\\nProcessing\\n29:\\n3638–3651.\\nhttps://doi.org/10.1109/TIP.2020.2963959.\\nBorsoi, Ricardo Augusto, Tales Imbiriba, Jos´e Carlos Moreira Bermudez, and Cedric\\nRichard.\\n2019.\\n“A\\nFast\\nMultiscale\\nSpatial\\nRegularization\\nfor\\nSparse\\nHyperspec-\\ntral\\nUnmixing.”\\nIEEE\\nGeoscience\\nand\\nRemote\\nSensing\\nLetters\\n16\\n(4):\\n598–602.\\nhttps://doi.org/10.1109/LGRS.2018.2878394.\\nBorsoi, Ricardo Augusto, Tales Imbiriba, Jos´e Carlos Moreira Bermudez, and C´edric Richard.\\n2020. “A Blind Multiscale Spatial Regularization Framework for Kernel-based Spectral Un-\\nmixing.” IEEE Transactions on Image Processing 29: 4965–4979.\\nBorsoi, Ricardo Augusto, Tales Imbiriba, Jos´e Carlos Moreira Bermudez, C´edric Richard,\\nJocelyn Chanussot, Lucas Drumetz, Jean-Yves Tourneret, Alina Zare, and Christian Jutten.\\n2021. “Spectral variability in hyperspectral data unmixing: A comprehensive review.” IEEE\\ngeoscience and remote sensing magazine 9 (4): 223–270.\\nCao, Xianghai, Chenguang Li, Jie Feng, and Licheng Jiao. 2023a. “Semi-supervised feature\\nlearning for disjoint hyperspectral imagery classification.” Neurocomputing 526: 9–18.\\nCao, Xianghai, Haifeng Lin, Shuaixu Guo, Tao Xiong, and Licheng Jiao. 2023b. “Transformer-\\n30\\nbased masked autoencoder with contrastive loss for hyperspectral image classification.”\\nIEEE Transactions on Geoscience and Remote Sensing .\\nChen, Tao, Yang Liu, Yuxiang Zhang, Bo Du, and Antonio Plaza. 2022. “Superpixel-Based\\nCollaborative and Low-Rank Regularization for Sparse Hyperspectral Unmixing.” IEEE\\nTransactions on Geoscience and Remote Sensing .\\nCihan, Mucahit, Murat Ceylan, and Ahmet Haydar Ornek. 2022. “Spectral-spatial classifica-\\ntion for non-invasive health status detection of neonates using hyperspectral imaging and\\ndeep convolutional neural networks.” Spectroscopy Letters 55 (5): 336–349.\\nCivco, Daniel L. 1993. “Artificial neural networks for land-cover classification and mapping.”\\nInternational journal of geographical information science 7 (2): 173–186.\\nDi, Shuanhu, Miao Liao, Yuqian Zhao, Yang Li, and Yezhan Zeng. 2021. “Image superpixel\\nsegmentation based on hierarchical multi-level LI-SLIC.” Optics and Laser Technology 135:\\n106703. https://doi.org/https://doi.org/10.1016/j.optlastec.2020.106703.\\nDieste, ´Alvaro G, Francisco Arg¨uello, and Dora B Heras. 2023. “ResBaGAN: A Residual\\nBalancing GAN with Data Augmentation for Forest Mapping.” IEEE Journal of Selected\\nTopics in Applied Earth Observations and Remote Sensing .\\nDing, Yao, Zhili Zhang, Xiaofeng Zhao, Danfeng Hong, Wei Cai, Chengguo Yu, Nengjun\\nYang,\\nand\\nWeiwei\\nCai.\\n2022.\\n“Multi-feature\\nfusion:\\nGraph\\nneural\\nnetwork\\nand\\nCNN combining for hyperspectral image classification.” Neurocomputing 501: 246–257.\\nhttps://doi.org/https://doi.org/10.1016/j.neucom.2022.06.031.\\nDobigeon, Nicolas, Jean-Yves Tourneret, Cedric Richard, Jos´e Carlos Moreira Bermudez,\\nStephen McLaughlin, and Alfred O. Hero. 2014. “Nonlinear Unmixing of Hyperspec-\\ntral Images: Models and Algorithms.” IEEE Signal Processing Magazine 31 (1): 82–94.\\nhttps://doi.org/10.1109/MSP.2013.2279274.\\nFelzenszwalb, Pedro F, and Daniel P Huttenlocher. 2004. “Efficient Graph-Based Im-\\nage\\nSegmentation.”\\nInternational\\nJournal\\nof\\nComputer\\nVision\\n59\\n(2):\\n167–181.\\nhttps://doi.org/10.1023/B:VISI.0000022288.19776.77.\\nHalicek, Martin, Himar Fabelo, Samuel Ortega, James V Little, Xu Wang, Amy Y Chen, Gus-\\ntavo Marrero Callico, Larry L Myers, Baran D Sumer, and Baowei Fei. 2019. “Cancer de-\\ntection using hyperspectral imaging and evaluation of the superficial tumor margin variance\\nwith depth.” In Medical Imaging 2019: Image-Guided Procedures, Robotic Interventions,\\nand Modeling, Vol. 10951, 109511A. International Society for Optics and Photonics.\\nHe, Lin, Jun Li, Chenying Liu, and Shutao Li. 2018. “Recent Advances on Spec-\\ntral–Spatial\\nHyperspectral\\nImage\\nClassification:\\nAn\\nOverview\\nand\\nNew\\nGuide-\\nlines.” IEEE Transactions on Geoscience and Remote Sensing\\n56 (3): 1579–1597.\\nhttps://doi.org/10.1109/TGRS.2017.2765364.\\nInce,\\nTaner.\\n2020.\\n“Superpixel-Based\\nGraph\\nLaplacian\\nRegularization\\nfor\\nSparse\\nHyperspectral\\nUnmixing.”\\nIEEE\\nGeoscience\\nand\\nRemote\\nSensing\\nLetters\\n1–5.\\nhttps://doi.org/10.1109/lgrs.2020.3027055.\\nInce,\\nTaner.\\n2021.\\n“Double\\nSpatial\\nGraph\\nLaplacian\\nRegularization\\nfor\\nSparse\\nUnmixing.”\\nIEEE\\nGeoscience\\nand\\nRemote\\nSensing\\nLetters\\n1–5.\\nhttps://doi.org/10.1109/LGRS.2021.3065989.\\nIordache, Marian-Daniel, J M Bioucas-Dias, and Antonio Plaza. 2011. “Sparse Unmix-\\ning of Hyperspectral Data.” IEEE Trans. Geosc. Rem. Sens. 49 (6): 2014–2039.\\nhttps://doi.org/10.1109/TGRS.2010.2098413.\\nIordache,\\nMarian-Daniel,\\nJos´e\\nM.\\nBioucas-Dias,\\nand\\nAntonio\\nPlaza.\\n2012.\\n“To-\\ntal\\nvariation\\nspatial\\nregularization\\nfor\\nsparse\\nhyperspectral\\nunmixing.”\\nIEEE\\nTransactions\\non\\nGeoscience\\nand\\nRemote\\nSensing\\n50\\n(11\\nPART1):\\n4484–4502.\\nhttps://doi.org/10.1109/TGRS.2012.2191590.\\nIordache, Marian-Daniel, Jose M. Bioucas-Dias, and Antonio Plaza. 2014. “Collaborative\\nSparse Regression for Hyperspectral Unmixing.” IEEE Transactions on Geoscience and\\nRemote Sensing 52 (1): 341–354. https://doi.org/10.1109/TGRS.2013.2240001.\\nIordache, Marian-Daniel, Antonio Plaza, and Jos´e Bioucas-Dias. 2010. “On the use of spectral\\nlibraries to perform sparse unmixing of hyperspectral data.” 2nd Workshop on Hyperspectral\\n31\\nImage and Signal Processing: Evolution in Remote Sensing, WHISPERS 2010 - Workshop\\nProgram 1–4. https://doi.org/10.1109/WHISPERS.2010.5594888.\\nJampani, Varun, Deqing Sun, Ming-Yu Liu, Ming-Hsuan Yang, and Jan Kautz. 2018. “Super-\\npixel sampling networks.” In Proceedings of the European Conference on Computer Vision\\n(ECCV), 352–368.\\nKeshava, Nirmal, and J.F. Mustard. 2002. “Spectral unmixing.” IEEE Signal Processing Mag-\\nazine 19 (1): 44–57. https://doi.org/10.1109/79.974727.\\nKotzagiannidis,\\nMadeleine\\nS.,\\nand\\nCarola-Bibiane\\nSch¨onlieb.\\n2022.\\n“Semi-\\nSupervised\\nSuperpixel-Based\\nMulti-Feature\\nGraph\\nLearning\\nfor\\nHyperspectral\\nIm-\\nage\\nData.”\\nIEEE\\nTransactions\\non\\nGeoscience\\nand\\nRemote\\nSensing\\n60:\\n1–12.\\nhttps://doi.org/10.1109/TGRS.2021.3112298.\\nKumar, Vinod, Ravi Shankar Singh, and Yaman Dua. 2022. “Morphologically dilated con-\\nvolutional neural network for hyperspectral image classification.” Signal Processing: Image\\nCommunication 101: 116549. https://doi.org/https://doi.org/10.1016/j.image.2021.116549.\\nLandgrebe, David. 2002. “Hyperspectral image data analysis.” IEEE Signal Processing Mag-\\nazine 19 (1): 17–28. https://doi.org/10.1109/79.974718.\\nLevinshtein, Alex, Adrian Stere, Kiriakos N. Kutulakos, David J. Fleet, Sven J. Dickin-\\nson, and Kaleem Siddiqi. 2009. “TurboPixels: Fast superpixels using geometric flows.”\\nIEEE Transactions on Pattern Analysis and Machine Intelligence 31 (12): 2290–2297.\\nhttps://doi.org/10.1109/TPAMI.2009.96.\\nLi, Xueying, Pingping Fan, Zongmin Li, Huimin Qiu, Guangli Hou, Guangyuan Chen, and\\nGuoxing Ren. 2023a. “Hyperspectral images classification of small sample based on the\\nstrategy of sample enlargement by superpixel pair method.” International Journal of Remote\\nSensing 44 (20): 6259–6279.\\nLi, Zhi, Ruyi Feng, Lizhe Wang, and Tieyong Zeng. 2023b. “Spectral-spatial-sparse unmixing\\nwith superpixel-oriented graph Laplacian.” International Journal of Remote Sensing 44 (8):\\n2573–2589.\\nLiu, Qichao, Liang Xiao, Jingxiang Yang, and Zhihui Wei. 2021. “CNN-Enhanced Graph\\nConvolutional Network with Pixel- And Superpixel-Level Feature Fusion for Hyperspec-\\ntral Image Classification.” IEEE Transactions on Geoscience and Remote Sensing 59 (10):\\n8657–8671. https://doi.org/10.1109/TGRS.2020.3037361.\\nLiu, Yuwei, Hongbin Pu, and Da-Wen Sun. 2017. “Hyperspectral imaging technique for eval-\\nuating food quality and safety during various processes: A review of recent applications.”\\nTrends in food science & technology 69: 25–35.\\nMa, Wing-Kin, Jos´e M Bioucas-Dias, Tsung-Han Chan, Nicolas Gillis, Paul Gader, Antonio J\\nPlaza, ArulMurugan Ambikapathi, and Chong-Yung Chi. 2013. “A signal processing per-\\nspective on hyperspectral unmixing: Insights from remote sensing.” IEEE Signal Processing\\nMagazine 31 (1): 67–81.\\nMachairas, Va¨ıa, Matthieu Faessel, David C´ardenas-Pe˜na, Th´eodore Chabardes, Thomas Wal-\\nter, and Etienne Decenciere. 2015. “Waterpixels.” IEEE Transactions on Image Processing\\n24 (11): 3707–3716.\\nMacQueen, James. 1967. “Some methods for classification and analysis of multivariate observa-\\ntions.” Proceedings of the fifth Berkeley symposium on mathematical statistics and probability\\n1 (14): 281–297.\\nMei,\\nXiaoguang,\\nYong\\nMa,\\nChang\\nLi,\\nFan\\nFan,\\nJun\\nHuang,\\nand\\nJiayi\\nMa.\\n2018.\\n“Robust\\nGBM\\nhyperspectral\\nimage\\nunmixing\\nwith\\nsuperpixel\\nsegmenta-\\ntion based low rank and sparse representation.” Neurocomputing\\n275: 2783–2797.\\nhttps://doi.org/10.1016/j.neucom.2017.11.052.\\nMelgani, Farid, and Lorenzo Bruzzone. 2004. “Classification of hyperspectral remote sensing\\nimages with support vector machines.” IEEE Transactions on geoscience and remote sensing\\n42 (8): 1778–1790.\\nMookambiga, A., and V. Gomathi. 2021. “Kernel eigenmaps based multiscale sparse model for\\nhyperspectral image classification.” Signal Processing: Image Communication 99: 116416.\\nhttps://doi.org/https://doi.org/10.1016/j.image.2021.116416.\\n32\\nNaik, Nitesh, Kandasamy Chandrasekaran, Venkatesan Meenakshi Sundaram, and Prabha-\\nvathy Panneer. 2023. “Spatio-temporal analysis of land use/land cover change detection\\nin small regions using self-supervised lightweight deep learning.” Stochastic Environmental\\nResearch and Risk Assessment 37 (12): 5029–5049.\\nNoyel, Guillaume, Jesus Angulo, and Dominique Jeulin. 2020. “Morphological segmentation\\nof hyperspectral images.” arXiv preprint arXiv:2010.00853 .\\nP. Ghamisi, J. Plaza, Y. Chen, J. Li, and A. J. Plaza. 2017. “Advanced Spectral Classifiers for\\nHyperspectral Images: A review.” IEEE Geoscience and Remote Sensing Magazine 5 (1):\\n8–32.\\nPu, Hongbin, Qingyi Wei, and Da-Wen Sun. 2023. “Recent advances in muscle food safety eval-\\nuation: Hyperspectral imaging analyses and applications.” Critical Reviews in Food Science\\nand Nutrition 63 (10): 1297–1313.\\nQi, Lin, Jie Li, Xinbo Gao, Ying Wang, Chongyue Zhao, and Yu Zheng. 2019. “A novel joint dic-\\ntionary framework for sparse hyperspectral unmixing incorporating spectral library.” Neu-\\nrocomputing 356: 97–106. https://doi.org/https://doi.org/10.1016/j.neucom.2019.04.053.\\nQin, Anyong, Zhaowei Shang, Jinyu Tian, Yulong Wang, Taiping Zhang, and Yuan Yan Tang.\\n2018. “Spectral–spatial graph convolutional networks for semisupervised hyperspectral im-\\nage classification.” IEEE Geoscience and Remote Sensing Letters 16 (2): 241–245.\\nSaranathan, Arun M., and Mario Parente. 2016. “Uniformity-based superpixel segmentation\\nof hyperspectral images.” IEEE Transactions on Geoscience and Remote Sensing 54 (3):\\n1419–1430. https://doi.org/10.1109/TGRS.2015.2480863.\\nSchowengerdt, R.A. 2006. Remote Sensing: Models and Methods for Image Processing. 3rd ed.\\nElsevier Science.\\nSellars,\\nPhilip,\\nAngelica\\nI.\\nAviles-Rivero,\\nand\\nCarola-Bibiane\\nSch¨onlieb.\\n2020.\\n“Su-\\nperpixel\\nContracted\\nGraph-Based\\nLearning\\nfor\\nHyperspectral\\nImage\\nClassifica-\\ntion.” IEEE Transactions on Geoscience and Remote Sensing\\n58 (6): 4180–4193.\\nhttps://doi.org/10.1109/TGRS.2019.2961599.\\nShi, Jianbo, and Jitendra Malik. 2000. “Normalized cuts and image segmentation.”\\nIEEE Transactions on Pattern Analysis and Machine Intelligence 22 (8): 888–905.\\nhttps://doi.org/10.1109/34.868688.\\nShimoni, Michal, Rob Haelterman, and Christiaan Perneel. 2019. “Hyperspectral imag-\\ning for military and security applications: Combining Myriad processing and sens-\\ning techniques.” IEEE Geoscience and Remote Sensing Magazine\\n7 (2): 101–117.\\nhttps://doi.org/10.1109/MGRS.2019.2902525.\\nSomers, Ben, MacIel Zortea, Antonio Plaza, and Gregory P. Asner. 2012. “Automated ex-\\ntraction of image-based endmember bundles for improved spectral unmixing.” IEEE Jour-\\nnal of Selected Topics in Applied Earth Observations and Remote Sensing 5 (2): 396–408.\\nhttps://doi.org/10.1109/JSTARS.2011.2181340.\\nStehman, Stephen V. 1997. “Selecting and interpreting measures of thematic classification\\naccuracy.” Remote Sensing of Environment 62 (1): 77–89.\\nStutz, David, Alexander Hermans, and Bastian Leibe. 2018. “Superpixels: An evalua-\\ntion of the state-of-the-art.” Computer Vision and Image Understanding 166: 1–27.\\nhttps://doi.org/https://doi.org/10.1016/j.cviu.2017.03.007.\\nSubudhi, Subhashree, Ram Narayan Patro, Pradyut Kumar Biswal, and Fabio Dell’acqua.\\n2021. “A Survey on Superpixel Segmentation as a Preprocessing Step in Hyperspectral Image\\nAnalysis.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote\\nSensing 14: 5015–5035. https://doi.org/10.1109/JSTARS.2021.3076005.\\nTao, Chao, Hongbo Pan, Yansheng Li, and Zhengrou Zou. 2015. “Unsupervised spectral–spatial\\nfeature learning with stacked sparse autoencoder for hyperspectral imagery classification.”\\nIEEE Geoscience and remote sensing letters 12 (12): 2438–2442.\\nTu, Bing, Jinping Wang, Xudong Kang, Guoyun Zhang, Xianfeng Ou, and Longyuan Guo.\\n2018. “KNN-based representation of superpixels for hyperspectral image classification.”\\nIEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 11\\n(11): 4032–4047.\\n33\\nVan den Bergh, Michael, Xavier Boix, Gemma Roig, Benjamin De Capitani, and Luc Van Gool.\\n2012. “Seeds: Superpixels extracted via energy-driven sampling.” In Computer Vision–\\nECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13,\\n2012, Proceedings, Part VII 12, 13–26. Springer.\\nVasquez, Dionicio, and Jacob Scharcanski. 2018. “An iterative approach for obtaining multi-\\nscale superpixels based on stochastic graph contraction operations.” Expert Systems with\\nApplications 102: 57–69. https://doi.org/10.1016/j.eswa.2018.02.027.\\nVedaldi, Andrea, and Stefano Soatto. 2008. “Quick Shift and Kernel Methods for Mode Seek-\\ning.” Computer Vision – ECCV 2008 705–718.\\nVeganzones, Miguel A., Guillaume Tochon, Mauro Dalla-Mura, Antonio J. Plaza, and Jocelyn\\nChanussot. 2014. “Hyperspectral image segmentation using a new spectral unmixing-based\\nbinary partition tree representation.” IEEE Transactions on Image Processing 23 (8): 3574–\\n3589. https://doi.org/10.1109/TIP.2014.2329767.\\nWan, Sheng, Chen Gong, Ping Zhong, Bo Du, Lefei Zhang, and Jian Yang. 2019. “Multi-\\nscale dynamic graph convolutional network for hyperspectral image classification.” IEEE\\nTransactions on Geoscience and Remote Sensing 58 (5): 3162–3177.\\nWang, Murong, Xiabi Liu, Yixuan Gao, Xiao Ma, and Nouman Q. Soomro. 2017a. “Super-\\npixel segmentation: A benchmark.” Signal Processing: Image Communication 56: 28–39.\\nhttps://doi.org/https://doi.org/10.1016/j.image.2017.04.007.\\nWang, Rui, Heng-Chao Li, Wenzhi Liao, and Aleksandra Pizurica. 2016. “Double reweighted\\nsparse regression for hyperspectral unmixing.” In 2016 IEEE International Geoscience and\\nRemote Sensing Symposium (IGARSS), jul, 6986–6989. IEEE.\\nWang, Rui, Heng-Chao Li, Aleksandra Pizurica, Jun Li, Antonio Plaza, and William J.\\nEmery. 2017b. “Hyperspectral Unmixing Using Double Reweighted Sparse Regression\\nand Total Variation.” IEEE Geoscience and Remote Sensing Letters 14 (7): 1146–1150.\\nhttps://doi.org/10.1109/LGRS.2017.2700542.\\nWang, Xinyu, Yanfei Zhong, Liangpei Zhang, and Yanyan Xu. 2017c. “Spatial Group\\nSparsity\\nRegularized\\nNonnegative\\nMatrix\\nFactorization\\nfor\\nHyperspectral\\nUnmix-\\ning.” IEEE Transactions on Geoscience and Remote Sensing\\n55 (11): 6287–6304.\\nhttps://doi.org/10.1109/TGRS.2017.2724944.\\nWei, Xing, Qingxiong Yang, Yihong Gong, Narendra Ahuja, and Ming-Hsuan Yang. 2018.\\n“Superpixel Hierarchy.” IEEE Transactions on Image Processing 27 (10): 4838–4849.\\nhttps://doi.org/10.1109/TIP.2018.2836300.\\nYang, Chen, Lorenzo Bruzzone, Haishi Zhao, Yulei Tan, and Renchu Guan. 2018. “Superpixel-\\nbased unsupervised band selection for classification of hyperspectral images.” IEEE Trans-\\nactions on Geoscience and Remote Sensing 56 (12): 7230–7245.\\nYao, Jian, Marko Boben, Sanja Fidler, and Raquel Urtasun. 2015. “Real-time coarse-to-fine\\ntopologically preserving segmentation.” In Proceedings of the IEEE conference on computer\\nvision and pattern recognition, 2947–2955.\\nYe, Chuanlong, Shanwei Liu, Mingming Xu, and Zhiru Yang. 2022. “Combining low-rank\\nconstraint for similar superpixels and total variation sparse unmixing for hyperspectral\\nimage.” International Journal of Remote Sensing 43 (12): 4331–4351.\\nYi, Jiarui, and Miguel Velez-Reyes. 2018. “Low-dimensional enhanced superpixel represen-\\ntation with homogeneity testing for unmixing of hyperspectral imagery.” In Algorithms\\nand Technologies for Multispectral, Hyperspectral, and Ultraspectral Imagery XXIV, Vol.\\n10644 of Society of Photo-Optical Instrumentation Engineers (SPIE) Conference Series,\\nMay, 1064422.\\nYuan, Ye, Wei Zhang, Hai Yu, and Zhiliang Zhu. 2021. “Superpixels With Content-Adaptive\\nCriteria.” IEEE Transactions on Image Processing 30: 7702–7716.\\nZhang,\\nShaoquan,\\nJun\\nLi,\\nHeng-Chao\\nLi,\\nChengzhi\\nDeng,\\nand\\nAntonio\\nPlaza.\\n2018. “Spectral–Spatial Weighted Sparse Regression for Hyperspectral Image Unmix-\\ning.”\\nIEEE\\nTransactions\\non\\nGeoscience\\nand\\nRemote\\nSensing\\n56\\n(6):\\n3265–3276.\\nhttps://doi.org/10.1109/TGRS.2018.2797200.\\nZhang, Shaoquan, Jun Li, Javier Plaza, Heng-Chao Li, and Antonio Plaza. 2017. “Spatial\\n34\\nweighted sparse regression for hyperspectral image unmixing.” In 2017 IEEE International\\nGeoscience and Remote Sensing Symposium (IGARSS), jul, 225–228. IEEE.\\nZhang, Xinxin, Yuan Yuan, and Xuelong Li. 2022. “Reweighted Low-Rank and Joint-Sparse\\nUnmixing With Library Pruning.” IEEE Transactions on Geoscience and Remote Sensing\\n60: 1–16.\\nZhao,\\nYang,\\nYuan\\nYuan,\\nand\\nQi\\nWang.\\n2019.\\n“Fast\\nSpectral\\nClustering\\nfor\\nUnsupervised\\nHyperspectral\\nImage\\nClassification.”\\nRemote\\nSensing\\n11\\n(4).\\nhttps://doi.org/10.3390/rs11040399.\\nZheng, Cheng Yong, Hong Li, Qiong Wang, and C.L. Philip Chen. 2016. “Reweighted Sparse\\nRegression for Hyperspectral Unmixing.” IEEE Transactions on Geoscience and Remote\\nSensing 54 (1): 479–488. https://doi.org/10.1109/TGRS.2015.2459763.\\nZou, Jinlin, and Jinhui Lan. 2019. “A multiscale hierarchical model for sparse Hyperspectral\\nunmixing.” Remote Sensing 11 (5). https://doi.org/10.3390/rs11050500.\\n35\\n(a)\\nFalse-\\ncolour\\n(b)\\nGround-\\ntruth\\n(c)\\nCEGCNSLIC\\n(d)\\nCEGCNSLIC,\\n30dB\\n(e)\\nCEGCNSLIC,\\n20dB\\n(f)\\nCEGCNWPX\\n(g)\\nCEGCNWPX,\\n30dB\\n(h)\\nCEGCNWPX,\\n20dB\\n(i)\\nCEGCNH2BO\\n(j)\\nCEGCNH2BO,\\n30dB\\n(k)\\nCEGCNH2BO,\\n20dB\\nFigure 14. Salinas – false-colour representation, ground-truth and classification maps.\\n36\\n(a) False-colour\\n(b) Ground-truth\\n(c) CEGCNSLIC (d) CEGCNSLIC,\\n30dB\\n(e) CEGCNSLIC,\\n20dB\\n(f) CEGCNWPX (g)\\nCEGCNWPX,\\n30dB\\n(h)\\nCEGCNWPX,\\n20dB\\n(i) CEGCNH2BO (j)\\nCEGCNH2BO,\\n30dB\\n(k)\\nCEGCNH2BO,\\n20dB\\nFigure 15. Pavia University – false-colour representation, ground-truth and classification maps.\\n37\\n')])\n",
      "ResearchTopics(topic='Unsupervised image segmentation using deep clustering and contrastive learning', priority=1, query='contrastive unsupervised segmentation', timestamp='2024-11-23 18:21:07', research_papers=[ResearchPaper(title='A Dynamically Weighted Loss Function for Unsupervised Image Segmentation', authors=[arxiv.Result.Author('Boujemaa Guermazi'), arxiv.Result.Author('Riadh Ksantini'), arxiv.Result.Author('Naimul Khan')], abstract='Image segmentation is the foundation of several computer vision tasks, where\\npixel-wise knowledge is a prerequisite for achieving the desired target. Deep\\nlearning has shown promising performance in supervised image segmentation.\\nHowever, supervised segmentation algorithms require a massive amount of data\\nannotated at a pixel level, thus limiting their applicability and scalability.\\nTherefore, there is a need to invest in unsupervised learning for segmentation.\\nThis work presents an improved version of an unsupervised Convolutional Neural\\nNetwork (CNN) based algorithm that uses a constant weight factor to balance\\nbetween the segmentation criteria of feature similarity and spatial continuity,\\nand it requires continuous manual adjustment of parameters depending on the\\ndegree of detail in the image and the dataset. In contrast, we propose a novel\\ndynamic weighting scheme that leads to a flexible update of the parameters and\\nan automatic tuning of the balancing weight between the two criteria above to\\nbring out the details in the images in a genuinely unsupervised manner. We\\npresent quantitative and qualitative results on four datasets, which show that\\nthe proposed scheme outperforms the current unsupervised segmentation\\napproaches without requiring manual adjustment.', url='http://arxiv.org/abs/2403.11266v1', pdf_path='./papers/2403.11266v1.A_Dynamically_Weighted_Loss_Function_for_Unsupervised_Image_Segmentation.pdf', content='A Dynamically Weighted Loss Function for\\nUnsupervised Image Segmentation\\n1st Boujemaa Guermazi\\nElectrical, Computer, and Biomedical Engineering\\nToronto Metropolitan University\\nToronto, Ontario\\nbguermazi@ryerson.ca\\n2nd Riadh Ksantini\\nComputer Science\\nUniversity of Bahrain\\nZallaq, Bahrain\\nrksantini@uob.edu.bh\\n3rd Naimul Khan\\nElectrical, Computer, and Biomedical Engineering\\nToronto Metropolitan University\\nToronto, Ontario\\nn77khan@ryerson.ca\\nAbstract—Image segmentation is the foundation of several\\ncomputer vision tasks, where pixel-wise knowledge is a pre-\\nrequisite for achieving the desired target. Deep learning has\\nshown promising performance in supervised image segmentation.\\nHowever, supervised segmentation algorithms require a massive\\namount of data annotated at a pixel level, thus limiting their\\napplicability and scalability. Therefore, there is a need to invest\\nin unsupervised learning for segmentation. This work presents\\nan improved version of an unsupervised Convolutional Neural\\nNetwork (CNN) based algorithm that uses a constant weight\\nfactor to balance between the segmentation criteria of feature\\nsimilarity and spatial continuity, and it requires continuous\\nmanual adjustment of parameters depending on the degree of\\ndetail in the image and the dataset. In contrast, we propose a\\nnovel dynamic weighting scheme that leads to a flexible update\\nof the parameters and an automatic tuning of the balancing\\nweight between the two criteria above to bring out the details\\nin the images in a genuinely unsupervised manner. We present\\nquantitative and qualitative results on four datasets, which show\\nthat the proposed scheme outperforms the current unsupervised\\nsegmentation approaches without requiring manual adjustment.\\nIndex Terms—Image segmentation, unsupervised learning\\nI. INTRODUCTION\\nImage segmentation is fundamental to many application\\ndomains such as medical imaging, surveillance, self-driving\\ncars, and sports. Image classification allocates a category label\\nto the entire image. In contrast, image segmentation generates\\na category label for each input image pixel, dividing a whole\\npicture into subgroups known as image segments. Although\\nwe have made notable progress, the segmentation process is\\nstill challenging due to various factors such as illumination\\nvariation, occlusion, and background clutters.\\nClassical pioneering segmentation techniques such as active\\ncontour models (ACM) [1], k-means [2], and graph-based\\nsegmentation method (GS) [3] impose global and local data\\nand geometry constraints on the masks. As a result, these tech-\\nniques become sensitive to initialization and require heuristics\\nsuch as point resampling, making them unsuitable for modern\\napplications.\\nThis work was funded by the Natural Sciences and Engineering Research\\nCouncil of Canada.\\nCode\\nfor\\nthis\\nproject\\nis\\navailable\\nat:\\nhttps://github.com/bijou-\\nbijou/DynamicSeg\\nThe current state-of-the-art models are deep learning meth-\\nods based on convolutional neural networks (CNNs), espe-\\ncially the Mask R-CNN framework [4], which has been\\nsuccessfully put into practice for supervised semantic and\\ninstance image segmentation. Even so, such methods require\\na considerable amount of hand-labeled data, limiting their\\napplicability in many areas. The problem becomes much more\\nacute when it comes to pixel-wise classification, where the\\nannotation cost per image is expensive. A possible solution to\\nthis problem is unsupervised image segmentation, where the\\nimage is automatically segmented into semantically similar\\nregions. The task has been studied as a clustering problem\\nin recent literature, reaching promising results. The Differen-\\ntiable Feature Clustering [5] is a state-of-the-art CNN-based\\nclustering algorithm that simultaneously optimizes the pixel\\nlabels and feature representations [5]. It applies a combina-\\ntion of feature similarity and spatial continuity constraints\\nto backpropagate the model’s parameters. Feature similarity\\ncorresponds to the constraint that pixels in the same cluster\\nshould be similar to each other. Spatial continuity refers to\\nthe constraint that pixels in the same cluster should be next\\nto each other (continuous). However, to reach the desired\\nsegmentation result, [5] applies a manual parameter tuning to\\nfind the optimal balancing weight µ, which fails to achieve a\\ngood balance between the two aforementioned constraints de-\\npending on the degree of details in the image and dataset. This\\nwork introduces a novel dynamic weighting scheme that leads\\nto a flexible update of the parameters and an automatic tuning\\nof the balancing weight µ. We achieve this by conditioning the\\nvalue of µ to the number of predicted clusters and iteration\\nnumber. We dynamically prioritize one of the constraints at\\neach iteration to achieve a good balance. Experimental results\\non four benchmark datasets show that our method achieves\\nbetter quantitative metrics and qualitative segmentation results\\nby striking a better balance between feature similarity and\\nspatial continuity.\\nII. RELATED WORKS\\nMachine learning methods directly address the image seg-\\nmentation problem by considering various features found in\\nthe image, such as colour or pixel information. K-means\\narXiv:2403.11266v1  [eess.IV]  17 Mar 2024\\nFig. 1. The CNN Framework: a forward-backward process which is iterated T times to obtain the final prediction of the cluster labels cn.\\nclustering [2], for instance, is a region-based method that\\ndivides an image into K groups based on the discontinuity\\nproperties of the extracted features. K-means is widely used\\nfor unsupervised segmentation. However, the hard cluster-\\ning that assumes sharp boundaries between clusters in K-\\nmeans does not guarantee continuous areas. The graph-based\\nsegmentation (GS) [3] is another region-based method that\\nperforms segmentation based on pixel similarity, distance,\\nor colour weights. As a result, GS preserves details in low\\nvariability image regions while ignoring details in high vari-\\nability regions, plus GS has a complex computation. On the\\nother hand, The Invariant Information Clustering [6] is an\\nedge-based method and is easy to implement and rigorously\\ngrounded in information theory. The deep net IIC directly\\ntrains a randomly initialized neural network into semantic\\nclusters without the need for postprocessing to cluster the high-\\ndimensional representations.\\nMajority of the recent unsupervised segmentation models\\nutilize deep learning. X. Xia et al. [7] tackles the problem\\nof unsupervised image segmentation inspired by the concept\\nof U-net architecture [8]. They join two U-net structures\\ninto an auto-encoder followed by a post-processing phase to\\nrefine its prime segmentation. The model merges segments\\nusing a Hierarchical Segmentation method to form the final\\nimage segments. It is a computationally expensive process\\nand requires extensive hyperparameter tuning. L. Zhou et al.\\nproposed another neural network-based algorithm; the Deep\\nImage Clustering (DIC) [9]. DIC divided the image segmen-\\ntation problem into two steps; A feature transformation sub-\\nnetwork (FTS) to extract the features first, then a trainable\\ndeep clustering sub-network (DCS) that groups the pixels\\nto split the image into non-overlapping regions. The DIC\\nmodel has proven to be less sensitive to varying segmentation\\nparameters and has lower computation costs. However, it uses\\nsuperpixels to optimize the model parameters, which results\\nin pre-determined fixed boundaries for segmentation regions.\\nIn contrast to the approaches mentioned above that use\\nintermediate representations followed by post-processing, W.\\nKim et al. propose utilizing a clustering algorithm that jointly\\noptimizes the pixel labels and feature representations and up-\\ndates their parameters using backpropagation [5]. Furthermore,\\nin contrast to the superpixel-based refinement process [10],\\nthe authors present a novel spatial continuity loss function\\nto achieve dynamic segmentation boundaries as opposed to\\nutilizing superpixels. It is a simple process that uses the\\nbackpropagation of the feature similarity loss and a new\\nspatial continuity loss that resolves the problem caused by\\nsuperpixels in [10]. However, the weight coefficient for the\\nloss function used must be adjusted each time according to\\nthe datasets and the degree of detail of the image. Instead, we\\npropose a dynamic method for adjusting the weighting factor\\nautomatically, as described in the following sections.\\nIII. PROPOSED METHOD\\nA. The CNN framework\\nFor a fair comparison, we use the same network architecture\\nand training framework used in [5]. The model is shown in\\nFigure (1). M convolutional components are used to produce\\na p-dimensional feature map r. The CNN subnetwork consists\\nof 2D Conv layers, Relu functions, batch normalizations, and\\na final linear layer classifier that classifies the features of each\\npixel into q′ classes. A batch normalization function is applied\\nto the response map r to get a normalized map r′. Lastly,\\nthe argmax function is used to select the dimension that has\\nthe maximum value in r′\\nn. Each pixel is assigned the corre-\\nsponding cluster label cn, which is identical to allocating each\\npixel to the closest point among the q′ representative points.\\nDuring the backward propagation, first, the loss L (defined in\\nthe next section) is calculated. Then, the convolutional filters’\\nparameters and the classifier’s parameters are updated with\\nstochastic gradient descent. This forward-backward process\\nis iterated T times to obtain the final prediction of the\\ncluster labels cn. The segmentation problem is handled in an\\nunsupervised manner without knowing the exact number of\\nclusters. The latter must be flexible according to the content\\nof the image. Therefore, we must allocate a large number q\\nto the initial cluster labels q’. After that, similar or spatially\\nrelated pixels are iteratively integrated to update the number\\nof clusters q′.\\nB. Proposed dynamic weighting scheme\\nThe loss function in [5] is designed to strike a balance\\nbetween feature similarity and spatial continuity. The loss\\nfunction L from [5] is shown in Equation (1).\\nL = Lsim({r′\\nn, cn}) + µLcon({r′\\nn}),\\n(1)\\nwhere, µ: Weight for balancing; Lsim : feature similarity;\\nLcon: spatial continuity; cn: cluster labels ; r′\\nn: normalized\\nresponse.\\nThe loss function in equation (1) consists of two parts. The\\nfirst part is the feature similarity loss Lsim which is the cross-\\nentropy loss between the normalized response map r′\\nn and the\\ncluster labels cn. Minimizing this loss would reveal network\\nweights that ease the extraction of more accurate attributes\\nfor segmentation. Thus, Lsim ensures that pixels of similar\\nfeatures should be assigned the same label. The second part\\nis the Manhattan Distance L1 Norm of horizontal and vertical\\ndifferences of the response map r′\\nn as a spatial continuity\\nloss which redresses the deficiency caused by superpixels\\n[10]. This additional loss component Lcon has proven to be\\nefficient in removing an excessive number of labels due to\\ntheir complex patterns or textures and ensuring that continuous\\npixels are assigned the same label.\\nWhile the loss L as mentioned above can result in reason-\\nably accurate unsupervised segmentation results as reported in\\n[5], the segmentation results are susceptible to the balancing\\nparameter µ. Figure (2) show examples of the sensitivity to\\nthis parameter on an example image from the BSD500 dataset.\\nAs can be seen, for µ = 50 and µ = 100, the segmentation\\nis coarse, resulting in sky, buildings, and coastal regions.\\nHowever, the image is further segmented with µ = 1 and\\nµ = 5, where buildings are further segmented into glass\\nbuildings, concrete buildings, and different floors. Although\\nthe authors argue that the value of µ is proportional to the\\ncoarseness of segmentation, We see that the results are not\\nconsistent, e.g. the segmentation for µ = 50 appears coarser\\nthan µ = 100.\\nThis poses a problem in practice. Such high sensitivity to\\nthe parameter means that for each dataset, this parameter has\\nto be tuned extensively to obtain a result that is semantically\\nmore meaningful.\\nIn this work, we propose changing the weighting param-\\neter’s value during training dynamically. Our observation is\\nthat we can prioritize feature similarity during training at the\\nearlier iterations and gradually shift focus to spatial continuity\\n(or vice versa). We suggest a new dynamic loss function\\nthat includes a continuous variable µ. The weight µ depends\\ndirectly on the number of predicted clusters and iterations. We\\nexamine two versions of the proposed weighting scheme:\\n• Start by gathering continuous regions and shifting focus\\nto feature similarity later. We call this approach Feature\\nSimilarity Focus (FSF). In this case, the performed trials\\nlead to a linear function of the number of clusters (q’)\\nfor the new dynamic balancing weight µ′ = (q′/µ)\\nas shown in equation(2). We tried other versions that\\nFig. 2. Results for different µ values on a sample image from the BSD500\\ndataset using the approach in [5].\\nvary exponentially with the value of q′; However, such\\nfunctions resulted in a rapid change in the value of µ′,\\nwhich was not conducive to the balance we sought to\\nachieve between the two constraints.\\nLF SF = Lsim({r′\\nn, cn}) + (q′/µ)Lcon({r′\\nn})\\n(2)\\n• Start by prioritizing feature similarity criteria early in\\ntraining and end with a higher spatial continuity weight.\\nWe call this approach Spatial Continuity Focus (SCF). In\\nthis case, the proposed dynamic weight would be the mul-\\ntiplicative inverse of the number of clusters µ′ = (µ/q′)\\nas shown in equation (3). Similar to FSF, we tried an\\nexponential form, but the decay was too fast for it to be\\neffective.\\nLSCF = Lsim({r′\\nn, cn}) + (µ/q′)Lcon({r′\\nn})\\n(3)\\nIV. EXPERIMENTAL RESULTS\\nA. Experiment Setup\\nThe objective of the experiments is to show that our\\nproposed dynamic weighting approach provides us with a\\nmore semantically meaningful segmentation. We replicate the\\nexperiments performed in [5]. For all the tests, we fixed the\\nnumber of components in the feature extraction phase M to\\n3. In addition, we set the dimension of the features space p\\nequal to the dimension of cluster space q equal to 100. Finally,\\nwe report the mean Intersection Over Union mIOU overall\\nimages for the benchmark datasets. Ground truth is only used\\nduring the assessment phase and has no bearing on the training\\nprocess.\\nBerkley Segmentation Dataset BSD500 [11] and PASCAL\\nVisual Object Classes 2012 [12] are used to evaluate the\\nsegmentation results quantitatively and qualitatively. BSD500\\nconsists of 500 color and grayscale natural images. Following\\nthe experimental setup in [5], we used the 200 color images\\nof the BSD500 test-set to evaluate all the models. Since\\nTABLE I\\nCOMPARISON OF mIOU FOR UNSUPERVISED SEGMENTATION ON BSD500 AND PASCAL VOC2012. BEST SCORES ARE IN BOLD.\\ndataset\\nMethod\\nBSD500 All\\nBSD500 Fine\\nBSD500 Coarse\\nBSD500 Mean\\nPASCAL VOC2012\\nIIC [6]\\n0.172\\n0.151\\n0.207\\n0.177\\n0.201\\nk-means clustering\\n0.240\\n0.221\\n0.265\\n0.242\\n0.238\\nGraph-based Segmentation [3]\\n0.313\\n0.295\\n0.325\\n0.311\\n0.286\\nCNN-based + superpixels [10]\\n0.226\\n0.169\\n0.324\\n0.240\\n***\\nCNN-based + weighted loss, µ = 5 [5]\\n0.305\\n0.259\\n0.374\\n0.313\\n0.288*\\nSpatial Continuity Focus µ′ = 100/q′\\n0.329\\n0.288\\n0.406\\n0.341\\n0.289\\nSpatial Continuity Focus µ′ = 50/q′\\n0.330\\n0.290\\n0.407\\n0.342\\n0.290\\nFeature Similarity Focus µ′ = q′/10\\n0.330\\n0.297\\n0.390\\n0.339\\n0.280\\nFeature Similarity Focus µ′ = q′/15\\n0.349\\n0.307\\n0.420\\n0.359\\n0.275\\n* All the results are copied from [5], except for the results on Pascal VOC 2012, as there is no indication in [5] which images from the\\ndataset were used for evaluation. Therefore, we chose 150 random images and re-produced the results for this dataset only.\\nFig. 3. Qualitative Results on select BSD500 and PASCAL VOC2012 images. Same color corresponds to the pixels being assigned the same clustering label\\nby the algorithm. Please read Section IV-C for discussion on these results.\\nthe BSD500 dataset contains multiple types of ground truth,\\nwe set three types of mIOU counting to assess the given\\nresults; ”BSD500 All” takes into account all the ground truth\\nfiles, ”BSD500 Fine” considers the only ground truth file per\\nimage that has the most significant number of segments, and\\n”BSD500 Coarse” takes only the ground truth file that contains\\nthe smallest number of segments. We defined ”BSD500 Mean”\\nas the average value of the above three measurements. For\\nPASCAL VOC2012, we considered each segment an individ-\\nual entity ignoring the object classification. VOC2012 is a\\nlarge dataset containing 17,124 images with 2,913 images with\\nsemantic segmentation annotations. We randomly chose 150\\nof the semantic segmentation images to evaluate our method.\\nFor the Icoseg [13] and Pixabay [14] datasets, select images\\nare used to demonstrate additional qualitative results only. All\\nof the aforementioned experimental settings are identical to\\nthe settings from [5] for a fair comparison.\\nB. Quantitative results\\nTo demonstrate the effectiveness of the dynamic loss func-\\ntion, we compare the results with The Invariant Information\\nClustering (IIC) [6], the k-means clustering [2] and the graph-\\nbased segmentation (GS) [3] in addition to the two CNN-\\nbased methods in [10] and [5]. One important note is that\\nwe report the same results as in [5], except for the results\\nof the method [5] on Pascal VOC 2012, as there is no clue\\nabout how the images were selected. In addition, the method\\nFig. 4.\\nQualitative Results on select Icoseg and Pixabay images. Same color corresponds to the pixels being assigned the same clustering label by the\\nalgorithm. Please read Section IV-C for discussion on these results.\\nis sensitive to the random initialization of the neural network\\nweights, resulting in different mIOU scores every time we re-\\nrun the experiment. Therefore, we could not exactly reproduce\\nthe results reported in the paper using their provided code base.\\nFor a fair comparison, we fixed the initialization to a single\\nset of values for all methods, including ours.\\nThe Differentiable Feature Clustering [5] uses a fixed value\\nof µ. Yet five is the ideal value of µ for BSD500 [11], and\\nPASCAL VOC 2012 datasets [12], as that achieved the best\\nresults as reported in [5]. In contrast, for our proposed SCF\\nand FSF methods, the old µ is still the only hyper-parameter\\nthat needs to be tuned, but it is integrated into a dynamic loss\\nfunction, as shown in equations (3) and (2). The best µ for\\nSCF and FSF clustering were experimentally determined from\\n{25, 45, 50, 55, 60, 75 100, 200} and {2, 10, 15, 25,50, 100},\\nrespectively. We report results for µ = 10 and µ = 15 for FSF\\nclustering ; µ = 100 and µ = 50 for SCF clustering.\\nAs illustrated in Table I, the graph-based segmentation (GS)\\nmethod outperformed the Differentiable Feature Clustering [5]\\non “BSD500 all” and “BSD500 fine”. However, our proposed\\nmethod outperforms [10], IIC, k-means, and, in particular, [5]\\nand GS for both datasets obtaining the best mIOU scores.\\nWhile FSF shows the best performance for the BSD500\\ndataset, SCF achieves the best scores in the Pascal VOC2012\\ndataset and the second-best score in the BSD500 dataset.\\nC. Qualitative results\\nWe also provide qualitative results on a few images as done\\nin [5]. As shown in Figure (3), our model is more effective\\nin bringing out segmentation regions that are semantically\\nrelated. For example, For the ”Show Jumping” image (column\\n5), the horse and the obstacle are classified as the same class\\nby [5] (both yellow). However, for both FSF and SCF, the\\nhorse and the obstacle are appropriately distinguished. For the\\n”ship” image (column 2), [5] fails to differentiate between the\\nsky and the body of the ship (both red), but both proposed\\nSCF and FSF can do it successfully.\\nFurther qualitative results are shown for select Icoseg [13]\\nand Pixabay [14] datasets that were also used in [5]. These re-\\nsults can be seen in Figure (4). The qualitative results on these\\ndatasets are presented to further demonstrate that our proposed\\napproaches do not require as much parameter tuning as [5]\\ndoes. Figure (4) highlights that the baseline Differentiable\\nFeature Clustering [5] is quite parameter sensitive. For each\\ndataset, the weighting balance µ must be tuned extensively to\\nobtain a more semantically meaningful result. For instance,\\nPASCAL VOC 2012 and BSD500 datasets require a small\\nbalancing value µ = 5. While, Icoseg [13] and Pixabay\\n[14] datasets need a much larger balance value µ = 50 and\\nµ = 100, respectively. On the other hand, As illustrated in\\nFigures (3) and (4), our proposed method has proven effective\\nin dealing with different datasets using the same weight for\\nboth FSF and SCF. The proposed methods also bring out\\ndetails in an unsupervised manner that is semantically more\\nmeaningful. For example, using the Feature Similarity Focus\\nmethod (FSF), the red car from the iCoseg dataset (column\\n2 row 4 in Figure (4)) displays more detail on the tires and\\nmore precise building outlines than the details extracted by\\n[5] (column 2 row 2), where the car is partially blended into\\nthe road. Similarly, for the peppers image (column 4 in Figure\\n(4)), [5] was unable to identify the shapes of the individual\\npeppers accurately. Both of our proposed method do a much\\nbetter job, even with the same value of µ as the other images.\\nComparing the proposed SCF and FSF, SCF shows potency\\nin class segmentation, while FSF performs better in object\\nsegmentation tasks. For instance, in the third row in Figure (3),\\nthe SCF method segmented the image into the sky, dust, and\\nmonuments. However, the FSF segmented the image into the\\nsky, dust, pyramid, and Sphinx. Depending on the application\\ncontext, one of these approaches could be more favorable.\\nV. CONCLUSION\\nIn this paper, we propose a dynamic weighting scheme for\\nCNN-based unsupervised segmentation. Our proposed FSF\\nand SCF approaches can strike a balance between feature\\nsimilarity and spatial continuity for better segmentation. Qual-\\nitative and quantitative results on four datasets show that\\nour proposed approach outperforms classical methods for\\nunsupervised image segmentation such as k-means clustering\\nand a graph-based segmentation method, and outperforms the\\nstate-of-the-art deep learning model, the Differentiable Feature\\nClustering, [5] while producing semantically more meaningful\\nresults.\\nREFERENCES\\n[1] Michael Kass, Andrew Witkin, and Demetri Terzopoulos,\\n“Snakes:\\nActive contour models,” International Journal of Computer Vision, vol.\\n1, no. 4, pp. 321–331, jan 1988.\\n[2] J.MacQueen et all, “Some methods for classification and analysis of\\nmultivariate observations,” Proceedings of the fifth Berkeley symposium\\non mathematical statistics and probability, vol. 1, no. 14, pp. 281–297,\\n1967.\\n[3] Pedro F. Felzenszwalb and Daniel P. Huttenlocher, “Efficient Graph-\\nBased Image Segmentation,” International Journal of Computer Vision,\\nvol. 59, no. 2, pp. 167–181, sep 2004.\\n[4] Piotr Doll, Ross Girshick, and Facebook Ai, “Mask R-CNN,” CVPR,\\n2017.\\n[5] Wonjik Kim, Asako Kanezaki, and Masayuki Tanaka, “Unsupervised\\nLearning of Image Segmentation Based on Differentiable Feature Clus-\\ntering,” IEEE Transactions on Image Processing, vol. 29, pp. 8055–\\n8068, 2020.\\n[6] J.Henriques X.Ji and A.Vedaldi, “Invariant Information Clustering for\\nUnsupervised Image Classification and Segmentation,” Proceedings of\\nthe IEEE/CVF International Conference on Computer Vision (ICCV),\\n2019.\\n[7] Xide Xia and Brian Kulis, “W-Net: A Deep Model for Fully Unsuper-\\nvised Image Segmentation,” arXiv, nov 2017.\\n[8] Nassir Navab, Joachim Hornegger, William M. Wells, and Alejandro F.\\nFrangi, Medical Image Computing and Computer-Assisted Intervention –\\nMICCAI 2015, vol. 9351 of Lecture Notes in Computer Science, Springer\\nInternational Publishing, Cham, 2015.\\n[9] Lei Zhou and Weiyufeng Wei,\\n“DIC: Deep Image Clustering for\\nUnsupervised Image Segmentation,” IEEE Access, vol. 8, pp. 34481–\\n34491, 2020.\\n[10] Asako Kanezaki, “Unsupervised Image Segmentation by Backpropaga-\\ntion,” Icassp, pp. 2–4, 2018.\\n[11] Pablo Arbel´aez, Michael Maire, Charless Fowlkes, and Jitendra Malik,\\n“Contour detection and hierarchical image segmentation,” IEEE Trans-\\nactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 5,\\npp. 898–916, 2011.\\n[12] M Everingham, M Everingham, A Zisserman, A Zisserman, C Williams,\\nand C Williams, “The PASCAL Visual Object Classes Challenge 2006\\n(VOC2006) results,” International Journal of Computer Vision, vol. 88,\\nno. 2, pp. 303–338, 2010.\\n[13] Dhruv Batra, Adarsh Kowdle, Devi Parikh, Jiebo Luo, and Tsuhan Chen,\\n“iCoseg: Interactive co-segmentation with intelligent scribble guidance,”\\nin 2010 IEEE Computer Society Conference on Computer Vision and\\nPattern Recognition. jun 2010, pp. 3169–3176, IEEE.\\n[14] Asako Kanezaki, “Unsupervised image segmentation by backpropaga-\\ntion,” in 2018 IEEE International Conference on Acoustics, Speech and\\nSignal Processing (ICASSP), 2018, pp. 1543–1547.\\n'), ResearchPaper(title='Contrastive Registration for Unsupervised Medical Image Segmentation', authors=[arxiv.Result.Author('Lihao Liu'), arxiv.Result.Author('Angelica I Aviles-Rivero'), arxiv.Result.Author('Carola-Bibiane Schönlieb')], abstract='Medical image segmentation is a relevant task as it serves as the first step\\nfor several diagnosis processes, thus it is indispensable in clinical usage.\\nWhilst major success has been reported using supervised techniques, they assume\\na large and well-representative labelled set. This is a strong assumption in\\nthe medical domain where annotations are expensive, time-consuming, and\\ninherent to human bias. To address this problem, unsupervised techniques have\\nbeen proposed in the literature yet it is still an open problem due to the\\ndifficulty of learning any transformation pattern. In this work, we present a\\nnovel optimisation model framed into a new CNN-based contrastive registration\\narchitecture for unsupervised medical image segmentation. The core of our\\napproach is to exploit image-level registration and feature-level from a\\ncontrastive learning mechanism, to perform registration-based segmentation.\\nFirstly, we propose an architecture to capture the image-to-image\\ntransformation pattern via registration for unsupervised medical image\\nsegmentation. Secondly, we embed a contrastive learning mechanism into the\\nregistration architecture to enhance the discriminating capacity of the network\\nin the feature-level. We show that our proposed technique mitigates the major\\ndrawbacks of existing unsupervised techniques. We demonstrate, through\\nnumerical and visual experiments, that our technique substantially outperforms\\nthe current state-of-the-art unsupervised segmentation methods on two major\\nmedical image datasets.', url='http://arxiv.org/abs/2011.08894v3', pdf_path='./papers/2011.08894v3.Contrastive_Registration_for_Unsupervised_Medical_Image_Segmentation.pdf', content='1\\nContrastive Registration for\\nUnsupervised Medical Image Segmentation\\nLihao Liu, Angelica I Aviles-Rivero, and Carola-Bibiane Sch¨onlieb\\nAbstract—Medical image segmentation is an important task in\\nmedical imaging, as it serves as the ﬁrst step for clinical diagnosis\\nand treatment planning. Whilst major success has been reported\\nusing deep learning supervised techniques, they assume a large\\nand well-representative labelled set. This is a strong assumption\\nin the medical domain where annotations are expensive, time-\\nconsuming, and inherent to human bias. To address this problem,\\nunsupervised segmentation techniques have been proposed in the\\nliterature. Yet, none of the existing unsupervised segmentation\\ntechniques reach accuracies that come even near to the state of\\nthe art of supervised segmentation methods. In this work, we\\npresent a novel optimisation model framed in a new CNN-based\\ncontrastive registration architecture for unsupervised medical\\nimage segmentation called CLMorph. The core idea of our\\napproach is to exploit image-level registration and feature-level\\ncontrastive learning, to perform registration-based segmentation.\\nFirstly, we propose an architecture to capture the image-to-\\nimage transformation mapping via registration for unsupervised\\nmedical image segmentation. Secondly, we embed a contrastive\\nlearning mechanism in the registration architecture to enhance\\nthe discriminative capacity of the network at the feature level.\\nWe show that our proposed CLMorph technique mitigates\\nthe major drawbacks of existing unsupervised techniques. We\\ndemonstrate, through numerical and visual experiments, that our\\ntechnique substantially outperforms the current state-of-the-art\\nunsupervised segmentation methods on two major medical image\\ndatasets.\\nIndex Terms—Contrastive Learning, Image Registration, Im-\\nage Segmentation, Deep Learning, Brain Segmentation\\nI. INTRODUCTION\\nMedical image segmentation is the task of partitioning an\\nimage into multiple regions, which ideally reﬂect qualities\\nsuch as well-deﬁned structures guided by boundaries in the\\nimage domain. This task has been successfully applied in a\\nrange of medical applications using data coming from different\\nparts of human anatomy including the heart [1], [2], lungs [3]\\nand brain [4]. Medical image segmentation is a relevant task;\\nas it is the ﬁrst step for several clinical applications such as\\ntumor localisation and neuropsychiatric disorder diagnosis.\\nThe body of literature has reported several successful\\ntechniques for medical image segmentation, in which major\\nprogress has been reported using fully-supervised convolu-\\ntional neural networks (CNNs) [5], [6], [7], [8] or partially-\\nsupervised CNN methods [9], [10], [11], [12].These tech-\\nniques rely on learning prior mapping information from pair-\\nwise data (mapping from medical images to their manual\\nsegmentation) to achieve astonishing performance at the same\\nL. Liu., A.I. Aviles-Rivero and C. Sch¨onlieb are with the Department\\nof Applied Mathematics and Theoretical Physics, University of Cambridge.\\nCambridge CB3 0WA, UK. Corresponding author: ll610@cam.ac.uk.\\nlevel as, and sometimes outperforming, radiologists. However,\\na major constraint of these methods is the assumption of\\nhaving a well-representative set of annotations for training,\\nwhich is not always possible in the medical domain. Moreover,\\nobtaining such annotations is time-consuming, expensive, and\\nrequires expert knowledge. To deal with these constraints,\\na body of research has explored unsupervised segmentation\\ntechniquess [13], [14], [15], [16], in which no annotations are\\nneeded.\\nNotably, in recent years unsupervised techniques have\\nbecome a great focus of attention as they do not require\\nsegmentation labels. In this context, clustering algorithms\\nare often applied to perform unsupervised segmentation by\\ngrouping the image content with similar intensities [17],\\n[18], [19], [20]. However, these methods are still limited\\nperformance-wise; as it is difﬁcult to learn any image-to-\\nmask transformation mapping when relying solely on images.\\nTo further embed the transformation mapping information\\nwithin an unsupervised architecture for better medical image\\nsegmentation performance, one feasible solution is to cast the\\nsegmentation task as an unsupervised deformable registration\\nproblem [21], [22], [23], [24], [16], [25], [26], [27], [28], [29],\\n[30]. The goal of this perspective is to ﬁnd an optimal image-\\nto-image transformation mapping to align a set of images in\\none coordinate system.\\nIn this work, we aim to ﬁnd an optimal image-to-image\\ntransformation mapping z between an unaligned image x\\nand a reference image y, which is usually formulated by a\\nmetric function φz. Beneﬁting from similar morphological\\nattributes between the medical image and its corresponding\\nsegmentation mask (such as the shape of different organs),\\none can also transfer the segmentation mask of the reference\\nimage yseg back to the coordinate system of the unaligned\\nimage. This process is with the purpose of obtaining the\\nsegmentation mask of the unaligned image xseg using the\\noptimal mapping information z. Hence, when the image is\\ncorrectly registered, the segmentation mask is automatically\\nobtained, which we called registration-based segmentation\\n(aka atlas-based segmentation).\\nFollowing this philosophy, we develop an unsupervised\\nsegmentation model based on a registration architecture. A\\ncentral observation of existing registration methods is that they\\nonly focus on capturing the mapping information on the image\\nlevel, but fail to enhance the feature-level representation.\\nMost recently, unsupervised feature representation learning\\nhas demonstrated promising results. In particular, contrastive-\\nbased models, such as SimCLR [31] and BYOL [32], are\\nreaching performance comparable to those produced by su-\\narXiv:2011.08894v3  [cs.CV]  20 Jul 2022\\n2\\npervised techniques for different tasks. The main idea is\\nthat by contrasting images to others, the differences between\\nimages are easily remembered within a network (i.e., learning\\ndistinctiveness); thus making the learned feature more robust\\nto discriminate images with different labels. Therefore, to\\nfurther improve the feature-level learning by contrasting the\\nunaligned image x to the reference image y, we propose\\nto embed the contrast feature learning in the registration\\narchitecture to extract feature maps with richer information,\\nproducing better unsupervised segmentation results.\\nIn this paper, we present a novel Contrastive Learning\\nregistration architecture based on VoxelMorph for unsuper-\\nvised medical image segmentation, which we named named\\nCLMorph. Our proposed CLMorph is a simple yet effective\\nunsupervised segmentation model. Unlike existing techniques,\\nour approach combines image-level registration and feature-\\nlevel contrastive representation learning. More speciﬁcally, our\\ntechnique works as follows. We ﬁrst propose two weight-\\nshared feature encoders, where CNN features are extracted\\nfrom the unaligned images and reference images, respectively.\\nThen, by contrasting the extracted CNN features from the\\nunaligned images and reference images, one can obtain CNN\\nfeatures with richer information. Moreover, we use one single\\ndecoder to capture the mapping information z from the con-\\ntrasted CNN features. Finally, we use the spatial transform\\nnetwork of [33] along with the captured z to align the\\nsegmentation mask, of the reference image to the coordinate\\nsystem of unaligned images, to obtain the segmentation mask\\nin an unsupervised manner. Our main contributions are:\\n• We propose a simple yet effective contrastive registration\\narchitecture for unsupervised medical image segmenta-\\ntion, in which we highlight the combination of image-\\nlevel registration architecture and feature-level contrastive\\nlearning, which we called CLMorph. To the best of our\\nknowledge, this is the ﬁrst work that embeds contrastive\\nlearning in a registration architecture for unsupervised\\nsegmentation.\\n• We evaluate our technique using a range of numerical\\nand visual results on two major benchmark datasets. We\\nshow that, by casting the unsupervised segmentation task\\nvia registration with feature-level contrast, we can largely\\nimprove the unsupervised segmentation performance and\\nreduce the performance gap between supervised and\\nunsupervised segmentation techniques.\\n• We demonstrate that our contrastive registration architec-\\nture, as a by-product, can also lead to a better registration\\nperformance than the state-of-the-art unsupervised regis-\\ntration techniques.\\nII. RELATED WORK\\nThe body of literature has reported impressive results for\\nimage segmentation. In this section, we review the existing\\ntechniques in turn.\\nA. Supervised Medical Image Segmentation\\nIn this section, we revise exiting techniques that heavily\\nrely on annotations (i.e. supervised techniques), and the close\\nrelated paradigm of one-shot learning where labels are still\\nneeded for the technique to work.\\nFully-supervised medical image segmentation. Medical\\nimage segmentation has been extensively investigated in the\\nliterature, in which supervised methods have been most suc-\\ncessful. Early works learn to segment the different organs\\nbased on hand-crafted features including thresholding [34],\\nstatistical model [35] and Bayesian model [36]. However, a\\nmajor drawback is that they are not capable of capturing\\nhigh-level semantic information. Thereby, they tend to fail in\\nsegmenting those organs accurately.\\nMore recently, convolutional neural networks (CNN) based\\nmethods, which train with annotations to capture the high-\\nlevel semantic information, have demonstrated remarkable\\nperformance beyond radiologist execution, such as U-Net [5],\\nnnU-Net [6], VoxResNet [7], and MedicalNet [8]. Although\\neffective in biomedical image segmentation, these supervised\\nmethods rely heavily on a well-representative set of annota-\\ntions. This is a strong assumption in the medical domain as\\nannotations are expensive to obtain (in many medical tasks\\nthere is a need for at least two readers), highly uncertain, and\\nrequire expert knowledge. Hence, when applying the trained\\nmodel to another dataset such approaches often fail to segment\\ncorrectly.\\nOne-shot medical image segmentation. Recently, the com-\\nmunity has moved towards the perspective of developing\\ntechniques that require a tiny labelled set. For example, the\\nprinciples of one-shot learning have been reported in the\\nliterature for medical image segmentation. For example, the\\nauthors of [9] proposed DataAug, which uses the learned\\nspatial and intensity transformation to synthesize labelled\\nimages for one-shot medical image segmentation. Another\\nwork using this philosophy is LT-Net [10]. In that work, the\\nauthors embedded a cycle consistency loss into a registration\\narchitecture to perform one-shot medical image segmentation.\\nHowever, in the training process, one still needs to provide a\\nset of labels that are well-representative for the task at hand.\\nAn alternative to those sets of techniques is unsupervised\\nlearning which has recently been a great focus of attention in\\nthe medical imaging community. This paradigm is the focus\\nof this work and existing techniques are discussed next.\\nB. Unsupervised Medical Image Segmentation\\nClustering. For unsupervised segmentation, clustering algo-\\nrithms [17], [18], [19], [20] have been extensively explored.\\nThe idea is to divide the image into different groups of pix-\\nels/voxels according to the similarity of image intensities (i.e.,\\neach pixel value). Besides clustering techniques, works based\\non Autoencoders [37] and GANs [38] have also been explored\\nfor unsupervised medical image segmentation. However, due\\nto the lack of segmentation masks, which determines if it is\\nimpossible to learn any image-to-mask mapping information,\\nthese algorithms are still inefﬁcient for the task of segmenta-\\ntion.\\nRegistration-based segmentation. To enable the mapping\\ninformation in the training process, another feasible solution\\nis to cast the segmentation task an unsupervised deformable\\n3\\nSmooth \\nLoss \\n STN\\nUnaligned \\nImage (x) \\nReference \\nImage (y) \\nTransform \\nPattern (  )        \\nReconstruction\\nLoss \\nSTN\\nRegistered\\nImage\\nWeighed Layer \\nConcatenation \\nProjection Layer\\nLoss Input \\nFeature Map \\nTraining \\nContrastive \\nLoss \\nTesting \\nFig. 1. The overall workﬂow of the proposed network architecture: CLMorph. It takes two images as input for the two CNNs, which generates two sets of\\nfeature maps (see blue and yellow blocks). Two projection layers are applied to the feature maps to produce two vector presentations of the input images.\\nBased on these vector presentations, a contrastive loss is employed. It then recursively concatenates and enlarges the contrasted CNN feature maps, from the\\ntwo CNNs, using a single decoder until the transformation mapping is obtained. Based on the transformation mapping, the reconstruction and smooth loss\\nare adopted to ensure that the registration is well-performed. After the above training process is completed, we adopt the learned transformation mapping to\\ntransfer the reference mask to obtain the segmentation mask of the unaligned image.\\nregistration process [21], [22], [23], [24], [16], [25], [26], [27],\\n[28], [29], [30]. Instead of seeking the image-to-mask transfor-\\nmation mapping, the goal of unsupervised registration methods\\nis to ﬁnd an optimal image-to-image transformation mapping.\\nSpeciﬁcally, given an unaligned image x and a reference image\\ny, the main goal of unsupervised registration methods is to\\ncalculate the latent variable z, which contains the image-to-\\nimage transformation mapping information [26], [27]. Bene-\\nﬁted from the morphological similarity between the medical\\nimage and its corresponding segmentation mask, unsupervised\\nregistration architecture can transfer the segmentation mask\\nof the reference image yseg back to the coordinate system\\nof the unaligned image to obtain the segmentation mask of\\nthe unaligned image xseg, using the optimal transformation\\nmapping z. Hence, as long as the image is correctly registered,\\nthe segmentation mask is automatically obtained, which we\\ncall registered segmentation.\\nContrastive learning. The current unsupervised registration\\nmethods only focus on capturing the mapping information\\nfrom the image level and ignore the feature-level represen-\\ntation learning. To further enhance the feature representa-\\ntion learning without annotations, a body of researchers has\\ndemonstrated promising results using contrastive representa-\\ntion learning including Momentum Contrast (MoCo) [39],\\nSimCLR [31], Contrastive Multiview Coding (CMC) [40],\\nand BYOL [32]. The main idea of contrastive representa-\\ntion learning is to maximise the differences between images\\nfrom different groups as well as to maximise the agreements\\nbetween images and their different augmented views, such\\nas SimCLR [31]. Another research line relies on teacher-\\nstudent learning mechanisms to learn from each other without\\ngroup difference (no negative pairs), such as BYOL [32]. Both\\ndirections are effective for learning features more robust to\\ndiscriminate images from different groups. We show these\\nﬁndings in our experiments.\\nCurrent contrastive learning methods mainly focus on im-\\nproving the discriminating capacity of CNN-based models on\\nimage classiﬁcation tasks. Likewise, other techniques directly\\nemploy contrastive learning methods for the downstream seg-\\nmentation tasks in a supervised setting [41] or semi-supervised\\nmanner [41], [42]. To further transfer the robust feature-\\nlevel learning of the contrastive learning mechanism to the\\ndownstream tasks (i.e., segmentation), we propose to embed\\nthe contrast feature learning in the registration architecture to\\nextract feature maps with richer information for unsupervised\\nsegmentation. To our best knowledge, this is the ﬁrst work that\\nembeds contrastive learning in a registration architecture for\\nunsupervised segmentation\\nIII. PROPOSED TECHNIQUE\\nThis section contains two key parts, the description of:\\ni) the registration architecture which employs a CNN-based\\nprobabilistic model to calculate image-to-image transforma-\\ntion mapping z, and ii) our new full optimisation model that is\\ncomposed of a reconstruction and a smooth loss and combined\\nwith a contrastive learning mechanism.\\nA. Our Technique Overview\\nIn this section, we describe the workﬂow of our technique,\\nin which we highlight their main components that are de-\\nscribed in detail in the following subsections.\\n4\\nWe display the overall workﬂow of our unsupervised seg-\\nmentation technique in Figure 1. Our technique takes as input\\ntwo 3D images, the unaligned image x and the reference\\nimage y, which are fed into a siamese (two-symmetric) weight-\\nshared 3D CNNs to extract the highly semantic feature maps\\nfrom the unaligned and reference images – these parts are\\nillustrated in blue and yellow colours in Figure 1. We then\\nemploy a contrastive loss on the projection of the two extracted\\nCNN feature maps, which forces the network to contrast\\nthe difference between the two CNN feature maps. The two\\nsymmetric weight-shared CNNs can generate more robust\\nCNN feature maps via back-propagating the contrastive loss\\nduring training.\\nBased on the contrasted feature maps of the two CNNs,\\nwe use a single decoder to integrate all the feature maps,\\nwith different resolutions of the two CNNs, and estimate the\\ntransformation mapping z – see the green part in Figure 1. We\\nﬁrst concatenate the feature maps with the same resolutions\\nof the two CNNs. We then adopt a decoder to recursively use\\nthe feature maps, with high-level semantic information (low\\nresolutions) to reﬁne the feature maps with low-level detailed\\ninformation (high resolutions), until we obtain a feature map\\nwith the same resolution as the input images. We introduce\\na probabilistic model to estimate the optimal transformation\\nmapping z, which is based on the recovered image-resolution\\nfeature map. After training and ﬁnding the optimal z, we\\nobtain the segmentation output by aligning the segmentation\\nmask of the reference images to the coordinate system of the\\nunaligned image. We do this using the optimal z via a spatial\\ntransform network [33].\\nB. Transformation Mapping Estimation\\nGiven a pair images called unaligned image x : X →Ωand\\na reference image y : X →Ω, where X = [w]×[h]×[d] ⊂Z3\\nis the input domain and Ωis the value data domain (i.e., gray\\nscale), we seek to determine an optimal transformation map-\\nping z, which parametrises a spatial transformation function\\ndenoted as ψz, such that the transformed unaligned image,\\nx ◦φz, is aligned with y.\\nTo compute the transformation mapping, we estimate z\\nby maximising the posterior registration probability p(z|y; x)\\ngiven x and y – that is, to estimate the central tendency of\\nthe posterior probability (maximum a posteriori estimation\\nMAP). However, solving the partition function is intractable\\nand cannot be solved analytically. To address this problem,\\none can use variational inference and approximate the solution\\nthrough an optimisation problem over variational parameters.\\nFollowing this principle, we adopt a CNN-based variational\\napproach to compute p(z|y; x). We ﬁrst introduce an ap-\\nproximate posterior probability qψ(z|y; x) which we assume\\nis normally distributed. To measure the similarity between\\nthese two distributions, a divergence D(qψ(z|y; x)||p(z|y; x))\\nmeasure can be applied, e.g., [43], [44], [45]. We use the most\\ncommonly used divergence: the Kullback-Leibler (KL) diver-\\ngence [46], [47]. With this purpose, we seek to minimise the\\nKL divergence from qψ(z|y; x) to p(z|y; x) which expression\\nreads:\\nψ∗= min\\nψ KL[qψ(z|y; x) || p(z|y; x)]\\n= min\\nψ Eq[log qψ(z|y; x) −log p(z|y; x)]\\n= min\\nψ Eq[log qψ(z|y; x) −log p(z, y; x)\\np(y; x) ]\\n= min\\nψ Eq[log qψ(z|y; x) −log p(z, y; x) + log p(y; x)]\\n= min\\nψ Eq[log qψ(z|y; x) −log p(z, y; x)] + log p(y; x)\\n= min\\nψ KL[qψ(z|y; x) || p(z)] −Eq[log p(y|z; x)] + const,\\n(1)\\nwhere Eq[log p(y|z; x)] is the evidence, KL[qψ(z|y; x) is the\\nevidence lower bound (ELBO), and const is a normalisation\\nconstant. qψ(z|y; x) comes from a multivariate normal distri-\\nbution N:\\nqψ(z|y; x) = N(z; µz|y;x, σ2\\nz|y;x) ,\\n(2)\\nwhere µz|y;x and σ2\\nz|y;x denote the mean and variance of the\\ndistribution respectively, which can be directly learned through\\nthe convolutional layers; as shown in Figure 1. Whilst p(z) and\\np(y|z; x) follow the multivariate normal distribution, which are\\nmodelled as:\\np(z) = N(z; 0, σ2\\nz) ,\\n(3)\\np(y|z; x) = N(y; x ◦φz, σ2) ,\\n(4)\\nwhere σz is the variance (a diagonal matrix) of this distribution\\nand x◦φz is the noisy observed registered image in which σ2\\nis the variance of the noisy term. We remark that whilst several\\nworks in the literature have addressed the problem of image\\nregistration from a probabilistic perspective, for example, the\\nworks of that [21], [16], we emphasise that our framework\\ngoes beyond a solely probabilistic principle, and in fact, our\\nnovel function is based on other principles such as contrastive\\nterm, smooth and reconstruction terms. They are explained in\\nthe next subsection.\\nC. Full Optimisation Model\\nIn this section, we detail our optimisation model which\\nis composed of a reconstruction and smooth loss, and a\\ncontrastive mechanism.\\nReconstruction & Smooth loss. According to the deriva-\\ntion from (1), there are two terms to be optimised. The ﬁrst\\nterm is the KL divergence between the approximate posterior\\nprobability qψ(z|y; x) and the prior probability p(z), and the\\nsecond term is the expected log-likelihood Eq[log p(y|z; x)].\\nBased on our assumptions from (2)-(4), the derivation is\\nwritten as:\\nLcs(ψ; x, y) =KL[qψ(z|y; x) || p(z)] −Eq[log p(y|z; x)]\\n=1\\n2[tr(σ2\\nz|y;x) + ||µz|y;x||2 −log det(σ2\\nz|y;x)]\\n+\\n1\\n2σ2 ||y −x ◦φz||2.\\n(5)\\n5\\nFor sake of clarity in the notation, we decouple Lcs to detail\\nthe terms. The ﬁrst term (the second line) is a close form of\\nKL[qψ(z|y; x). It enforces the posterior qψ(z|y; x) to be close\\nto the prior p(z). We called this term the smooth loss:\\nLsmooth = 1\\n2[tr(σ2\\nz|y;x) + ||µz|y;x||2 −log det(σ2\\nz|y;x)], (6)\\nwhere the log det(σ2\\nz|y;x) is a Jacobian determinant which\\nspatially smooths the means. The second term (the third line)\\nis the expected log-likelihood Eq[log p(y|z; x)]. It enforces\\nthe registered image x ◦φZ to be similar to reference image\\ny, which we refer as our reconstruction loss:\\nLrecon =\\n1\\n2σ2 ||y −x ◦φz||2\\n(7)\\nContrastive\\nloss.\\nContrastive\\nlearning\\nis\\na\\nlearning\\nparadigm that seeks to learn distinctiveness. It aims to max-\\nimise agreements between images and their augmented views,\\nand from different groups via a contrastive loss in a latent\\nspace. In our work, we follow a four components principle\\nfor the contrastive learning process, which is described next.\\nThe ﬁrst component in our contrastive learning process is\\nthe 3D images. The image contents are basically the same\\nincluding the number of brain structures and the relative loca-\\ntions of each structure. They are mainly different in structure\\nsizes. Therefore, we view the unaligned and the reference\\nimages as images sampled from different augmented views.\\nAs our second component, we set the two CNN encoders\\nas weight-shared. This with the purpose of ensuring that the\\nCNN-based encoder can extract uniﬁed CNN features from the\\nunaligned and reference images. For the third component, we\\nadopt a fully-connected layer, as the projection layer, to map\\nthe CNN features to a latent space where contrastive loss is\\napplied. Finally, our fourth component is our contrastive loss\\nfollowing the standard deﬁnition presented in [48], [31].\\nFrom our four-component process, we can now formalise\\nthe loss we used in our framework. It is based on a\\ncontrastive loss [48], [31], [39] which is a function whose\\nvalue is low when the image is similar to its augmented\\nsample and dissimilar to all other samples. Formally, given\\na set of images I, we view our unaligned image x and\\nreference image y as an augmented image pair, and any\\nother images in I as negative samples. Moreover, we denote\\nsim(u, v) =\\nuTv\\n||u||·||v|| as the cosine similarity between u and\\nv. We then deﬁne the contrastive loss function as:\\nLcontrast = −log\\nexp (sim(f(x), f(y))/τ)\\nP\\ni∈I\\n1i̸=x exp (sim(f(x), f(i))/τ)\\n(8)\\nwhere f() denotes the CNN encoder, f(x) and f(y) are the\\ngenerated features from the CNN encoder, and 1i̸=x ∈{0, 1}\\nis an indicator, which values 1 only when i ̸= x. We also\\ndeﬁne τ as a temperature hyperparameter [49].\\nOptimisation model. Our unsupervised framework is com-\\nposed of three loss functions. The ﬁrst two ones are di-\\nrectly derived from the optimisation model described in (5),\\nin which the image-to-image reconstruction loss and the\\ntransformation mapping (deformation ﬁeld) smooth loss are\\nintroduced. Whilst the third loss is the contrastive loss as\\nin (8) which forces the network to contrast the difference\\nbetween unaligned images and reference images. The total loss\\nis formulated as:\\nLtotal = Lrecon + αLsmooth + βLcontrast\\n=\\n1\\n2σ2 ||y −x ◦φz||2 + α\\n\\x121\\n2[tr(σ2\\nz|y;x) + ||µz|y;x||2\\n−logdet(σ2\\nz|y;x)]\\n\\x13\\n+ β\\n\\x12\\n−log\\nexp (sim(f(x), f(y))/τ)\\nP\\ni∈I\\n1i̸=x exp (sim(f(x), f(i))/τ)\\n\\x13\\n,\\n(9)\\nwhere α and β are the hyper-parameters balancing Lrecon,\\nLsmooth and Lcontrast, which we empirically set as 1 and\\n0.01, respectively.\\nIV. EXPERIMENTAL RESULTS\\nIn this section, we describe the range of experiments that\\nwe conducted to validate our proposed technique.\\nA. Dataset Description & Evaluation Protocol\\nBenchmark datasets. We evaluate our technique using two\\nbenchmarking datasets: the LONI Probabilistic Brain Atlas\\n(LPBA40) dataset [50] and the MindBoggle101 dataset [51].\\nThe characteristics are detailed next.\\nThe LPBA40 dataset [50] is composed of a series of maps\\nfrom the regions of the brain. It contains 40 T1-weighted\\n3D brain images, from 40 human healthy volunteers, of size\\n181×217×181 with a uniform space of 1×1×1 mm3. The\\n3D brain volumes were manually segmented to identify 56\\nstructures. Whilst the MindBoggle101 dataset [51] is com-\\nposed of a collection of 101 T1-weighted 3D brain MRIs\\nfrom several public datasets: OASIS-TRT-20, NKI-TRT-20,\\nNKI-RS-22, MMRR-21, and Extra-18. It contains 101 skull-\\nstripped T1-weighted 3D brain MRI volumes, from healthy\\nsubjects, of size 182×218×182 that are evenly spaced by\\n1×1×1 mm3. From this dataset, we use 62 MRI images\\nfrom OASIS-TRT-20, NKI-TRT-20, and NKI-RS-22, since\\nthey are already wrapped to MNI152 space and have manual\\nsegmentation masks (50 anatomical labels).\\nEvaluation metrics. To evaluate our technique against the\\nstate-of-the-art unsupervised brain image segmentation, we use\\nthree widely-used metrics: the Dice similarity coefﬁcient [52],\\nHausdorff distance (HD) [53], and average symmetric sur-\\nface distance (ASSD) [54]. Dice measures the region-based\\nsimilarity between the predicted segmentation and the ground\\ntruth. The higher the Dice is, the better the model performs.\\nWhilst HD and ASSD measure the longest distance (a.k.a.\\nlargest difference) and average surface distance (a.k.a. average\\nboundary difference) between the predicted segmentation and\\nthe ground truth, respectively. The lower the HD and ASSD\\nis, the better the model performs. The detailed formulation can\\nbe found in [55]\\n6\\nTABLE I\\nNUMERICAL COMPARISON OF OUR TECHNIQUE VS SOTA TECHNIQUES FOR THE LPBA40 AND MINDBOGGLE101 DATASETS. THE NUMERICAL VALUES\\nDISPLAY THE AVERAGE OF DICE, HD, AND ASSD OVER ALL REGIONS. THE HIGHER THE DICE IS THE BETTER THE MODEL PERFORMS. WHILST, THE\\nLOWER THE HD AND ASSD ARE THE BETTER THE MODEL PERFORMS. THE BEST PERFORMANCE IS DENOTED IN BOLD FONT. THE PER-REGION RESULTS\\nARE ILLUSTRATED IN FIG. 2 AND FIG. 3.\\nLPBA40 DATASET\\nMindBoggle101 DATASET\\nDice\\nHD\\nASSD\\nDice\\nHD\\nASSD\\nUtilzReg [56]\\n0.665±0.06\\n14.16±2.34\\n1.906±0.09\\n0.439±0.17\\n18.21±3.04\\n0.972±0.09\\nSyN [57]\\n0.701±0.04\\n13.61±2.56\\n1.986±0.08\\n0.543±0.16\\n16.83±3.68\\n0.926±0.07\\nVoxelMorph [26]\\n0.716±0.08\\n11.38±3.46\\n1.874±0.41\\n0.559±0.12\\n15.03±2.92\\n1.171±0.10\\nFAIM [58]\\n0.729±0.07\\n11.04±4.11\\n1.809±0.40\\n0.583±0.04\\n14.50±2.71\\n1.108±0.08\\nLapIRN [28]\\n0.739±0.07\\n10.77±3.85\\n1.727±0.60\\n0.607±0.04\\n14.54±2.58\\n1.041±0.08\\nVTN [29]\\n0.745±0.08\\n11.11±3.41\\n1.601±0.31\\n0.626±0.04\\n14.64±2.57\\n1.007±0.09\\nPDD-Net [30]\\n0.749±0.06\\n10.75±3.77\\n1.714±0.42\\n0.632±0.05\\n15.21±2.92\\n0.990±0.10\\nCLMorph (Ours)\\n0.763±0.07\\n10.38±2.59\\n1.458±0.31\\n0.646±0.04\\n12.76±2.52\\n0.892±0.05\\nFrontal\\nParietal\\nOccipital\\nTemporal\\nCingulate\\nPutamen\\nHippo\\nAverage\\n0\\n10\\n20\\n30\\nHD Comparision on LPBA40\\nFrontal\\nParietal\\nOccipital\\nTemporal\\nCingulate\\nPutamen\\nHippo\\nAverage\\n0\\n1\\n2\\n3\\nASSD Comparision on LPBA40\\nFrontal\\nParietal\\nOccipital\\nTemporal\\nCingulate\\nPutamen\\nHippo\\nAverage\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nDice Comparision on LPBA40 \\nVoxelMorph\\nFAIM\\nLapIRN\\nVTN\\nPDD-Net\\nOurs\\nFig. 2. Numerical comparisons between our framework and existing SOTA\\ntechniques on the LPBA40 dataset. The results are reported in terms of the\\nDice, HD, and ASSD metrics reﬂecting a per region, of the brain, evaluation.\\nB. Implementation Details & Running Scheme\\nIn this section, we provide the implementation details and\\nthe training & testing scheme that we followed to produce the\\nreported results.\\nImplementation details. In our experiments, we follow\\nFrontal\\nParietal\\nOccipital\\nTemporal\\nCingulate\\nAverage\\n0.2\\n0.4\\n0.6\\n0.8\\nDice Comparision on MindBoggle101\\nFrontal\\nParietal\\nOccipital\\nTemporal\\nCingulate\\nAverage\\n0\\n10\\n20\\n30\\nHD Comparision on MindBoggle101\\nFrontal\\nParietal\\nOccipital\\nTemporal\\nCingulate\\nAverage\\n0.6\\n0.9\\n1.2\\n1.5\\n1.8\\n ASSD Comparision on MindBoggle101\\nFig. 3.\\nComparisons on the MindBoggle101 dataset. We compare our\\ntechnique against existing SOTA methods. The comparisons displayed are\\nin terms of the Dice, HD, and ASSD metrics for each region on the brain.\\nthe training and testing splitting setting from [59] for the\\nLPBA40 dataset, where the ﬁrst 30 images are used as training\\ndatasets and the last 10 images are used as testing datasets. We\\ngroup the 56 structures into seven large regions such that we\\ncan show the segmentation results more intuitively. Moreover\\nand for the MindBoggle101 dataset, we follow the protocol\\n7\\nZoom-In\\nViews\\nZoom-In\\nViews\\nZoom-In\\nViews\\nUnaligned Image\\nReference Image\\nGround Truth\\nFaim\\nVoxelMorph\\nOurs\\nFig. 4. Visual comparisons between our technique and unsupervised SOTA techniques for segmentation. The rows show the three views from the 3D data. The\\nthird column displays the ground truth whilst the last three samples segmentation from our technique and the compared ones. Zoom-in views are displayed\\nto highlight interesting regions where our technique performs better than the other models.\\nfrom [58] and use 42 images from NKI-TRT-20 and NKI-RS-\\n22 for training and 20 images from OASIS-TRT-20 for testing.\\nFor evaluation purposes, we also group the 50 small regions\\ninto ﬁve larger regions of interest. As for the reference image,\\nwe select the image that is the most similar to the anatomical\\naverage as the atlas. In particular, we selected image #30\\nas the reference image for the LPBA40 dataset, and image\\n#39 for the Mindboggle101. Our proposed technique has been\\nimplemented in Pytorch [60].\\nWe follow the standard pre-processing protocol to normalise\\nthe images to have zero mean and unit variance. We randomly\\nselect two images as the pair-wise input data (unaligned image\\nand reference image). Moreover, we enrich our input data by\\napplying three different transformations (data augmentation) in\\nthe following order: random ﬂips in the y coordinate, random\\nrotation with an angle of fewer than 10 degrees, and random\\ncrops of size 160×192×160.\\nTraining scheme. In our training scheme, we initialise the\\nparameters of all convolutional layers following the initialisa-\\ntion protocol of that [61]. We also initialise the parameters of\\nall batch normalisation layers using random variables drawn\\nfrom Gaussian distributions with zero mean and 0.1 deriva-\\ntion [62]. Moreover, we use Adam optimiser [63] with a batch\\nsize of 8. We set the initial learning rate to 3×10−3 and then\\ndecrease it by multiplying 0.1 every 20 epochs, and terminate\\nthe training process after 200 epochs. Due to the small number\\nof the dataset, which is a common issue in the 3D medical\\nimage area, we did not split the data into a validation set from\\nthe training or testing sets for model selection. Hence, we use\\nthe above learning rate decay policy to decrease the learning\\nrate to a small value in the last few epochs to ensure the model\\ndoes not diverge, and save the last epoch model as our ﬁnal\\nmodel. This is a standard protocol followed in the area. Our\\ntechnique took ∼20 hours to train on a single Tesla P100\\nGPU.\\nTesting scheme. After training, we performed unsupervised\\nsegmentation based on the learned parameters. We ﬁrst fed the\\nunaligned image x and the reference image y into the trained\\nnetwork to calculate the transformation relation z. Then, we\\nused a spatial transform network (STN) [33], to align the\\nsegmentation mask of the reference image according to the\\ncalculated z, to obtain the segmentation result of the unaligned\\nimage. On average, our method takes less than 10 seconds to\\nprocess one whole MRI image on a single GPU (Tesla P100\\nGPU).\\nC. Results & Discussion\\nIn this section, we present the numerical and visual results\\noutlined in previous subsections, and discuss our ﬁndings and\\nhow they are compared with current existing techniques.\\n8\\nReference Image\\nUnaligned Image\\nFaim\\nVoxelMorph\\nOurs\\nZoom-In\\nViews\\nZoom-In\\nViews\\nZoom-In\\nViews\\nTransformation \\nPattern \\nFig. 5. Visual comparison of our technique and the SOTA techniques for image registration. The rows display the x, y, and z views from the 3D medical\\ndata. The columns display outputs samples from FAIM, VoxelMorph, and our technique. The zoom-in views show interesting structures that clearly show the\\nimprovement in terms of preserving the brain structures and ﬁne details. The last column (deformation ﬁeld) presents the transformation mapping z between\\nunaligned image and reference image produced by our method.\\nTABLE II\\nNUMERICAL RESULTS FOR OUR REGISTRATION PROCESS. THE REPORTED\\nRESULTS DENOTE THE AVERAGE DICE SCORE OVER ALL REGIONS OF THE\\nBRAIN. THE BEST PERFORMANCE IS IN BOLD.\\nLPBA40\\nMindBoggle101\\nUtilzReg [56]\\n0.665±0.06\\n0.440±0.16\\nSyN [57]\\n0.700±0.04\\n0.540±0.15\\nVoxelMorph [26]\\n0.709±0.07\\n0.558±0.10\\nFAIM [58]\\n0.720±0.08\\n0.566±0.09\\nLapIRN [28]\\n0.727±0.09\\n0.608±0.08\\nVTN [29]\\n0.722±0.06\\n0.598±0.04\\nPDD-Net [30]\\n0.733±0.05\\n0.612±0.08\\nCLMorph (Ours)\\n0.750±0.08\\n0631±0.05\\nNumerical comparison with the state-of-the-art tech-\\nniques. We begin by evaluating our method against the state-\\nof-the-art methods unsupervised brain image registration meth-\\nods on the LPBA40 dataset: UtilzReg [56], VoxelMorph [27],\\nFAIM [58], LapIRN [28], VTN [29], and PDD-Net [30]. We\\nremark that we use the registration architecture from other\\nstate-of-the-arts methods to predict the transformation map-\\nFig. 6. Statistical analysis. Multiple comparisons are performed followed by\\na paired Wilcoxon test and p-values adjusted using the Bonferroni method.\\nOur technique reported signiﬁcant statistically different among all compared\\ntechniques.\\nping z, and then adopt the predicted transformation mapping\\nz to perform segmentation as illustrated in the testing stage in\\nFig. 1 for image segmentation. We report the global results in\\nTable I for the LPBA40 and MindBoggle101 dataset, in order\\nto understand the general behaviour and performance of our\\ntechnique over the SOTA methods. The displayed numbers are\\nthe average of the image metrics across the dataset. In a close\\nlook at the results, we observe that the compared techniques\\nperform similarly close to each other, whilst our technique\\noutperforms all other techniques for all evaluation metrics by\\n9\\nTABLE III\\nABLATION STUDY, IN TERMS OF THE MEAN DICE, HD, AND ASSD METRICS, OF OUR PROPOSED TECHNIQUE. THE TOP PART DISPLAYS A NUMERICAL\\nCOMPARISON OF OUR TECHNIQUE VS FULLY SUPERVISED SEGMENTATION. THE BOTTOM PART DISPLAYS A COMPARISON OF OUR TECHNIQUE WITH ITS\\nDIFFERENT COMPONENTS. THE LAST TWO ROWS DISPLAY OUR FRAMEWORK UNDER DIFFERENT CONTRASTIVE PRINCIPLES BYOL [32].\\nLPBA40\\nMindBoggle101\\nDice\\nHD\\nASSD\\nDice\\nHD\\nASSD\\nFULLY SUPERVISED BASELINE\\nU-Net (Upper Bound)\\n0.832\\n-\\n-\\n0.811\\n-\\n-\\nOUR UNSUPERVISED TECHNIQUE\\nLrecon\\n0.702±0.07\\n12.82±3.33\\n1.842±0.58\\n0.552±0.20\\n15.11±2.26\\n1.210±0.13\\nLrecon + Lsmooth\\n0.716±0.08\\n11.38±3.46\\n1.874±0.41\\n0.559±0.12\\n15.03±2.92\\n1.171±0.10\\nLrecon + Lcontrast\\n0.751±0.10\\n10.92±2.28\\n1.801±0.47\\n0.604±0.14\\n14.85±2.77\\n0.993±0.21\\nLrecon + Lsmooth + Lcontrast (ours)\\n0.763±0.07\\n10.38±2.59\\n1.458±0.31\\n0.646±0.04\\n12.76±2.52\\n0.892±0.05\\nLrecon + Lbyol\\n0.744±0.18\\n11.41±3.29\\n1.693±0.77\\n0.619±0.13\\n14.73±2.39\\n1.153±0.89\\nLrecon + Lsmooth + Lbyol\\n0.761±0.08\\n10.41±2.12\\n1.462±0.53\\n0.649±0.08\\n12.88±2.44\\n0.894±0.10\\nSagittal View\\nCoronal View\\nAxial View\\nCLMorph\\nVoxelMorph\\nFig. 7.\\nDeformation ﬁeld generated from the non-contrasted feature maps\\n(VoxelMorph) and contrasted feature maps (our CLMorph). The red box\\nhighlight the region with large deformation, in which deformation ﬁeld\\ngenerated from contrasted feature maps is smoother than the deformation\\nﬁeld generated from non-contrasted feature maps.\\na signiﬁcant margin. This behaviour is consistent on the per\\nregion results on the two datasets whose results are reported\\nin Fig. 2 and Fig. 3. From these boxplots, we can observe that\\nthe performance gain of our technique is signiﬁcant for all\\nregions in both datasets. These comparisons, therefore, show\\nthat the combination of the contrastive learning mechanism\\nand the registration architecture can better discriminate brain\\nstructures and generate more accurate segmentation outputs.\\nVisual comparisons. To further support our numerical\\nresults, we present a set of visual comparisons of a selection\\nof images for our technique and the compared ones. We\\nstart by displaying the unsupervised segmentation outputs in\\nFigure 4. By visual inspection, we observe that the segmen-\\ntation outputs, generated from FAIM and VoxelMorph, tend\\nto fail to segment correctly several relevant regions of the\\nbrain, and they do not adapt correctly to the contour of the\\nbrain structure. The zoom-in views in Figure 4 highlight these\\neffects – for example the red region on the brain that the\\ncompared techniques fail to correctly segment; and the yellow\\nregion that is not well-captured by FAIM and VoxelMorph. By\\ncontrast, our technique was able to perform better in this regard\\nby capturing ﬁne details in both cases. Overall, our technique\\n0.71\\n0.72\\n0.73\\n0.74\\n0.75\\n0.76\\n0.001\\n0.01\\n0.1\\n1\\n10\\nAlpha\\n0.001\\n0.01\\n0.1\\n1\\n10\\nBeta\\n0.71\\n0.715\\n0.72\\n0.725\\n0.73\\n0.735\\n0.74\\n0.745\\n0.75\\n0.755\\n0.76\\nDice\\n0.763\\nFig. 8. Hyper-parameter Searching. We display different combination values\\nfor α and β used in (9). The parameters values are evaluated using the Dice\\nmetric. The best combination is α = 1 and β = 0.01, which gives the highest\\nDice result of 0.763.\\nis able to accommodate better with the ﬁne details in the brain\\nregions, producing segmentation closer to the ground truth.\\nWe also included the visualisation of the deformation ﬁeld\\ngenerated from the contrasted feature (w/ contrastive loss)\\nand non-contrasted feature (w/o contrastive loss). According\\nto Fig. 7, when the deformation between the moving and\\nﬁxed images is large, the deformation ﬁeld generated from\\nthe contrasted feature is smoother, because the contrasted\\nfeature maps have high alikeness. Whilst the deformation ﬁeld\\ngenerated from non-contrasted features tends to be less smooth\\nbecause there is a huge difference between the feature maps\\nextracted from the moving and ﬁxed images.\\nStatistical Analysis. We support statistically our visual\\nand numerical ﬁndings. To do this, we run a Friedman test\\nfor multiple comparisons, χ2(5)=29.78,p < 0.0001, followed\\nby a Wilcoxon test for pair-wise comparisons. As shown in\\nFig. 6, the pair-wise test between groups revealed a statistically\\nsigniﬁcant difference metric-wise of our technique against the\\nrest of compared techniques.\\nBy-product registration accuracy. As a by-product of\\n10\\nTABLE IV\\nNUMERICAL COMPARISON OF OUR TECHNIQUE VS SOTA TECHNIQUES\\nFOR THE ACDC CARDIAC DATASET.\\nDice\\nHD\\nASSD\\nSyN [57]\\n0.761±0.12\\n11.9±1.79\\n1.581±0.07\\nVoxelMorph [26]\\n0.787±0.09\\n11.8±2.01\\n1.591±0.05\\nLVM-S3 [21]\\n0.795±0.06\\n9.90±2.22\\n1.544±0.06\\nCLMorph (Ours)\\n0.810±0.05\\n10.23±2.08\\n1.482±0.05\\nour methods, we also provide the numerical and the visual\\nregistration results. Based on the learned deformation ﬁeld,\\nwe register the segmentation mask of the unaligned image\\nto the coordinate system of the reference image to get the\\naligned segmentation mask. Then compare it with the refer-\\nence image’s mask to get the dice scores; see Table II. We also\\npresent visual results displayed in Figure 5. We observe that\\nour technique was able to better preserve the ﬁne details in\\nthe registration outputs than the compared techniques. Most\\nnotable, this can be observed in the zoom-in views. For\\nexample, one can observe that our technique is able to preserve\\nbetter the brain structures whilst the compared techniques fail\\nin these terms. Moreover, we can see that our registration\\noutputs are closer to the reference image displaying less blurry\\neffects whilst keeping ﬁne details. We now underline the\\nmain message of our paper: our optimisation model fulﬁlls\\nthe intended purposes, and at this point in time our technique\\noutperforms the SOTA unsupervised segmentation results.\\nHyperparameter searching. We also run a set of new\\nexperiments for the hyperparameter analysis. Speciﬁcally, we\\nset the weight for the smooth loss (α) and contrastive loss\\n( β) as 0.001, 0.01, 0.1, 1, 10, respectively. As shown in\\nFig. 8, the best combination is our current setting (alpha=1\\nand beta=0.01), which gives the highest dice result of 0.763.\\nAblation study. To demonstrate that each component of our\\ntechnique fulﬁlls a purpose, we include an ablation study to\\nevaluate their inﬂuence on performance. We consider our three\\nmajor components in our model whose results are displayed in\\nTable III. Our ablation study is performed on the two bench-\\nmark datasets, in order to understand the general behaviour\\nof our technique. The results are displayed in terms of the\\nDice metric and we progressively evaluate our method with\\ndifferent losses combinations. From the results in Table III,\\nwe can observe that whilst the contrastive mechanism, in\\nour technique, indeed provides a positive effect in terms of\\nperformance, it beneﬁts our carefully designed components.\\nIn this work, we also pose the question of – at this point\\nin time, what is the performance gap between supervised\\nand unsupervised segmentation techniques? To respond to this\\nquestion, we use as baseline U-Net [5]. From the results,\\nin Table III, one can observe that our technique opens the\\ndoor for further investigation in unsupervised techniques, as\\nthe performance shows potential towards the one reported by\\nsupervised techniques.\\nAdditional experiments on a cardiac dataset. We also\\nprovide a set of new results using an additional cardiac dataset\\ncalled ACDC [64]. The dataset is for medical image registra-\\ntion. It is composed of 150 labelled images. We selected 100\\nimages for training whilst the remaining 50 images for testing.\\nThe results are reported in Table IV. From the results, we\\nobserve that our model outperforms the compared techniques\\nin terms of Dice and ASSD metrics by a large margin.\\nWhilst readily competing against LVM-S3 in terms of HD\\nmetric. These results further support the generalisation and\\nperformance of our CLMorph technique.\\nV. CONCLUSION\\nThis paper presents a novel CNN-based registration archi-\\ntecture for unsupervised medical image segmentation. Firstly,\\nwe proposed to use unsupervised registration-based segmenta-\\ntion by capturing the image-to-image transformation mapping.\\nSecondly, to promote the image- and feature-level learning, for\\nbetter segmentation results, we embed a contrastive feature\\nlearning mechanism into the registration architecture. Our\\nnetwork can learn to be more discriminative to the different\\nimages via contrasting unaligned image and reference images.\\nWe show that our carefully designed optimisation model\\nmitigates some major drawbacks of existing unsupervised\\ntechniques. We demonstrate, through several experiments, that\\nour technique is able to report state-of-the-art results for\\nunsupervised medical image segmentation. Whilst supervised\\ntechniques still report better performance than unsupervised\\nones, in this work, we show the potentials in terms of\\nperformance when no labels are available. This is of a great\\ninterest particularly in domains such as the medical area where\\nannotations require expert knowledge and are expensive.\\nACKNOWLEDGEMENTS\\nLL gratefully acknowledges the ﬁnancial support from a\\nGSK scholarship and a Girton College Graduate Research\\nFellowship at the University of Cambridge. AIAR gratefully\\nacknowledges the ﬁnancial support of the CMIH, CCIMI\\nand C2D3 University of Cambridge. CBS acknowledges sup-\\nport from the Philip Leverhulme Prize, the Royal Soci-\\nety Wolfson Fellowship, the EPSRC grants EP/S026045/1\\nand EP/T003553/1, EP/N014588/1, EP/T017961/1, the Well-\\ncome Innovator Award RG98755, the Leverhulme Trust\\nproject Unveiling the invisible, the European Union Horizon\\n2020 research and innovation programme under the Marie\\nSkodowska-Curie grant agreement No. 777826 NoMADS, the\\nCantab Capital Institute for the Mathematics of Information\\nand the Alan Turing Institute.\\nREFERENCES\\n[1] C. Chen, C. Qin, H. Qiu, G. Tarroni, J. Duan, W. Bai, and D. Rueckert,\\n“Deep learning for cardiac image segmentation: A review,” Frontiers in\\nCardiovascular Medicine, vol. 7, p. 25, 2020.\\n[2] J. Liu, A. I. Aviles-Rivero, H. Ji, and C.-B. Sch¨onlieb, “Rethinking\\nmedical image reconstruction via shape prior, going deeper and faster:\\nDeep joint indirect registration and reconstruction,” Medical Image\\nAnalysis, vol. 68, p. 101930, 2021.\\n[3] S. Hu, E. A. Hoffman, and J. M. Reinhardt, “Automatic lung segmen-\\ntation for accurate quantitation of volumetric X-ray CT images,” IEEE\\ntransactions on medical imaging, vol. 20, no. 6, pp. 490–498, 2001.\\n[4] A. de Brebisson and G. Montana, “Deep neural networks for anatomical\\nbrain segmentation,” in Proceedings of the IEEE conference on computer\\nvision and pattern recognition (CVPR), 2015, pp. 20–28.\\n11\\n[5] O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutional net-\\nworks for biomedical image segmentation,” in International Conference\\non Medical image computing and computer-assisted intervention (MIC-\\nCAI).\\nSpringer, 2015, pp. 234–241.\\n[6] F. Isensee, P. F. Jaeger, S. A. Kohl, J. Petersen, and K. H. Maier-Hein,\\n“nnu-net: a self-conﬁguring method for deep learning-based biomedical\\nimage segmentation,” Nature Methods, vol. 18, no. 2, pp. 203–211, 2021.\\n[7] H. Chen, Q. Dou, L. Yu, J. Qin, and P.-A. Heng, “VoxResNet: Deep\\nvoxelwise residual networks for brain segmentation from 3D MR\\nimages,” NeuroImage, vol. 170, pp. 446–455, 2018.\\n[8] S. Chen, K. Ma, and Y. Zheng, “Med3d: Transfer learning for 3d medical\\nimage analysis,” arXiv preprint arXiv:1904.00625, 2019.\\n[9] A. Zhao, G. Balakrishnan, F. Durand, J. V. Guttag, and A. V. Dalca,\\n“Data augmentation using learned transformations for one-shot medical\\nimage segmentation,” in Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition, 2019, pp. 8543–8553.\\n[10] S. Wang, S. Cao, D. Wei, R. Wang, K. Ma, L. Wang, D. Meng,\\nand Y. Zheng, “Lt-net: Label transfer by learning reversible voxel-\\nwise correspondence for one-shot medical image segmentation,” in\\nProceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition, 2020, pp. 9162–9171.\\n[11] N. M. Portela, G. D. Cavalcanti, and T. I. Ren, “Semi-supervised\\nclustering for MR brain image segmentation,” Expert Systems with\\nApplications, vol. 41, no. 4, pp. 1492–1497, 2014.\\n[12] W. Cui, Y. Liu, Y. Li, M. Guo, Y. Li, X. Li, T. Wang, X. Zeng, and\\nC. Ye, “Semi-supervised brain lesion segmentation with an adapted\\nmean teacher model,” in International Conference on Medical image\\ncomputing and computer-assisted intervention (MICCAI).\\nSpringer,\\n2019, pp. 554–565.\\n[13] B. Alfano, A. Brunetti, E. M. Covelli, M. Quarantelli, M. R. Panico,\\nA. Ciarmiello, and M. Salvatore, “Unsupervised, automated segmenta-\\ntion of the normal brain using a multispectral relaxometric magnetic\\nresonance approach,” Magnetic resonance in medicine, vol. 37, no. 1,\\npp. 84–93, 1997.\\n[14] C. Lee, S. Huh, T. A. Ketter, and M. Unser, “Unsupervised connectivity-\\nbased thresholding segmentation of midsagittal brain MR images,”\\nComputers in biology and medicine, vol. 28, no. 3, pp. 309–338, 1998.\\n[15] A. V. Dalca, J. Guttag, and M. R. Sabuncu, “Anatomical priors in\\nconvolutional networks for unsupervised biomedical segmentation,” in\\nProceedings of the IEEE conference on computer vision and pattern\\nrecognition (CVPR), 2018, pp. 9290–9299.\\n[16] A. V. Dalca, E. Yu, P. Golland, B. Fischl, M. R. Sabuncu, and\\nJ. E. Iglesias, “Unsupervised deep learning for bayesian brain MRI\\nsegmentation,” in International Conference on Medical image computing\\nand computer-assisted intervention (MICCAI). Springer, 2019, pp. 356–\\n365.\\n[17] H. Ng, S. Ong, K. Foong, P.-S. Goh, and W. Nowinski, “Medical\\nimage segmentation using k-means clustering and improved watershed\\nalgorithm,” in 2006 IEEE southwest symposium on image analysis and\\ninterpretation.\\nIEEE, 2006, pp. 61–65.\\n[18] B. N. Li, C. K. Chui, S. Chang, and S. H. Ong, “Integrating spatial\\nfuzzy clustering with level set methods for automated medical image\\nsegmentation,” Computers in biology and medicine, vol. 41, no. 1, pp.\\n1–10, 2011.\\n[19] A. Jose, S. Ravi, and M. Sambath, “Brain tumor segmentation using\\nk-means clustering and fuzzy c-means algorithms and its area calcu-\\nlation,” International Journal of Innovative Research in Computer and\\nCommunication Engineering, vol. 2, no. 3, 2014.\\n[20] K. Tian, S. Zhou, and J. Guan, “DeepCluster: A general clustering\\nframework based on deep learning,” in Joint European Conference on\\nMachine Learning and Knowledge Discovery in Databases.\\nSpringer,\\n2017, pp. 809–825.\\n[21] J. Krebs, H. Delingette, B. Mailh´e, N. Ayache, and T. Mansi, “Learning\\na probabilistic model for diffeomorphic registration,” IEEE transactions\\non medical imaging, vol. 38, no. 9, pp. 2165–2176, 2019.\\n[22] S. Zhao, T. Lau, J. Luo, I. Eric, C. Chang, and Y. Xu, “Unsupervised 3d\\nend-to-end medical image registration with volume tweening network,”\\nIEEE journal of biomedical and health informatics, vol. 24, no. 5, pp.\\n1394–1404, 2019.\\n[23] B. D. de Vos, F. F. Berendsen, M. A. Viergever, H. Sokooti, M. Staring,\\nand I. Iˇsgum, “A deep learning framework for unsupervised afﬁne and\\ndeformable image registration,” Medical image analysis, vol. 52, pp.\\n128–143, 2019.\\n[24] J. Zhang, “Inverse-consistent deep networks for unsupervised de-\\nformable image registration,” arXiv preprint arXiv:1809.03443, 2018.\\n[25] A. V. Dalca, G. Balakrishnan, J. Guttag, and M. R. Sabuncu, “Unsu-\\npervised learning for fast probabilistic diffeomorphic registration,” in\\nInternational Conference on Medical Image Computing and Computer-\\nAssisted Intervention.\\nSpringer, 2018, pp. 729–738.\\n[26] G. Balakrishnan, A. Zhao, M. R. Sabuncu, J. Guttag, and A. V. Dalca,\\n“Voxelmorph: a learning framework for deformable medical image\\nregistration,” IEEE transactions on medical imaging, vol. 38, no. 8, pp.\\n1788–1800, 2019.\\n[27] ——, “An unsupervised learning model for deformable medical image\\nregistration,” in Proceedings of the IEEE conference on computer vision\\nand pattern recognition, 2018, pp. 9252–9260.\\n[28] T. C. Mok and A. Chung, “Large deformation diffeomorphic image\\nregistration with laplacian pyramid networks,” in International Confer-\\nence on Medical Image Computing and Computer-Assisted Intervention.\\nSpringer, 2020, pp. 211–221.\\n[29] S. Zhao, Y. Dong, E. I. Chang, Y. Xu et al., “Recursive cascaded\\nnetworks for unsupervised medical image registration,” in Proceedings\\nof the IEEE/CVF international conference on computer vision, 2019,\\npp. 10 600–10 610.\\n[30] M. P. Heinrich, “Closing the gap between deep and conventional\\nimage registration using probabilistic dense displacement networks,” in\\nInternational Conference on Medical Image Computing and Computer-\\nAssisted Intervention.\\nSpringer, 2019, pp. 50–58.\\n[31] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple frame-\\nwork for contrastive learning of visual representations,” arXiv preprint\\narXiv:2002.05709, 2020.\\n[32] J.-B. Grill, F. Strub, F. Altch´e, C. Tallec, P. Richemond, E. Buchatskaya,\\nC. Doersch, B. Avila Pires, Z. Guo, M. Gheshlaghi Azar et al.,\\n“Bootstrap your own latent-a new approach to self-supervised learning,”\\nAdvances in neural information processing systems, vol. 33, pp. 21 271–\\n21 284, 2020.\\n[33] M. Jaderberg, K. Simonyan, A. Zisserman et al., “Spatial transformer\\nnetworks,” in Advances in neural information processing systems, 2015,\\npp. 2017–2025.\\n[34] H. Suzuki and J.-i. Toriwaki, “Automatic segmentation of head MRI im-\\nages by knowledge guided thresholding,” Computerized medical imaging\\nand graphics, vol. 15, no. 4, pp. 233–240, 1991.\\n[35] B. Fischl, D. H. Salat, E. Busa, M. Albert, M. Dieterich, C. Haselgrove,\\nA. Van Der Kouwe, R. Killiany, D. Kennedy, S. Klaveness et al., “Whole\\nbrain segmentation: automated labeling of neuroanatomical structures in\\nthe human brain,” Neuron, vol. 33, no. 3, pp. 341–355, 2002.\\n[36] Y. Yang, A. Tannenbaum, D. Giddens, and A. Stillman, “Automatic seg-\\nmentation of coronary arteries using bayesian driven implicit surfaces,”\\nin 2007 4th IEEE International Symposium on Biomedical Imaging:\\nFrom Nano to Macro.\\nIEEE, 2007, pp. 189–192.\\n[37] C. Baur, B. Wiestler, S. Albarqouni, and N. Navab, “Scale-space\\nautoencoders for unsupervised anomaly segmentation in brain mri,” in\\nInternational Conference on Medical Image Computing and Computer-\\nAssisted Intervention.\\nSpringer, 2020, pp. 552–561.\\n[38] Y. Song, T. Zhou, J. Y.-C. Teoh, J. Zhang, and J. Qin, “Unsupervised\\nlearning for ct image segmentation via adversarial redrawing,” in In-\\nternational Conference on Medical Image Computing and Computer-\\nAssisted Intervention.\\nSpringer, 2020, pp. 309–320.\\n[39] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast\\nfor unsupervised visual representation learning,” in Proceedings of the\\nIEEE conference on computer vision and pattern recognition (CVPR),\\n2020, pp. 9729–9738.\\n[40] Y. Tian, D. Krishnan, and P. Isola, “Contrastive multiview coding,” arXiv\\npreprint arXiv:1906.05849, 2019.\\n[41] X. Zhao, R. Vemulapalli, P. Mansﬁeld, B. Gong, B. Green, L. Shapira,\\nand Y. Wu, “Contrastive learning for label-efﬁcient semantic segmenta-\\ntion,” arXiv preprint arXiv:2012.06985, 2020.\\n[42] K. Chaitanya, E. Erdil, N. Karani, and E. Konukoglu, “Contrastive\\nlearning of global and local features for medical image segmentation\\nwith limited annotations,” arXiv preprint arXiv:2006.10511, 2020.\\n[43] S.-I. Amari, “\\\\alpha -divergence is unique, belonging to both f-\\ndivergence and bregman divergence classes,” IEEE Transactions on\\nInformation Theory, vol. 55, no. 11, pp. 4925–4931, 2009.\\n[44] C. Stein et al., “A bound for the error in the normal approximation to the\\ndistribution of a sum of dependent random variables,” in Proceedings\\nof the Sixth Berkeley Symposium on Mathematical Statistics and Prob-\\nability, Volume 2: Probability Theory.\\nThe Regents of the University\\nof California, 1972.\\n[45] T. Minka et al., “Divergence measures and message passing,” Technical\\nreport, Microsoft Research, Tech. Rep., 2005.\\n[46] S. Kullback and R. A. Leibler, “On information and sufﬁciency,” The\\nannals of mathematical statistics, vol. 22, no. 1, pp. 79–86, 1951.\\n[47] T. M. Cover, Elements of information theory.\\nJohn Wiley & Sons,\\n1999.\\n12\\n[48] R. Hadsell, S. Chopra, and Y. LeCun, “Dimensionality reduction by\\nlearning an invariant mapping,” in 2006 IEEE Computer Society Con-\\nference on Computer Vision and Pattern Recognition (CVPR’06), vol. 2.\\nIEEE, 2006, pp. 1735–1742.\\n[49] Z. Wu, Y. Xiong, S. X. Yu, and D. Lin, “Unsupervised feature learning\\nvia non-parametric instance discrimination,” in Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition, 2018, pp.\\n3733–3742.\\n[50] D. W. Shattuck, M. Mirza, V. Adisetiyo, C. Hojatkashani, G. Salamon,\\nK. L. Narr, R. A. Poldrack, R. M. Bilder, and A. W. Toga, “Construction\\nof a 3d probabilistic atlas of human cortical structures,” Neuroimage,\\nvol. 39, no. 3, pp. 1064–1080, 2008.\\n[51] A. Klein and J. Tourville, “101 labeled brain images and a consistent\\nhuman cortical labeling protocol,” Frontiers in neuroscience, vol. 6, p.\\n171, 2012.\\n[52] L. R. Dice, “Measures of the amount of ecologic association between\\nspecies,” Ecology, vol. 26, no. 3, pp. 297–302, 1945.\\n[53] D. P. Huttenlocher, G. A. Klanderman, and W. J. Rucklidge, “Comparing\\nimages using the hausdorff distance,” IEEE Transactions on pattern\\nanalysis and machine intelligence, vol. 15, no. 9, pp. 850–863, 1993.\\n[54] V. Yeghiazaryan and I. D. Voiculescu, “Family of boundary overlap\\nmetrics for the evaluation of medical image segmentation,” Journal of\\nMedical Imaging, vol. 5, no. 1, p. 015006, 2018.\\n[55] L. Liu, X. Hu, L. Zhu, C.-W. Fu, J. Qin, and P.-A. Heng, “ψ-net:\\nStacking densely convolutional lstms for sub-cortical brain structure\\nsegmentation,” IEEE transactions on medical imaging, vol. 39, no. 9,\\npp. 2806–2817, 2020.\\n[56] F.-X. Vialard, L. Risser, D. Rueckert, and C. J. Cotter, “Diffeomorphic\\n3D image registration via geodesic shooting using an efﬁcient adjoint\\ncalculation,” International Journal of Computer Vision, vol. 97, no. 2,\\npp. 229–241, 2012.\\n[57] B. B. Avants, N. Tustison, G. Song et al., “Advanced normalization tools\\n(ants),” Insight j, vol. 2, no. 365, pp. 1–35, 2009.\\n[58] D. Kuang and T. Schmah, “Faim–a convnet method for unsupervised\\n3d medical image registration,” in International Workshop on Machine\\nLearning in Medical Imaging.\\nSpringer, 2019, pp. 646–654.\\n[59] L. Liu, X. Hu, L. Zhu, and P.-A. Heng, “Probabilistic multilayer\\nregularization network for unsupervised 3d brain image registration,” in\\nInternational Conference on Medical image computing and computer-\\nassisted intervention (MICCAI).\\nSpringer, 2019, pp. 346–354.\\n[60] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., “Pytorch: An\\nimperative style, high-performance deep learning library,” in Advances\\nin neural information processing systems, 2019, pp. 8026–8037.\\n[61] K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectiﬁers:\\nSurpassing human-level performance on imagenet classiﬁcation,” in\\nProceedings of the IEEE international conference on computer vision,\\n2015, pp. 1026–1034.\\n[62] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\\nwith deep convolutional neural networks,” Communications of the ACM,\\nvol. 60, no. 6, pp. 84–90, 2017.\\n[63] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\\narXiv preprint arXiv:1412.6980, 2014.\\n[64] O. Bernard, A. Lalande, C. Zotti, F. Cervenansky, X. Yang, P.-A.\\nHeng, I. Cetin, K. Lekadir, O. Camara, M. A. G. Ballester et al.,\\n“Deep learning techniques for automatic mri cardiac multi-structures\\nsegmentation and diagnosis: is the problem solved?” IEEE transactions\\non medical imaging, vol. 37, no. 11, pp. 2514–2525, 2018.\\n'), ResearchPaper(title='Unsupervised Contrastive Learning with Simple Transformation for 3D Point Cloud Data', authors=[arxiv.Result.Author('Jincen Jiang'), arxiv.Result.Author('Xuequan Lu'), arxiv.Result.Author('Wanli Ouyang'), arxiv.Result.Author('Meili Wang')], abstract='Though a number of point cloud learning methods have been proposed to handle\\nunordered points, most of them are supervised and require labels for training.\\nBy contrast, unsupervised learning of point cloud data has received much less\\nattention to date. In this paper, we propose a simple yet effective approach\\nfor unsupervised point cloud learning. In particular, we identify a very useful\\ntransformation which generates a good contrastive version of an original point\\ncloud. They make up a pair. After going through a shared encoder and a shared\\nhead network, the consistency between the output representations are maximized\\nwith introducing two variants of contrastive losses to respectively facilitate\\ndownstream classification and segmentation. To demonstrate the efficacy of our\\nmethod, we conduct experiments on three downstream tasks which are 3D object\\nclassification (on ModelNet40 and ModelNet10), shape part segmentation (on\\nShapeNet Part dataset) as well as scene segmentation (on S3DIS). Comprehensive\\nresults show that our unsupervised contrastive representation learning enables\\nimpressive outcomes in object classification and semantic segmentation. It\\ngenerally outperforms current unsupervised methods, and even achieves\\ncomparable performance to supervised methods. Our source codes will be made\\npublicly available.', url='http://arxiv.org/abs/2110.06632v2', pdf_path='./papers/2110.06632v2.Unsupervised_Contrastive_Learning_with_Simple_Transformation_for_3D_Point_Cloud_Data.pdf', content='The Visual Comptuer 2022\\nUnsupervised Contrastive Learning with Simple\\nTransformation for 3D Point Cloud Data\\nJincen Jiang1, Xuequan Lu2, Wanli Ouyang3 and Meili Wang1*\\n1College of Information Engineering, Northwest A&F University, China.\\n2School of Information Technology, Deakin University, Australia.\\n3School of Electrical and Information Engineering, The University of Sydney, Australia.\\n*Corresponding author(s). E-mail(s): wml@nwsuaf.edu.cn;\\nContributing authors: jinec@nwsuaf.edu.cn; xuequan.lu@deakin.edu.au;\\nwanli.ouyang@sydney.edu.au;\\nAbstract\\nThough a number of point cloud learning methods have been proposed to handle unordered points, most of them\\nare supervised and require labels for training. By contrast, unsupervised learning of point cloud data has received\\nmuch less attention to date. In this paper, we propose a simple yet effective approach for unsupervised point cloud\\nlearning. In particular, we identify a very useful transformation which generates a good contrastive version of an\\noriginal point cloud. They make up a pair. After going through a shared encoder and a shared head network, the\\nconsistency between the output representations are maximized with introducing two variants of contrastive losses\\nto respectively facilitate downstream classiﬁcation and segmentation. To demonstrate the efﬁcacy of our method,\\nwe conduct experiments on three downstream tasks which are 3D object classiﬁcation (on ModelNet40 and Mod-\\nelNet10), shape part segmentation (on ShapeNet Part dataset) as well as scene segmentation (on S3DIS). Com-\\nprehensive results show that our unsupervised contrastive representation learning enables impressive outcomes\\nin object classiﬁcation and semantic segmentation. It generally outperforms current unsupervised methods, and\\neven achieves comparable performance to supervised methods. Our source codes will be made publicly available.\\nKeywords: unsupervised contrastive learning; point cloud; 3D object classiﬁcation; semantic segmentation\\n1 Introduction\\nPoint cloud, as an effective representation for 3D geo-\\nmetric data, has attracted noticeable attention recently.\\nIt has been used for learning based segmentation, clas-\\nsiﬁcation, object detection, etc. Promising results have\\nbeen achieved among those application ﬁelds. In this\\nwork, we focus on the use of 3D point clouds for clas-\\nsiﬁcation and segmentation tasks. They respectively\\ntarget to automatically recognize 3D objects and pre-\\ndict segment labels, which are crucial in multimedia\\ncomputing, robotics, etc.\\nMost of existing methods for 3D point cloud anal-\\nysis [1–9] use annotated data for training. Neverthe-\\nless, annotation is time-consuming and costly, espe-\\ncially for a considerable amount of data. In the real\\nworld, it is particularly challenging to have annotated\\ndata for training all the time. Unsupervised learning is\\na good alternative [10–13]. For example, Latent-GAN\\n[10] used a deep architecture of Autoencoder (AE),\\nand trained a minimal GAN in the AE’s latent space\\nfor learning representations of point clouds. Fold-\\ningNet [11] proposed a new AE to get the codeword\\n1\\narXiv:2110.06632v2  [cs.CV]  24 Jan 2023\\nThe Visual Comptuer 2022\\nwhich can represent the high dimensional embed-\\nding point cloud, and the fully-connected decoder\\nwas replaced with the folding-based decoder. MAP-\\nVAE [12] conducted half-to-half predictions (splitting\\npoint cloud into a front half and a back half with\\nseveral angles), and then combined them with global\\nself-supervision to capture the geometry and struc-\\nture of the point cloud. 3D-PointCapsNet [13] used\\nthe encoder-decoder structure, and concatenated the\\nfeatures from the encoder to form the point capsules.\\nThese methods usually employ the AE as the back-\\nbone, and often suffer from the curse of less quality\\nrepresentations. As a result, they may still induce\\nless desired performance on downstream tasks (e.g.\\nclassiﬁcation and segmentation).\\nMotivated by the above analysis, we propose an\\nunsupervised representation learning method which\\ncan facilitate the downstream 3D object classiﬁca-\\ntion and semantic segmentation. Our core idea is to\\nmaximize the agreement or consistency between the\\nrepresentations of the original point cloud and its\\ntransformed version (i.e. contrastive version).\\nWe simply utilize one transformation to generate\\nthe transformed version point cloud and pair it with\\nthe original ones. And then feed them into a shared\\nbase encoder network (e.g. former part of PointNet [7]\\nwith global feature), followed by a subsequent projec-\\ntion head network (e.g. latter part of PointNet: several\\nmlp layers). The agreement maximization is imposed\\non the outputs of the projection head network, to facil-\\nitate the training efﬁciency and better preserve the rich\\nrepresentations output from the encoder. Since there\\nare no labels involved in training, it is unsupervised\\nrepresentation learning for 3D point cloud data.\\nTo validate our unsupervised method, we conduct\\nexperiments for the object classiﬁcation task on Mod-\\nelNet40 and ModelNet10, the shape part segmentation\\ntask on ShapeNet Part dataset, and the scene segmen-\\ntation task on the S3DIS dataset. Extensive results\\nshow that our unsupervised contrastive representa-\\ntive learning enables impressive outcomes in terms\\nof the three tasks. Our method generally outperforms\\nstate-of-the-art unsupervised techniques, and is even\\ncomparable to certain supervised counterparts.\\nThe contributions of this paper are:\\n• an unsupervised representation learning approach\\nwhich is simple yet effective on 3D point cloud\\ndata,\\n• a simple transformation in generating a good con-\\ntrastive version of an original point cloud, which is\\nbetter than other complex transformations,\\n• two variants of point cloud based contrastive losses\\nfor downstream classiﬁcation and segmentation,\\nrespectively,\\n• experiments and analysis on three tasks (classiﬁca-\\ntion, shape part segmentation and scene segmen-\\ntation), achieving promising performance with our\\nsimple contrastive learning.\\n2 Related Work\\nUnlike 2D images, which consist of regular and uni-\\nform pixels, point cloud data are often irregular,\\nsparse and contaminated with noise/outliers during\\nthe obtaining procedure of scanning and processing\\n[14–17]. 3D point cloud learning techniques can be\\ngenerally classiﬁed into three categories: (1) voxel\\nbased [1–3], (2) view based [4–6, 18, 19] and (3)\\npoint based [7–9, 20–27]. Voxel based methods often\\ninvolve resolution and memory issues, and view based\\napproaches are often criticized for the tedious pre-\\nprocessing, i.e. projecting each 3D object onto 2D\\nimage planes. Point based techniques are capable of\\nlearning features from point cloud data straightfor-\\nwardly. In fact, most of these methods are supervised.\\nVoxel based techniques. 3D volumetric CNNs\\n(Convolutional Neural Network) imitates classical 2D\\nCNNs by performing voxelization on the input point\\ncloud. 3D ShapeNets was designed for learning volu-\\nmetric shapes [1]. Riegler et al. proposed OctNet for\\ndeep learning with sparse 3D data [2]. Wang et al. pre-\\nsented an Octree-based CNN for 3D shape analysis,\\nwhich was called O-CNN [3]. These methods are pro-\\nposed to improve 3D volumetric CNNs and reach high\\nvolume resolutions.\\nView based methods. View based methods are to\\nproject 3D point cloud data onto the regular image\\nplanes. For example, MVCNNs used multiple images\\nrendered from the 3D shapes to ﬁt classical 2D CNNs\\n[4]. Su et al. proposed to utilize a sparse set of sam-\\nples in a high-dimensional lattice as the representation\\nof a collection of points [18]. Zhou et al. proposed\\nthe multi-view saliency guided deep neural network\\n(MVSG-DNN) which contains three modules to cap-\\nture and extract the features of individual views to\\ncompile 3D object descriptors for 3D object retrieval\\nand classiﬁcation [19]. Xu et al. used a LSTM-based\\nnetwork to recurrently aggregate the 3D objects shape\\nThe Visual Comptuer 2022\\n3\\nembedding from an image sequence and estimate\\nimages of unseen viewpoints, aiming at the fusion of\\nmultiple views’ features [28]. Huang et al. devised a\\nview mixture model (VMM) to decompose the mul-\\ntiple views into a few latent views for the descriptor\\nconstruction [29]. Li et al. presented an end-to-end\\nframework to learn local multi-view descriptors for\\n3D point clouds [5]. Lyu et al. projected 3D point\\nclouds into 2D image space by learning the topology-\\npreserving graph-to-grid mapping [6].\\nPoint based methods. PointNet is a seminal work\\non point based learning [7]. In PointNet, max-pooling\\noperation is used to learn permutation-invariant fea-\\ntures. The original authors introduced PointNet++,\\na hierarchical neural network that applied PointNet\\nrecursively on a nested partitioning of the input point\\nset [8]. It achieved better learning outcomes than\\nPointNet. Later, pointCNN was introduced to learn an\\nX-transformation from the input points, to promote\\nthe weighting of the input features and the permuta-\\ntion of the points into a latent order [9]. PointConv,\\na density re-weighted convolution, was proposed to\\nfully approximate the 3D continuous convolution on\\nany set of 3D points [20]. Xu et al. proposed Spider-\\nCNN to extract geometric features from point clouds\\n[21]. Liu et al. designed a Relation-Shape Convolu-\\ntional Neural Network to learn the geometric topology\\nconstraints among points [22]. Simonovsky et al. gen-\\neralized the convolution operator from regular grids\\nto arbitrary graphs and applied it to point cloud clas-\\nsiﬁcation [30]. Parametric Continuous Convolution\\nwas introduced to exploit parameterized kernel func-\\ntions that spanned the full continuous vector space\\n[31]. Li et al. came up with a self-organizing net-\\nwork which applied hierarchical feature aggregation\\nusing self-organizing map [32]. It included a point\\ncloud auto-encoder as pre-training to improve network\\nperformance. Komarichev et al. presented an annu-\\nlar convolution operator to better capture the local\\nneighborhood geometry of each point by specify-\\ning the (regular and dilated) ring-shaped structures\\nand directions in the computation [23]. Zhao et al.\\nput forwarded PointWeb to enhance local neighbor-\\nhood features for point cloud processing [33]. Xie et\\nal. developed a new representation by adopting the\\nconcept of shape context as the building block and\\ndesigned a model (ShapeContextNet) for point cloud\\nrecognition [34]. Wang et al. designed a new neu-\\nral network module dubbed EdgeConv which acts\\non graphs dynamically computed in each layer [24].\\nMore recently, Fujiwara et al. proposed to embed\\nthe distance ﬁeld to neural networks [35]. Lin et al.\\ndeﬁned learnable kernels with a graph max-pooling\\nmechanism for their 3D Graph Convolution Networks\\n(3D-GCN) [25]. Yan et al. presented the adaptive sam-\\npling and the local-nonlocal modules for robust point\\ncloud processing [36]. Qiu et al. proposed a network\\nconsidering both low-level geometric information of\\n3D space points explicitly and high-level local geo-\\nmetric context of feature space implicitly [37]. Chen\\net al. presented a hierarchical attentive pooling graph\\nnetwork (HAPGN) for segmentation which includes\\nthe gated graph attention network to get a better rep-\\nresentation of local features and hierarchical graph\\npooling module to learn hierarchical features [38].\\nLiu et al. devised a point context encoding module\\n(PointCE) and a semantic context encoding loss (SCE-\\nloss) to capture the rich semantic context of a point\\ncloud adaptively, achieving improved segmentation\\nperformance [39].\\nUnsupervised representation learning. Yang et\\nal. proposed an autoencoder (AE), referred to as Fold-\\ningNet, for unsupervised learning on point cloud data\\n[11]. MAP-VAE was proposed to enable the learn-\\ning of global and local geometry by jointly leverag-\\ning global and local self-supervision [12]. Rao et al.\\npresented bidirectional reasoning between the local\\nstructures and the global shape for unsupervised rep-\\nresentation learning of point clouds [40]. It used a\\nmuch larger RSCNN as backbone (4×RSCNN) [22].\\nZhang et al. presented an explainable machine learn-\\ning method for point cloud classiﬁcation by build-\\ning local-to-global features through iterative one-hop\\ninformation exchange, and feeding the feature vec-\\ntor to a random forest classiﬁer for classiﬁcation [41].\\nDifferent from them, we create a contrastive pair\\nfor each point cloud, and our framework simply con-\\nsists of an encoder network and a head network. The\\nencoder outputs global representations (features) for\\ndownstream networks and the head outputs projection\\nfeatures (a smaller size) for calculating the loss.\\nMore recently, Xie et al. presented an unsuper-\\nvised pre-training framework called PointContrast\\nfor high-level scene understanding tasks [42]. Their\\nﬁndings demonstrated that the learned representation\\ncould generalize across domains. [42] focused on 3D\\nscenes (pretrained on a very large-scale generated\\ndataset (about 1 terabyte), and sophisticatedly con-\\nsidered matched points (i.e. common points) of two\\ndifferent views (at least 30% overlap) as pairs. Unlike\\nthat, our point cloud level based approach simply\\nThe Visual Comptuer 2022\\ncontrastive\\ntransformation\\ninput point cloud\\ncontrastive pair\\nbase\\nencoder\\nnetwork\\nm\\nm\\nglobal feature\\ncontrastive\\nvector\\noriginal point cloud\\ntransformed\\npoint cloud\\nMLP\\nprojection\\nhead\\nprojection\\nfeature\\ncontrastive\\nvector\\ncontrastive\\nloss\\n(+)\\n(-)\\n(-)\\nfeature space\\nk\\nk\\nFig. 1 Overview of our unsupervised contrastive representation learning method. Given a point cloud, the transformation (rotation in this\\nwork) is used to the get transformed version of the original point cloud, which deﬁnes a pair. Then, the pairs are input to the base encoder\\nnetwork (e.g. PointNet or DGCNN) to learn the global feature of each model. The projection head is further used to reduce the global feature\\ndimension and for effective loss degradation. The contrastive loss encourages a pair of point clouds to be consistent in the feature space.\\nuses a rotational transformation to generate a trans-\\nformed version of an original point cloud. It can easily\\nget a great pose discrepancy, without requiring point\\ncloud overlap to satisfy the demand of obtaining a\\ncertain number of matched points. In essence, a pair\\nof matched points are treated as a pair in [42] to\\nlearn point-level features, while a pair of point clouds\\n(a point cloud consisting of a series of points) are\\nregarded as a pair in our work. Treating the point\\nclouds as the pair in our method has the advantage\\nof learning better global representations when com-\\npared with [42]. It is also intuitive and straightforward\\nto use point cloud level, while PointContrast [42]\\ncan hardly obtain point cloud representations directly\\nand is suitable for point-wise tasks, e.g., scene seg-\\nmentation. In comparison, our global feature of point\\ncloud level can be easily used in both point cloud\\nlevel and point-wise tasks (e.g., classiﬁcation and seg-\\nmentation). Meanwhile, for unsupervised learning we\\nderive two variants of contrastive losses based on\\npoint clouds, which respectively facilitate two dif-\\nferent types of downstream tasks (i.e., classiﬁcation\\nand segmentation). Finally, the backbone networks are\\ndifferent: they use a Sparse Residual U-Net which\\nrequires voxelization of point clouds, while we use a\\nsimple encoder-head structure.\\nDifference from SimCLR [43]: It is derived on top\\nof SimCLR [43], but is largely different from Sim-\\nCLR: (1) SimCLR is designed for 2D images, and our\\ncontrastive learning is for 3D point cloud data, which\\ninvolves irregular point distribution, and poses more\\nchallenges than 2D images with regular grids. (2)\\nSimCLR uses multiple transformations for an original\\nimage, and these two different versions of images act\\nas a pair. By contrast, we only generate a transformed\\nversion of the original point cloud with a simple trans-\\nformation (e.g. rotation), thus forming a contrastive\\npair of this point cloud (i.e. a pair in point cloud\\nlevel). (3) SimCLR typically uses TPU resources with\\na large batch size for training, while we demonstrate\\nan elegant approach for 3D point cloud representa-\\ntion learning, which is simple yet effective, without\\nrequiring TPU computing resources.\\n3 Method\\nIn this work, we take 3D object classiﬁcation and\\nsemantic segmentation (shape and scene) as the down-\\nstream tasks of our unsupervised contrastive repre-\\nsentation learning. To clearly elaborate our method,\\nwe take downstream object classiﬁcation as an exam-\\nple when designing the unsupervised stage, and we\\nwill later explain how to extend it to shape and scene\\nsegmentation.\\nGiven an unlabeled point cloud, we ﬁrst use a\\ntransformation (i.e. rotation) to generate its trans-\\nformed version, thus constructing a contrastive point\\ncloud pair for this original point cloud. They are fed\\ninto a base encoder network, in order to learn a pair of\\nglobal features or representations. The global features\\nare then passed to a projection head network, to obtain\\nanother pair of representations. These two representa-\\ntions from the projection head network target to reach\\na maximum agreement with the aid of a contrastive\\nloss function. Figure 1 shows the framework of our\\nunsupervised contrastive representation learning.\\nThe Visual Comptuer 2022\\n5\\n3.1 Unsupervised Contrastive\\nRepresentation Learning\\nContrastive transformation. Unlike 2D images,\\npoint cloud data often have an irregular distribution\\nin 3D space, and have a complex degree of freedom.\\nGiven this, it is more difﬁcult to identify the practi-\\ncally useful transformations for constructing a good\\ncontrastive pair of a point cloud. SimCLR [43] uti-\\nlized different types of transformations for a single\\nimage (e.g. cropping, rotation), and generated two\\ntransformed versions. In contrast, we only use one\\ntransformation for simplicity, and pair the original\\npoint cloud with the transformed version, that is, a pair\\nincluding the original point cloud and its transformed\\ncounterpart. Common transformations in 3D space are\\nrotation, cutout, crop, scaling, smoothing, noise cor-\\nruption, etc. Since heavy noise corruption will destroy\\nthe object shapes, we exclude this for transformations\\napplied here. Jittering is analogous to light noise, and\\nwe use jittering for data augmentation, following the\\nprotocol of the state-of-the-art point based methods.\\nIn this work, we select rotation as the transformation,\\nand use it to generate a transformed version for the\\noriginal point cloud. We provide the discussion of the\\nchoice in ablation studies (Section 4.6).\\nBase encoder network. Point based networks,\\nsuch as PointNet [7], DGCNN [24], Pointﬁlter [14],\\noften involve a pooling layer to output the global fea-\\nture for an input point cloud. The former part of a point\\nbased network before this layer (inclusive) can be nat-\\nurally viewed as a based encoder in our framework. In\\nother words, the input point cloud can be encoded into\\na latent representation vector (i.e. the global feature).\\nIn this sense, we can simply extract this former part of\\nany such point based networks as a base encoder net-\\nwork in our unsupervised contrastive representation\\nframework. In this work, we select some state-of-\\nthe-art point based networks including PointNet and\\nDGCNN as the backbone, and extract their former\\nparts as our base encoder accordingly. It is interest-\\ning to discover that the encoders involving T-Net (i.e.\\ntransformation net) will hinder the learning of unsu-\\npervised contrastive representations. We deduce that\\nT-Net accounts for various rotated point cloud aug-\\nmentations, which degrades the ability of capturing\\na large contrast between the input pair. As such, we\\nremove the original T-Net (i.e. transformation net) in\\nthese encoders, if involved. We show the results of\\ndifferent encoders in Section 4.\\nProjection head network. Point based networks\\nusually have several fully connected layers to bridge\\nthe global feature with the ﬁnal k-class vector. Simi-\\nlar to the encoder, we can also simply extract the latter\\npart of a point based network as the projection head.\\nAlternatively, it is also ﬂexible to customize a projec-\\ntion head network by designing more or fewer fully\\nconnected layers.\\nMathematically, the ﬁnal k-class vector (or repre-\\nsentation vector) can be formulated as\\nzi = H(E(P)),\\nzj = H(E(P′)),\\n(1)\\nwhere P is an original point cloud and P′ is its\\ntransformed counterpart. E and H denote the encoder\\nnetwork and the projection head network, respectively.\\nContrastive loss function. We ﬁrst randomly\\nselect n samples, and use the selected transforma-\\ntion (i.e. rotation) to generate another n corresponding\\ntransformed counterparts, resulting in n pairs (2n\\nsamples) constituting the minibatch. Analogous to\\nSimCLR [43], we also do not explicitly deﬁne positive\\nor negative pairs. Instead, we select a pair as the posi-\\ntive pair, and the remaining (n−1) pairs (i.e. 2(n−1)\\nsamples) are simply regarded as negative pairs.\\nAs for the unsupervised loss function, InfoNCE\\n[44] is a widely-used loss function for unsupervised\\nrepresentation learning of 2D images. More recently,\\n[42] also utilized a similar loss for contrastive scene\\nrepresentation learning. Inspired by them, we also\\nintroduce a variant as our unsupervised loss function,\\nwhich is deﬁned as\\nL = −1\\nS\\nX\\n(i,j)∈S\\nlog\\nexp(zi · zj/τ)\\nP\\n(·,t)∈S,t̸=j exp(zi · zt/τ),\\n(2)\\nwhere S is the set of all positive pairs (point cloud\\nlevel), and τ is a temperature parameter. denotes the\\ncardinality of the set. The loss is computed using all\\nthe contrastive pairs, and is equivalent to applying\\nthe cross entropy with pseudo labels (e.g. 0 ∼15\\nfor 16 pairs). We found it works very well in our\\nunsupervised contrastive representation learning.\\n3.2 Downstream 3D Object Classiﬁcation\\nWe take 3D object classiﬁcation as the ﬁrst down-\\nstream task in this work, to validate our unsupervised\\nrepresentation learning. The above designed scheme\\nThe Visual Comptuer 2022\\nis immediately ready for the unsupervised represen-\\ntation learning to facilitate the downstream classiﬁca-\\ntion task. In particular, we will utilize two common\\nschemes for validation here. One is to train a linear\\nclassiﬁcation network by taking the learned represen-\\ntations of our unsupervised learning as input. Here, the\\nlearned representation is the global feature. We did not\\nchoose the k-class representation vector as it had less\\ndiscriminative features than the global feature in our\\nframework, and it induced a poor performance (see\\nSection 4.6). The other validation scheme is to initial-\\nize the backbone with the unsupervised trained model\\nand perform a supervised training. We will demon-\\nstrate the classiﬁcation results for these two validation\\nschemes in Section 4.\\n3.3 Downstream Semantic Segmentation\\nTo further demonstrate the effectiveness of our unsu-\\npervised representation learning, we also ﬁt the above\\nunsupervised learning scheme to the downstream\\nsemantic segmentation, including shape part segmen-\\ntation and scene segmentation. Since it is a different\\ntask from 3D object classiﬁcation, we need to design\\na new scheme to facilitate unsupervised training. We\\nstill use the rotation to generate a transformed ver-\\nsion of an original point cloud (e.g. a shape point\\ncloud or a split block from the scene), and view them\\nas a contrastive pair (i.e. point cloud level). As for\\nsegmentation, each point in the point cloud has a fea-\\nture representation. For unsupervised representation\\nlearning, we compute the mean of all point-wise cross\\nentropy to evaluate the overall similarity within the\\nmini-batch. We therefore deﬁne a loss function for\\nsemantic segmentation as:\\nL = −1\\nS\\nX\\n(a,b)∈S\\n1\\nP(a,b)\\nX\\n(i,j)∈P(a,b)\\nlog\\nexp(zi · zj/τ)\\nP\\n(·,t)∈P(a,b),t̸=j exp(zi · zt/τ),\\n(3)\\nwhere S is the set of all positive pairs (i.e. point\\ncloud a and b), and P(a,b) is the set of all point pairs\\n(i.e. the same point id) of the point cloud a and b. Sim-\\nilarly, we apply the cross entropy with pseudo labels\\nwhich match the point indices (e.g. 0 ∼2047 for 2048\\npoints).\\n4 Experimental Results\\n4.1 Datasets\\nObject classiﬁcation. We utilize ModelNet40 and\\nModelNet10 [1] for 3D object classiﬁcation. We fol-\\nlow the same data split protocols of PointNet-based\\nmethods [7, 8, 24] for these two datasets. For Mod-\\nelNet40, the train set has 9, 840 models and the test\\nset has 2, 468 models, and the datset consists of 40\\ncategories. For ModelNet10, 3, 991 models are for\\ntraining and 908 models for testing. It contains 10 cat-\\negories. For each model, we use 1, 024 points with\\nonly (x, y, z) coordinates as the input, which is also\\nconsistent with previous works.\\nNote that some methods [10, 11, 13, 45] are pre-\\ntrained under the ShapeNet55 dataset [47]. We also\\nconduct a version of ShapeNet55 training for the clas-\\nsiﬁcation task. We used the same dataset as [13],\\nwhich has 57, 448 models with 55 categories, and all\\nmodels will be used for unsupervised training. Follow-\\ning the same setting of previous work, we use 2, 048\\npoints as input.\\nWe provide comparison experiments with Point-\\nContrast [42] for the classiﬁcation task, and they use\\nthe ShapeNetCore [47] for ﬁnetuning. The dataset\\ncontains 51, 127 pre-aligned shapes from 55 cate-\\ngories, which has 35, 708 models for training, 5, 158\\nmodels for validation and 10, 261 models for testing.\\nWe use 1, 024 points as input which is the same as\\nPointContrast [42].\\nShape part segmentation. We use the ShapeNet\\nPart dataset [48] for shape part segmentation, which\\nconsists of 16, 881 shapes from 16 categories. Each\\nobject involves 2 to 6 parts, with a total number of\\n50 distinct part labels. We follow the ofﬁcial dataset\\nsplit and the same point cloud sampling protocol as\\n[47]. Only the point coordinates are used as input. Fol-\\nlowing [8, 24], we use mean Intersection-over-Union\\n(mIoU) as the evaluation metric.\\nScene segmentation. We also evaluate our model\\nfor scene segmentation on Stanford Large-Scale 3D\\nIndoor Spaces Dataset (S3DIS) [49]. This dataset con-\\ntains 3D scans of 271 rooms and 6 indoor areas,\\ncovering over 6, 000m2. We follow the same setting as\\n[8, 24]. Each room is split with 1m×1m area into little\\nblocks, and we sampled 4, 096 points of each block.\\nEach point is represented as a 9D vector, which means\\nthe point coordinates, RGB color and normalized loca-\\ntion for the room. Each point is annotated with one of\\nthe 13 semantic categories. We also follow the same\\nThe Visual Comptuer 2022\\n7\\nTable 1\\nClassiﬁcation results of unsupervised methods and our method (Linear Classiﬁer), on the datasets of ModelNet40 and ModelNet10.\\nBoth ShapeNet55 (upper part) and ModelNet40 (bottom part) pretrained datasets are provided.\\nMethods\\nPretrained Dataset\\nInput Data\\nResolution\\ne.g. # Points\\nModelNet40\\nAccuracy\\nModelNet10\\nAccuracy\\nLatent-GAN [10]\\nShapeNet55\\nxyz\\n2k\\n85.70\\n95.30\\nFoldingNet [11]\\nShapeNet55\\nxyz\\n2k\\n88.40\\n94.40\\nMRTNet [45]\\nShapeNet55\\nxyz\\nmulti-resolution\\n86.40\\n-\\n3D-PointCapsNet [13]\\nShapeNet55\\nxyz\\n2k\\n88.90\\n-\\nOurs (DGCNN)\\nShapeNet55\\nxyz\\n2k\\n89.37\\n-\\nVIPGAN [46]\\nModelNet40\\nviews\\n12\\n91.98\\n94.05\\nLatent-GAN [10]\\nModelNet40\\nxyz\\n2k\\n87.27\\n92.18\\nFoldingNet [11]\\nModelNet40\\nxyz\\n2k\\n84.36\\n91.85\\n3D-PointCapsNet [13]\\nModelNet40\\nxyz\\n1k\\n87.46\\n-\\nPointHop [41]\\nModelNet40\\nxyz\\n1k\\n89.10\\n-\\nMAP-VAE [12]\\nModelNet40\\nxyz\\n2k\\n90.15\\n94.82\\nGLR (RSCNN-Large) [40]\\nModelNet40\\nxyz\\n1k\\n92.9\\n-\\nOurs (PointNet [7])\\nModelNet40\\nxyz\\n1k\\n88.65\\n90.64\\nOurs (DGCNN [24])\\nModelNet40\\nxyz\\n1k\\n90.32\\n95.09\\nprotocol of adopting the six-fold cross validation for\\nthe six area.\\nPlease refer to the Appendices for additional\\ninformation and visual results.\\n4.2 Experimental Setting\\nWe use Adam optimizer for our unsupervised rep-\\nresentation training. We implemented our work with\\nTensorFlow, and use a single TITAN V GPU for\\ntraining (DGCNN using multiple GPUs).\\nFor downstream 3D object classiﬁcation on Mod-\\nelNet40, ModelNet10, ShapeNet55 and ShapeNet-\\nCore, we use a batch size of 32 (i.e. 16 contrastive\\npairs) for training and testing. Temperature hyper-\\nparameter τ is set as 1.0. We use the same dropouts\\nwith the original methods accordingly, i.e. 0.7 for\\nPointNet as backbone, 0.5 for DGCNN as backbone.\\nThe initial decay rate of batch normalization is 0.5,\\nand will be increased no lager than 0.99. The training\\nstarts with a 0.001 learning rate, and is decreased to\\n0.00001 with an exponential decay.\\nWe employ DGCNN as the backbone for seman-\\ntic segmentation. As for shape part segmentation on\\nShapeNet Part dataset, we utilize a batch size of 16\\n(i.e. 8 constrasive pairs) for training. We use a batch\\nsize of 12 (i.e. 6 constrasive pairs) for scene segmen-\\ntation on S3DIS. For the two tasks, we simply use a\\nbatch size of 1 during testing, and the other settings\\nfollow DGCNN.\\n4.3 3D Object Classiﬁcation\\nWe conduct two kinds of experiments to evaluate\\nthe learned representations of our unsupervised con-\\ntrastive learning. We ﬁrst train a simple linear classiﬁ-\\ncation network with the unsupervised representations\\nas input. Secondly, we take our unsupervised rep-\\nresentation learning as pretraining, and initialize the\\nweights of the backbone before supervised training.\\nTables 1 and 3 show 3D object classiﬁcation results\\nfor our method and a wide range of state-of-the-art\\ntechniques.\\nLinear classiﬁcation evaluation. In this part, we\\nuse the former part of PointNet [7] and DGCNN [24]\\nas the base encoder, and use the latter mlp layers\\nas the projection head. The learned features are used\\nas the input for training the linear classiﬁcation net-\\nwork. We use the test accuracy as the evaluation of\\nour unsupervised contrastive learning. Comparisons\\nare reported in Table 1. Regarding linear classiﬁcation\\nevaluation, our method with DGCNN as backbone\\nalways performs better than our method using Point-\\nNet as backbone, for example, 95.09% versus 90.64%\\nfor ModelNet10, 90.32% versus 88.65% for Model-\\nNet40. This is due to a more complex point based\\nstructure of DGCNN. Our method with DGCNN as\\nbackbone also outperforms most unsupervised tech-\\nniques, like two recent methods PointHop (1.22%\\ngain) and MAP-VAE (0.17% gain), and is comparable\\nto some supervised methods in Table 3, for exam-\\nple, 90.32% versus 90.6% (O-CNN) and 90.9% (SO-\\nNet with xyz) on ModelNet40, 95.09% versus 93.9%\\nThe Visual Comptuer 2022\\n(supervised 3D-GCN) on ModelNet10. The GLR [40]\\nmined rich semantic and structural information, and\\nused a larger RSCNN as backbone (4×RSCNN) [22],\\nresulting in a better accuracy than our method.\\nNotice\\nthat\\nsome\\nmethods\\nused\\na\\nlarger\\nShapeNet55 dataset for training [10, 11, 13, 45].\\nAlthough the previous work [12] re-implemented\\nthem by training on ModelNet40, they use 2048 points\\nrather than our 1024. To provide additional insights,\\nwe re-implement and train a state-of-the-art method\\n(3D-PointCapsNet [13]) on ModelNet40 with 1024\\npoints, and train a linear classiﬁer for evaluation. We\\nchoose this method since its code is publicly available\\nand it is recent work. From Table 1, it is obvious that\\nour method (DGCNN as backbone) still outperforms\\n3D-PointCapsNet by a 2.86% margin.\\nTo show our learned representations have the\\ntransfer capability, we also train our method (DGCNN\\nas backbone) on ShapeNet55 dataset for unsuper-\\nvised contrastive learning and then feed the Mod-\\nelNet40 dataset to the trained model to get point\\ncloud features. We use these features to train a lin-\\near classiﬁer on ModelNet40 for evaluation. From\\nTable 1 we can see that our method achieves the best\\nresult compared with other state-of-art methods on the\\nsame settings (Latent-GAN [10], FoldingNet [11], and\\n3D-PointCapsNet [13]), exceeding them by 3.67%,\\n0.97%, and 0.47%, respectively.\\nWe also conduct an experiment for training with\\nlimited data to verify the capability of our pretrained\\nmodel with linear classiﬁer. The results can be seen\\nin Table 2. Our pretrained model achieves 86.6% with\\n30% data, which is 2.4% higher compared with Fold-\\ningNet’s [11] 84.2%. With less data, e.g. 10% of the\\ndata, our result is 0.6% lower than FoldingNet. We\\nsuspect that with far fewer data, our contrastive learn-\\ning method could less effectively learn the features\\nof each sample with such simple transformation, and\\nresult in less improvement.\\nTable 2\\nComparison results of classiﬁcation accuracy with\\nlimited training data (different ratios).\\nMethods\\n10%\\n20%\\n30%\\nFoldingNet [11])\\n81.2\\n83.6\\n84.2\\nOurs (DGCNN as backbone)\\n80.6\\n85.4\\n86.6\\nPretraining evaluation. In addition to the above\\nevaluation using a linear classiﬁer, we further utilize\\nthe pre-training evaluation to demonstrate the efﬁ-\\ncacy of our unsupervised contrastive representation\\nlearning. Speciﬁcally, we also select PointNet and\\nDGCNN as the backbone, in which the part before and\\nincluding the global feature is regarded as the base\\nencoder, and the remaining classiﬁcation branch (i.e.\\nseveral mlp layers) as the projection head. After our\\nunsupervised representation training, we initialize the\\ncorresponding network with the unsupervised trained\\nmodel, and then perform the supervised training. Table\\n3 shows the comparison results of our method and\\nthe state-of-the-art 3D object classiﬁcation techniques\\nwith supervised training.\\nThe pretraining evaluation based on our unsuper-\\nvised representation learning sees an improvement\\nover the original backbone network, increased from\\n89.2% to 90.44% (1.24% increase) with PointNet\\nand from 92.2% to 93.03% (0.83% increase) with\\nDGCNN on ModelNet40. Regarding ModelNet10, the\\naccuracy of PointNet as our backbone for pretraining\\nevaluation is 94.38%, which is on par with the super-\\nvised 3D-GCN (93.9%). It is interesting to see that\\nour method (DGCNN as backbone) is the best one on\\nModelNet10 in the pretraining evaluation, while the\\nsecond best is achieved by two very recent supervised\\nmethods (Neural Implicit [35] and PointASNL [36]).\\n[35] even used a large weight matrix as the input for\\nclassiﬁcation training. For ModelNet40, our method\\n(DGCNN as backbone) achieves 93.03%, outperform-\\ning almost all techniques including both supervised\\nand unsupervised ones. For example, our method in\\nthis case outperforms the very recent supervised meth-\\nods including 3D-GCN [25], Neural Implicit [35] and\\nPointASNL [36].\\nCompared to using PointNet as backbone, taking\\nDGCNN as backbone achieves a better classiﬁca-\\ntion accuracy, for example, 95.93% versus 94.38%,\\n93.03% versus 90.44%. Similarly, we believe this is\\nmainly because DGCNN exploits richer information\\nthan PointNet.\\nPointContrast [42] also presented an unsupervised\\ncontrastive learning approach, which is based on point\\nlevel while ours is based on point cloud level. They\\nvalidated their effectiveness on some datasets using\\nthe pretrain-ﬁnetuning strategy. In order to provide\\na potential comparison with it, we also used the\\nShapeNetCore dataset for the classiﬁcation task with\\npretraining evaluation. The comparison results are\\nshown in Table 4, and we can see that our method\\n(DGCNN as backbone) outperforms them by 0.5%,\\nthough PointContrast is pretrained on a rather larger\\ndataset (ScanNet). Note that our method is not suitable\\nto be pretrained on ScanNet since this downstream\\nThe Visual Comptuer 2022\\n9\\nTable 3\\nClassiﬁcation results of other supervised methods and our method (Pretraining), on the datasets of ModelNet40 and ModelNet10.\\nWe distinguish the results of other methods from our method by the line.\\nMethods\\nInput Data\\nResolution\\ne.g. # Points\\nModelNet40\\nAccuracy\\nModelNet10\\nAccuracy\\nKd-Net (depth=10) [50]\\ntree\\n210 × 3\\n90.6\\n93.3\\nPointNet++ [8]\\nxyz\\n1k\\n90.7\\n-\\nKCNet [51]\\nxyz\\n1k\\n91.0\\n94.4\\nMRTNet [45]\\nxyz\\n1k\\n91.2\\n-\\nSO-Net [32]\\nxyz\\n2k\\n90.9\\n94.1\\nKPConv [52]\\nxyz\\n6.8k\\n92.9\\n-\\nPointNet++ [8]\\nxyz, normal\\n5k\\n91.9\\n-\\nSO-Net [32]\\nxyz, normal\\n5k\\n93.4\\n-\\nO-CNN [3]\\nxyz, normal\\n-\\n90.6\\n-\\nPointCNN [9]\\nxyz\\n1k\\n92.2\\n-\\nPCNN [53]\\nxyz\\n1k\\n92.3\\n94.9\\nPoint2Sequence [54]\\nxyz\\n1k\\n92.6\\n95.3\\nRS-CNN (voting) [22]\\nxyz\\n1k\\n93.6\\n-\\nNeural Implicit [35]\\nweights\\n1024 × 256\\n92.2\\n95.7\\nPointASNL [36]\\nxyz\\n1k\\n92.9\\n95.7\\n3D-GCN [25]\\nxyz\\n1k\\n92.1\\n93.9\\nHAPGN [38]\\nxyz\\n1k\\n91.7\\n-\\nMVSG-DNN [19]\\nviews\\n12\\n92.3\\n94.0\\nScratch (PointNet [7])\\nxyz\\n1k\\n89.2\\n-\\nOurs (PointNet [7])\\nxyz\\n1k\\n90.44 (+1.24)\\n94.38\\nScratch (DGCNN [24])\\nxyz\\n1k\\n92.2\\n-\\nOurs (DGCNN [24])\\nxyz\\n1k\\n93.03 (+0.83)\\n95.93\\ntask is for classiﬁcation (requiring point cloud level\\nfeatures for classiﬁcation) while ScanNet has point-\\nwise labels. The good performance of our method\\nis mainly due to the proper design of point cloud\\nlevel based contrastive pairs and contrastive learn-\\ning, so that we can directly obtain the global fea-\\nture from contrastive representation learning. We also\\nre-implement DGCNN [24] on the ShapeNetCore\\ndataset, which further demonstrates the effectiveness\\nof our method by increasing from 84.0% (original\\nDGCNN) to 86.2%. In comparison with PointContrast\\nwhich improved 0.6% from the version of training\\nfrom scratch, we achieve 2.2% increase.\\nTable 4\\nComparison results of PointContrast and our method on\\nthe dataset of ShapeNetCore with Pretraining evaluation. Note that\\n* represents that the model is trained on ScanNet.\\nMethods\\nAccuracy\\nTrained from scratch (PointContrast [42])\\n85.1\\nPointContrast* [42]\\n85.7\\nTrained from scratch (original DGCNN [24])\\n84.0\\nOurs (DGCNN as backbone)\\n86.2\\n4.4 Shape Part Segmentation\\nIn addition to the 3D object classiﬁcation, we also\\nverify our method on shape part segmentation. The\\nsegmentation results are listed in Table 5. Here we take\\nDGCNN as the backbone of our approach and simply\\nemploy the linear classiﬁer evaluation setting. It can\\nbe seen from the table that our method in linear clas-\\nsiﬁcation evaluation achieves 79.2% instance mIOU\\nand 75.5% class mIOU, which are remarkably better\\nthan state-of-the-art unsupervised techniques includ-\\ning MAP-VAE [12] and Multi-task [55]. Speciﬁcally,\\nour method outperforms MAP-VAE [12] and Multi-\\ntask [55] by a margin of 7.55% and 3.4%, respectively,\\nin terms of class mIOU. Figure 2 illustrates some\\nexamples of our method (Linear Classiﬁer setting) on\\nthe task of shape part segmentation.\\n4.5 Scene Segmentation\\nWe also test our method for the scene segmentation\\ntask on the S3DIS dataset, which typically appears\\nto be more challenging than the shape part segmen-\\ntation. Similarly, we utilize DGCNN as the backbone\\nThe Visual Comptuer 2022\\nTable 5 Shape part segmentation results of our method (Linear Classiﬁer) and state-of-the-art techniques on ShapeNet Part dataset. We\\ndistinguish between supervised and unsupervised learning methods by the line.\\nMethods\\nSupervised\\nclass\\nmIOU\\ninstance\\nmIOU\\nair.\\nbag\\ncap\\ncar\\nchair\\near.\\nguit.\\nkni.\\nlam.\\nlap.\\nmot.\\nmug\\npist.\\nrock.\\nska.\\ntab.\\nKd-Net [50]\\nyes\\n77.4\\n82.3\\n80.1\\n74.6\\n74.3\\n70.3\\n88.6\\n73.5\\n90.2\\n87.2\\n81.0\\n84.9\\n87.4\\n86.7\\n78.1\\n51.8\\n69.9\\n80.3\\nMRTNet [45]\\nyes\\n79.3\\n83.0\\n81.0\\n76.7\\n87.0\\n73.8\\n89.1\\n67.6\\n90.6\\n85.4\\n80.6\\n95.1\\n64.4\\n91.8\\n79.7\\n87.0\\n69.1\\n80.6\\nPointNet [7]\\nyes\\n80.4\\n83.7\\n83.4\\n78.7\\n82.5\\n74.9\\n89.6\\n73.0\\n91.5\\n85.9\\n80.8\\n95.3\\n65.2\\n93.0\\n81.2\\n57.9\\n72.8\\n80.6\\nKCNet [51]\\nyes\\n82.2\\n84.7\\n82.8\\n81.5\\n86.4\\n77.6\\n90.3\\n76.8\\n91.0\\n87.2\\n84.5\\n95.5\\n69.2\\n94.4\\n81.6\\n60.1\\n75.2\\n81.3\\nRS-Net [56]\\nyes\\n81.4\\n84.9\\n82.7\\n86.4\\n84.1\\n78.2\\n90.4\\n69.3\\n91.4\\n87.0\\n83.5\\n95.4\\n66.0\\n92.6\\n81.8\\n56.1\\n75.8\\n82.2\\nSO-Net [32]\\nyes\\n81.0\\n84.9\\n82.8\\n77.8\\n88.0\\n77.3\\n90.6\\n73.5\\n90.7\\n83.9\\n82.8\\n94.8\\n69.1\\n94.2\\n80.9\\n53.1\\n72.9\\n83.0\\nPointNet++ [8]\\nyes\\n81.9\\n85.1\\n82.4\\n79.0\\n87.7\\n77.3\\n90.8\\n71.8\\n91.0\\n85.9\\n83.7\\n95.3\\n71.6\\n94.1\\n81.3\\n58.7\\n76.4\\n82.6\\nDGCNN [24]\\nyes\\n82.3\\n85.2\\n84.0\\n83.4\\n86.7\\n77.8\\n90.6\\n74.7\\n91.2\\n87.5\\n82.8\\n95.7\\n66.3\\n94.9\\n81.1\\n63.5\\n74.5\\n82.6\\nKPConv [52]\\nyes\\n85.1\\n86.4\\n84.6\\n86.3\\n87.2\\n81.1\\n91.1\\n77.8\\n92.6\\n88.4\\n82.7\\n96.2\\n78.1\\n95.8\\n85.4\\n69.0\\n82.0\\n83.6\\nNeural Implicit [35]\\nyes\\n-\\n85.2\\n84.0\\n80.4\\n88.0\\n80.2\\n90.7\\n77.5\\n91.2\\n86.4\\n82.6\\n95.5\\n70.0\\n93.9\\n84.1\\n55.6\\n75.6\\n82.1\\n3D-GCN [25]\\nyes\\n82.1\\n85.1\\n83.1\\n84.0\\n86.6\\n77.5\\n90.3\\n74.1\\n90.9\\n86.4\\n83.8\\n95.6\\n66.8\\n94.8\\n81.3\\n59.6\\n75.7\\n82.8\\nHAPGN [38]\\nyes\\n87.1\\n89.3\\n87.1\\n85.7\\n90.1\\n86.2\\n91.7\\n78.3\\n94.3\\n85.9\\n82.6\\n95.2\\n77.9\\n94.3\\n90.1\\n73.9\\n90.3\\n90.6\\nMAP-VAE [12]\\nno\\n67.95\\n-\\n62.7\\n67.1\\n73.0\\n58.5\\n77.1\\n67.3\\n84.8\\n77.1\\n60.9\\n90.8\\n35.8\\n87.7\\n64.2\\n45.0\\n60.4\\n74.8\\nMulti-task [55]\\nno\\n72.1\\n77.7\\n78.4\\n67.7\\n78.2\\n66.2\\n85.5\\n52.6\\n87.7\\n81.6\\n76.3\\n93.7\\n56.1\\n80.1\\n70.9\\n44.7\\n60.7\\n73.0\\nOurs (DGCNN)\\nno\\n75.5\\n79.2\\n76.3\\n76.6\\n82.5\\n65.8\\n85.9\\n67.1\\n86.6\\n81.3\\n79.2\\n93.8\\n55.8\\n92.8\\n73.5\\n53.1\\n61.3\\n76.6\\nairplane\\nbag\\ncap\\ncar\\nchair\\nguitar\\nknife\\nlamp\\nmotorbike\\nmug\\nskateboard\\ntable\\nFig. 2\\nSome examples of shape part segmentation using our method (Linear Classiﬁer setting).\\nand adopt the Linear Classiﬁer evaluation setting. We\\nare not able to compare our method with unsupervised\\nmethods like MAP-VAE [12] and Multi-task [55],\\nsince they did not provide scene segmentation results\\nand their source codes are not publicly available. Table\\n6 lists the comparisons of 1 fold testing on Area 5. It is\\nobserved that our method even outperforms the super-\\nvised PointNet in terms of mean accuracy. Due to the\\nunsupervised property, our method is inferior to the\\nsupervised PointCNN and ﬁne-tuned PointContrast.\\nOur method has relatively smaller mean IOU, which\\nis probably due to the imbalanced categories and the\\nlimited minibatch size. The performance could be fur-\\nther improved if more powerful computing resources\\nare allowed. Figure 3 shows a visual example for scene\\nsegmentation.\\nTable 6 Scene segmentation results of our method (Linear\\nClassiﬁer) and some state-of-the-art techniques on testing Area 5\\n(Fold 1) of the S3DIS dataset.\\nMethods\\nSupervised\\nMean\\naccuracy\\nMean\\nIOU\\nPointNet [7]\\nyes\\n49.0\\n41.1\\nPointCE [39]\\nyes\\n-\\n51.7\\nPointCNN [9]\\nyes\\n63.9\\n57.3\\nPointContrast [42]\\nyes\\n76.9\\n70.3\\nOurs (DGCNN)\\nno\\n59.4\\n32.6\\n4.6 Ablation Studies\\nTransformation. One of the key elements in our\\nunsupervised representation learning is using 180◦\\nrotation around the Y axis to get the transformation.\\nTo comprehensively study the inﬂuence of transfor-\\nmation on representations, we consider many common\\ntransformations including rotation, cutout, crop, scale,\\njittering and smoothing. Figure 4 visualizes different\\ntransformations for a point cloud.\\nThe Visual Comptuer 2022\\n11\\nGround truth\\nOur result\\nFig. 3 Visual result of scene segmentation.\\nWe list the comparison results of the above trans-\\nformations in Table 7. It can be clearly observed that\\nour choice attains the best accuracy, which is unlike\\nSimCLR [43] that utilizes two different transforma-\\ntions of an image as the pair. We suspect that rotation\\nis a very simple and effective transformation for 3D\\npoint cloud data, and a larger valid rotation would gen-\\nerate a greater pose discrepancy (i.e., contrast) in 3D\\nspace. As such, our choice using 180◦rotation around\\nthe Y axis is better than others.\\nFurthermore, we apply two sequential transforma-\\ntions on one point cloud and make the result as a\\npair with the original point cloud. We chose the best\\ntransformation (i.e. rotate 180◦around Y axis) as the\\nﬁrst transformation, and then apply one of the rest of\\nthe transformations as the second. For more complex\\ntransformations, we group all the transformations into\\nfour categories, including rotation, scaling, jittering/s-\\nmoothing, and cropping/cutout, and apply these four\\ncategories on one point cloud sequentially. We show\\nthe results in Table 8. It can be seen that after applying\\nthe complex transformation, it is still not as good as\\nthe best choice shown above. We suspect that the com-\\nplex transformation may damage the information on\\nthe point cloud, thus leading to inferior results. Again,\\nthis veriﬁes that our choice is the best transformation\\nfor 3D point cloud data in generating contrastive pairs.\\nOutput of encoder versus output of projection\\nhead. We also compare the choices of using the output\\nof the base encoder (i.e. global feature) and the output\\nof the projection head for subsequent linear classiﬁ-\\ncation. Table 9 shows the comparison results of the\\ntwo choices on ModelNet40 and ModelNet10. We see\\nthat the former choice is better than the latter choice.\\nWe think the output of the base encoder involves more\\ndiscriminative features for the training of the linear\\nclassiﬁer.\\nCross validation. In addition to the above evalua-\\ntions, we further test the abilities of our unsupervised\\ncontrastive representation learning in a crossed eval-\\nuation setting. To achieve this, we use the learned\\nOriginal\\nRotate 180◦\\n(Y axis)\\nRotate 90◦\\n(Y axis)\\nRotate 45◦\\n(Y axis)\\nRotate 180◦\\n(X axis)\\nRotate 90◦\\n(X axis)\\nRotate 45◦\\n(X axis)\\nCutout\\nCrop\\nScale\\nJitter\\nSmooth\\nFig. 4 Illustration for transformations used in Table 7.\\nTable 7 Comparison of different contrastive transformation on\\nModelNet10. DGCNN [24] is the backbone. We use linear\\nclassiﬁcation evaluation for comparisons. % is used for\\nclassiﬁcation accuracy.\\nTransformation\\nMean class\\naccuracy\\nOverall\\naccuracy\\nrotate 180◦(Y axis)\\n94.88\\n95.09\\nrotate 90◦(Y axis)\\n94.12\\n94.53\\nrotate 45◦(Y axis)\\n94.09\\n94.20\\nrotate 180◦(X axis)\\n93.21\\n93.53\\nrotate 90◦(X axis)\\n93.30\\n93.42\\nrotate 45◦(X axis)\\n93.71\\n93.97\\ncutout\\n94.01\\n93.97\\ncrop\\n93.80\\n94.31\\nscale\\n94.10\\n94.20\\njitter\\n93.95\\n93.97\\nsmooth\\n93.93\\n94.08\\nrepresentations from the unsupervised trained model\\non ModelNet40 to further train a linear classiﬁer on\\nModelNet10, and vice versa. Classiﬁcation outcomes\\nare reported in Table 10. It can be observed that our\\nunsupervised representation learning is indeed work-\\ning in the cross-dataset setting. It also reveals that our\\nunsupervised method trained on a large dataset would\\nprobably beneﬁt the testing on another dataset greatly.\\nIn here, our method trained on ModelNet40 enables a\\nbetter cross-test accuracy, compared to unsupervised\\ntraining on ModelNet10 and testing on ModelNet40.\\nPretraining evaluation: initializing projection\\nhead. Projection head is very useful in maximizing the\\nagreement between the contrastive pair. However, it\\nThe Visual Comptuer 2022\\nTable 8 Comparison of more complex contrastive transformation\\non ModelNet10. We distinguish between the results of using only\\ntwo transformations and those of using four transformations (more\\ncomplex) by the line. “Rotate” means 180◦rotation around the Y\\naxis. DGCNN [24] is the backbone. We use linear classiﬁcation\\nevaluation for comparisons. % is used for classiﬁcation accuracy.\\nTransformation\\nMean class\\naccuracy\\nOverall\\naccuracy\\nrotate + cutout\\n93.43\\n93.64\\nrotate + crop\\n93.90\\n93.97\\nrotate + scale\\n94.11\\n94.08\\nrotate + jitter\\n94.33\\n94.42\\nrotate + smooth\\n93.41\\n93.64\\nrotate + scale + jitter + cutout\\n94.07\\n94.42\\nrotate + scale + jitter + crop\\n93.92\\n94.31\\nrotate + scale + smooth + cutout\\n93.56\\n93.86\\nrotate + scale + smooth + crop\\n93.78\\n94.20\\nrotate 180◦(Y axis)\\n94.88\\n95.09\\nTable 9 Comparison of using the output of encoder and projection\\nhead for linear classiﬁcation evaluation. PointNet [7] is the\\nbackbone. % is used for classiﬁcation accuracy.\\nComponent\\nDataset\\nMean class\\naccuracy\\nOverall\\naccuracy\\nencoder\\nModelNet40\\n83.81\\n88.65\\nhead\\nModelNet40\\n68.55\\n75.81\\nencoder\\nModelNet10\\n90.55\\n90.64\\nhead\\nModelNet10\\n81.57\\n82.59\\nTable 10 Cross validation for ModelNet40 and ModelNet10. We\\nperform unsupervised learning on one dataset and conduct the\\nclassiﬁcation task on another dataset. PointNet [7] is the backbone.\\n% is used for classiﬁcation accuracy.\\nUnsupervised\\ndataset\\nClassiﬁcation\\ndataset\\nMean class\\naccuracy\\nOverall\\naccuracy\\nModelNet40\\nModelNet10\\n90.00\\n90.51\\nModelNet10\\nModelNet40\\n77.13\\n82.87\\nmay hinder pretraining evaluation, if the correspond-\\ning part is initialized with the projection head of the\\nunsupervised model. Table 11 shows that initializing\\nencoder only produces better classiﬁcation accuracy\\nfor PointNet/DGCNN on ModelNet10/ModelNet40,\\nwhich conﬁrms the judgement that initializing encoder\\nonly is a better choice.\\nT-SNE Visualization. We utilize the t-SNE to\\nvisualize the features learned on ModelNet40 in\\nFigure 5. It can be seen that using only a linear clas-\\nsiﬁer can separate different features to some extent.\\nIt is worth noting that our model using unsupervised\\ncontrastive learning can better separate features after\\ntraining a linear classiﬁer, which implies that our con-\\ntrastive learning is useful and effectively facilitates the\\nclassiﬁcation task.\\nTable 11 Pretraining validation to determine whether using\\npreojection head for initialization. ModelNet40 and ModelNet10\\nare used for datasets. PointNet [7] and DGCNN[24] are the\\nbackbone. % is used for classiﬁcation accuracy.\\nBackbone\\nDataset\\nHead\\ninitialization\\nMean class\\naccuracy\\nOverall\\naccuracy\\nPointNet\\nModelNet10\\nyes\\n93.80\\n93.97\\nPointNet\\nModelNet10\\nno\\n94.23\\n94.38\\nPointNet\\nModelNet40\\nyes\\n86.64\\n90.22\\nPointNet\\nModelNet40\\nno\\n86.80\\n90.44\\nDGCNN\\nModelNet10\\nyes\\n95.05\\n95.09\\nDGCNN\\nModelNet10\\nno\\n95.78\\n95.93\\nDGCNN\\nModelNet40\\nyes\\n88.58\\n91.96\\nDGCNN\\nModelNet40\\nno\\n89.52\\n93.03\\nFig. 5 T-SNE visualization of features. (a) without contrastive\\nlearning, (b) with contrastive learning.\\n5 Conclusion\\nWe have presented an unsupervised representation\\nlearning method for 3D point cloud data. We identi-\\nﬁed that rotation is a very useful transformation for\\ngenerating a contrastive version of an original point\\ncloud. Unsupervised representations are learned via\\nmaximizing the correspondence between paired point\\nclouds (i.e. an original point cloud and its contrastive\\nversion). Our method is simple to implement and does\\nnot require expensive computing resources like TPU.\\nWe evaluate our unsupervised representations for\\nthe downstream tasks including 3D object classiﬁca-\\ntion, shape part segmentation and scene segmentation.\\nExperimental results demonstrate that our method\\ngenerates impressive performance. In the future, We\\nwould like to exploit semi-supervised techniques like\\n[57] to improve the performance. We would also like\\nto extend our approach to other interesting applica-\\ntions such as 3D object detection.\\nConﬂicts of Interests\\nThe authors declare that the work is original and has\\nnot been submitted elsewhere.\\nThe Visual Comptuer 2022\\n13\\ncontrastive\\ntransformation\\ninput point cloud\\ncontrastive pair\\nbase\\nencoder\\nnetwork\\nglobal feature\\ncontrastive\\nvector\\noriginal point cloud\\ntransformed\\npoint cloud\\nMLP\\nprojection\\nhead\\nprojection feature\\ncontrastive\\nvector\\ncontrastive\\nloss\\n(+)\\n(-)\\nfeature space\\nn x m\\n(+)\\n(-)\\n(-)\\npoint cloud\\nfeature\\npoint feature\\nn x m\\nn x k\\nn x k\\nFig. A1\\nOverview of our unsupervised contrastive learning for the downstream segmentation task. All the point clouds in the minibatch will\\nbe mapped into the feature space. The designed contrastive loss (shown in the main paper) encourages a pair of point clouds (original point\\ncloud and its transformed point cloud) to be consistent in the feature space, and the point-wise features of the same point ID also tend to be\\nconsistent.\\nAppendix A\\nOverview of\\nSegmentation\\nWe also achieve the task of point cloud semantic\\nsegmentation, including shape part segmentation and\\nscene segmentation. Different from the 3D object clas-\\nsiﬁcation task, we need to gain all the point-wise fea-\\ntures in the point cloud, which is the key to solve the\\nsegmentation task. For our unsupervised contrastive\\nlearning, as shown in Figure A1, we still consider the\\noriginal point cloud and its transformed point cloud as\\na contrastive pair. However, in order to ensure that the\\nfeature of each point in the point cloud will be learned,\\nwe use the mean of point-wise cross entropy to evalu-\\nate the point cloud similarity, and try to maximize the\\nsimilarity of the positive pair (all other pairs of point\\nclouds in the minibatch are viewed as negative pairs).\\nIn this unsupervised manner, our framework can learn\\nthe feature of each point in the point cloud.\\nAppendix B\\nAdditional Visual\\nResults on Scene\\nSegmentation\\nIn this section, we show more visual results on scene\\nsegmentation. Similarly, we utilize the Linear Clas-\\nsiﬁer setting for this downstream task. Figure B2\\nshows the visual results of several scenes. We can\\nobserve from the ﬁgure that our method produces\\nclose segmentation results to the ground truth. This\\ndemonstrates the capability of our unsupervised rep-\\nresentation learning method.\\nGround truth\\nOur result\\nFig. B2 Visual result of scene segmentation.\\nAppendix C\\nAdditional Visual\\nResults on Shape\\nPart Segmentation\\nIn this section, we put more visual results of our\\nmethod on the downstream shape part segmentation.\\nWe simply employ the Linear Classiﬁer setting for this\\ndownstream task. Figure C3 shows the visual results\\nof 32 models of 16 categories, involving 2 models\\nper category. As we can see from the ﬁgure, with\\nour unsupervised learned representations, a simple\\nlinear classiﬁer for the downstream task can gener-\\nate very similar visual results to the ground truth\\nsegmentation. It further conﬁrms the effectiveness of\\nour unsupervised method in learning distinguishable\\nrepresentations.\\nThe Visual Comptuer 2022\\nGround truth\\nOur result\\nGround truth\\nOur result\\nGround truth\\nOur result\\nGround truth\\nOur result\\nairplane\\nbag\\ncap\\ncar\\nchair\\nearphone\\nguitar\\nknife\\nlamp\\nlaptop\\nmotorbike\\nmug\\npistol\\nrocket\\nskateboard\\ntable\\nFig. C3 Some examples of all 16 categories in ShapeNet Part dataset.\\nThe Visual Comptuer 2022\\n15\\nReferences\\n[1] Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L.,\\nTang, X., Xiao, J.: 3d shapenets: A deep repre-\\nsentation for volumetric shapes. In: Proceedings\\nof the IEEE Conference on Computer Vision and\\nPattern Recognition, pp. 1912–1920 (2015)\\n[2] Riegler, G., Osman Ulusoy, A., Geiger, A.: Oct-\\nnet: Learning deep 3d representations at high\\nresolutions. In: Proceedings of the IEEE Confer-\\nence on Computer Vision and Pattern Recogni-\\ntion, pp. 3577–3586 (2017)\\n[3] Wang, P.-S., Liu, Y., Guo, Y.-X., Sun, C.-Y.,\\nTong, X.: O-cnn: Octree-based convolutional\\nneural networks for 3d shape analysis. ACM\\nTransactions on Graphics (TOG) 36(4), 1–11\\n(2017)\\n[4] Su, H., Maji, S., Kalogerakis, E., Learned-Miller,\\nE.: Multi-view convolutional neural networks\\nfor 3d shape recognition. In: Proceedings of\\nthe IEEE International Conference on Computer\\nVision, pp. 945–953 (2015)\\n[5] Li, L., Zhu, S., Fu, H., Tan, P., Tai, C.-L.: End-to-\\nend learning local multi-view descriptors for 3d\\npoint clouds. In: Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern\\nRecognition, pp. 1919–1928 (2020)\\n[6] Lyu, Y., Huang, X., Zhang, Z.: Learning to seg-\\nment 3d point clouds in 2d image space. In:\\nProceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition, pp.\\n12255–12264 (2020)\\n[7] Qi, C.R., Su, H., Mo, K., Guibas, L.J.: Pointnet:\\nDeep learning on point sets for 3d classiﬁca-\\ntion and segmentation. In: Proceedings of the\\nIEEE Conference on Computer Vision and Pat-\\ntern Recognition, pp. 652–660 (2017)\\n[8] Qi, C.R., Yi, L., Su, H., Guibas, L.J.: Point-\\nnet++: Deep hierarchical feature learning on\\npoint sets in a metric space. In: Advances in Neu-\\nral Information Processing Systems, pp. 5099–\\n5108 (2017)\\n[9] Li, Y., Bu, R., Sun, M., Wu, W., Di, X., Chen, B.:\\nPointcnn: Convolution on x-transformed points.\\nIn: Advances in Neural Information Processing\\nSystems, pp. 820–830 (2018)\\n[10] Achlioptas, P., Diamanti, O., Mitliagkas, I.,\\nGuibas, L.: Learning representations and genera-\\ntive models for 3d point clouds. In: International\\nConference on Machine Learning, pp. 40–49\\n(2018). PMLR\\n[11] Yang, Y., Feng, C., Shen, Y., Tian, D.: Fold-\\ningnet: Point cloud auto-encoder via deep grid\\ndeformation. In: Proceedings of the IEEE Con-\\nference on Computer Vision and Pattern Recog-\\nnition, pp. 206–215 (2018)\\n[12] Han, Z., Wang, X., Liu, Y.-S., Zwicker, M.:\\nMulti-angle point cloud-vae: unsupervised fea-\\nture learning for 3d point clouds from multiple\\nangles by joint self-reconstruction and half-to-\\nhalf prediction. In: 2019 IEEE/CVF Interna-\\ntional Conference on Computer Vision (ICCV),\\npp. 10441–10450 (2019). IEEE\\n[13] Zhao, Y., Birdal, T., Deng, H., Tombari, F.: 3d\\npoint capsule networks. In: Proceedings of the\\nIEEE Conference on Computer Vision and Pat-\\ntern Recognition, pp. 1009–1018 (2019)\\n[14] Zhang, D., Lu, X., Qin, H., He, Y.: Pointﬁlter:\\nPoint cloud ﬁltering via encoder-decoder mod-\\neling. IEEE Transactions on Visualization and\\nComputer Graphics, 1–1 (2020). https://doi.org/\\n10.1109/TVCG.2020.3027069\\n[15] Lu, D., Lu, X., Sun, Y., Wang, J.: Deep\\nfeature-preserving\\nnormal\\nestimation\\nfor\\npoint\\ncloud\\nﬁltering.\\nComputer-\\nAided\\nDesign\\n125,\\n102860\\n(2020).\\nhttps://doi.org/10.1016/j.cad.2020.102860\\n[16] Lu, X., Schaefer, S., Luo, J., Ma, L., He, Y.: Low\\nrank matrix approximation for 3d geometry ﬁl-\\ntering. IEEE Transactions on Visualization and\\nComputer Graphics, 1–1 (2020). https://doi.org/\\n10.1109/TVCG.2020.3026785\\n[17] Lu, X., Wu, S., Chen, H., Yeung, S., Chen,\\nW., Zwicker, M.: Gpf: Gmm-inspired feature-\\npreserving point set ﬁltering. IEEE Transac-\\ntions on Visualization and Computer Graph-\\nics 24(8), 2315–2326 (2018). https://doi.org/10.\\n1109/TVCG.2017.2725948\\nThe Visual Comptuer 2022\\n[18] Su, H., Jampani, V., Sun, D., Maji, S., Kaloger-\\nakis, E., Yang, M.-H., Kautz, J.: Splatnet: Sparse\\nlattice networks for point cloud processing. In:\\nProceedings of the IEEE Conference on Com-\\nputer Vision and Pattern Recognition, pp. 2530–\\n2539 (2018)\\n[19] Zhou, H.-Y., Liu, A.-A., Nie, W.-Z., Nie, J.:\\nMulti-view saliency guided deep neural network\\nfor 3-d object retrieval and classiﬁcation. IEEE\\nTransactions on Multimedia 22(6), 1496–1506\\n(2019)\\n[20] Wu, W., Qi, Z., Fuxin, L.: Pointconv: Deep con-\\nvolutional networks on 3d point clouds. In: Pro-\\nceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition, pp. 9621–9630\\n(2019)\\n[21] Xu, Y., Fan, T., Xu, M., Zeng, L., Qiao, Y.: Spi-\\ndercnn: Deep learning on point sets with param-\\neterized convolutional ﬁlters. In: Proceedings of\\nthe European Conference on Computer Vision\\n(ECCV), pp. 87–102 (2018)\\n[22] Liu, Y., Fan, B., Xiang, S., Pan, C.: Relation-\\nshape convolutional neural network for point\\ncloud analysis. In: Proceedings of the IEEE Con-\\nference on Computer Vision and Pattern Recog-\\nnition, pp. 8895–8904 (2019)\\n[23] Komarichev, A., Zhong, Z., Hua, J.: A-cnn:\\nAnnularly convolutional neural networks on\\npoint clouds. In: Proceedings of the IEEE Con-\\nference on Computer Vision and Pattern Recog-\\nnition, pp. 7421–7430 (2019)\\n[24] Wang, Y., Sun, Y., Liu, Z., Sarma, S.E., Bron-\\nstein, M.M., Solomon, J.M.: Dynamic graph cnn\\nfor learning on point clouds. Acm Transactions\\nOn Graphics (tog) 38(5), 1–12 (2019)\\n[25] Lin, Z.-H., Huang, S.-Y., Wang, Y.-C.F.: Convo-\\nlution in the cloud: Learning deformable kernels\\nin 3d graph convolution networks for point cloud\\nanalysis. In: Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recog-\\nnition, pp. 1800–1809 (2020)\\n[26] Jiang, L., Shi, S., Tian, Z., Lai, X., Liu, S., Fu,\\nC.-W., Jia, J.: Guided point contrastive learn-\\ning for semi-supervised point cloud semantic\\nsegmentation. In: Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision,\\npp. 6423–6432 (2021)\\n[27] Du, B., Gao, X., Hu, W., Li, X.: Self-contrastive\\nlearning with hard negative sampling for self-\\nsupervised point cloud learning. In: Proceedings\\nof the 29th ACM International Conference on\\nMultimedia, pp. 3133–3142 (2021)\\n[28] Xu,\\nC.,\\nLeng,\\nB.,\\nChen,\\nB.,\\nZhang,\\nC.,\\nZhou, X.: Learning discriminative and gener-\\native shape embeddings for three-dimensional\\nshape retrieval. IEEE Transactions on Multime-\\ndia 22(9), 2234–2245 (2019)\\n[29] Huang, J., Yan, W., Li, T.H., Liu, S., Li, G.:\\nLearning the global descriptor for 3d object\\nrecognition based on multiple views decomposi-\\ntion. IEEE Transactions on Multimedia (2020)\\n[30] Simonovsky, M., Komodakis, N.: Dynamic\\nedge-conditioned ﬁlters in convolutional neural\\nnetworks on graphs. In: Proceedings of the IEEE\\nConference on Computer Vision and Pattern\\nRecognition, pp. 3693–3702 (2017)\\n[31] Wang, S., Suo, S., Ma, W.-C., Pokrovsky, A.,\\nUrtasun, R.: Deep parametric continuous con-\\nvolutional neural networks. In: Proceedings of\\nthe IEEE Conference on Computer Vision and\\nPattern Recognition, pp. 2589–2597 (2018)\\n[32] Li, J., Chen, B.M., Hee Lee, G.: So-net: Self-\\norganizing network for point cloud analysis. In:\\nProceedings of the IEEE Conference on Com-\\nputer Vision and Pattern Recognition, pp. 9397–\\n9406 (2018)\\n[33] Zhao, H., Jiang, L., Fu, C.-W., Jia, J.: Pointweb:\\nEnhancing local neighborhood features for point\\ncloud processing. In: Proceedings of the IEEE\\nConference on Computer Vision and Pattern\\nRecognition, pp. 5565–5573 (2019)\\n[34] Xie, S., Liu, S., Chen, Z., Tu, Z.: Attentional\\nshapecontextnet for point cloud recognition. In:\\nProceedings of the IEEE Conference on Com-\\nputer Vision and Pattern Recognition, pp. 4606–\\n4615 (2018)\\n[35] Fujiwara, K., Hashimoto, T.: Neural implicit\\nThe Visual Comptuer 2022\\n17\\nembedding for point cloud analysis. In: Proceed-\\nings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition, pp. 11734–\\n11743 (2020)\\n[36] Yan, X., Zheng, C., Li, Z., Wang, S., Cui, S.:\\nPointasnl: Robust point clouds processing using\\nnonlocal neural networks with adaptive sam-\\npling. In: Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recogni-\\ntion, pp. 5589–5598 (2020)\\n[37] Qiu, S., Anwar, S., Barnes, N.: Geometric back-\\nprojection network for point cloud classiﬁcation.\\nIEEE Transactions on Multimedia (2021)\\n[38] Chen, C., Qian, S., Fang, Q., Xu, C.: Hapgn:\\nHierarchical attentive pooling graph network for\\npoint cloud segmentation. IEEE Transactions on\\nMultimedia (2020)\\n[39] Liu, H., Guo, Y., Ma, Y., Lei, Y., Wen, G.:\\nSemantic context encoding for accurate 3d point\\ncloud segmentation. IEEE Transactions on Mul-\\ntimedia (2020)\\n[40] Rao, Y., Lu, J., Zhou, J.: Global-local bidirec-\\ntional reasoning for unsupervised representation\\nlearning of 3d point clouds. In: Proceedings of\\nthe IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition, pp. 5376–5385 (2020)\\n[41] Zhang, M., You, H., Kadam, P., Liu, S., Kuo, C.-\\nC.J.: Pointhop: An explainable machine learn-\\ning method for point cloud classiﬁcation. IEEE\\nTransactions on Multimedia 22(7), 1744–1755\\n(2020)\\n[42] Xie, S., Gu, J., Guo, D., Qi, C.R., Guibas,\\nL., Litany, O.: Pointcontrast: Unsupervised pre-\\ntraining for 3d point cloud understanding. In:\\nEuropean Conference on Computer Vision, pp.\\n574–591 (2020). Springer\\n[43] Chen, T., Kornblith, S., Norouzi, M., Hinton,\\nG.: A simple framework for contrastive learning\\nof visual representations. In: International Con-\\nference on Machine Learning, pp. 1597–1607\\n(2020). PMLR\\n[44] Oord, A.v.d., Li, Y., Vinyals, O.: Representa-\\ntion learning with contrastive predictive coding.\\narXiv preprint arXiv:1807.03748 (2018)\\n[45] Gadelha, M., Wang, R., Maji, S.: Multiresolu-\\ntion tree networks for 3d point cloud processing.\\nIn: Proceedings of the European Conference on\\nComputer Vision (ECCV), pp. 103–118 (2018)\\n[46] Han, Z., Shang, M., Liu, Y.-S., Zwicker, M.:\\nView inter-prediction gan: Unsupervised repre-\\nsentation learning for 3d shapes by learning\\nglobal shape memories to support local view\\npredictions. In: Proceedings of the AAAI Con-\\nference on Artiﬁcial Intelligence, vol. 33, pp.\\n8376–8384 (2019)\\n[47] Chang, A.X., Funkhouser, T., Guibas, L., Han-\\nrahan, P., Huang, Q., Li, Z., Savarese, S.,\\nSavva, M., Song, S., Su, H., et al.: Shapenet:\\nAn information-rich 3d model repository. arXiv\\npreprint arXiv:1512.03012 (2015)\\n[48] Yi, L., Kim, V.G., Ceylan, D., Shen, I.-C., Yan,\\nM., Su, H., Lu, C., Huang, Q., Sheffer, A.,\\nGuibas, L.: A scalable active framework for\\nregion annotation in 3d shape collections. ACM\\nTransactions on Graphics (ToG) 35(6), 1–12\\n(2016)\\n[49] Armeni, I., Sax, S., Zamir, A.R., Savarese,\\nS.: Joint 2d-3d-semantic data for indoor scene\\nunderstanding. arXiv preprint arXiv:1702.01105\\n(2017)\\n[50] Klokov, R., Lempitsky, V.: Escape from cells:\\nDeep kd-networks for the recognition of 3d point\\ncloud models. In: Proceedings of the IEEE Inter-\\nnational Conference on Computer Vision, pp.\\n863–872 (2017)\\n[51] Shen, Y., Feng, C., Yang, Y., Tian, D.: Mining\\npoint cloud local structures by kernel correla-\\ntion and graph pooling. In: Proceedings of the\\nIEEE Conference on Computer Vision and Pat-\\ntern Recognition, pp. 4548–4557 (2018)\\n[52] Thomas, H., Qi, C.R., Deschaud, J.-E., Mar-\\ncotegui, B., Goulette, F., Guibas, L.J.: Kpconv:\\nFlexible and deformable convolution for point\\nclouds. In: Proceedings of the IEEE International\\nConference on Computer Vision, pp. 6411–6420\\n(2019)\\nThe Visual Comptuer 2022\\n[53] Atzmon, M., Maron, H., Lipman, Y.: Point con-\\nvolutional neural networks by extension oper-\\nators. ACM Transactions on Graphics (TOG)\\n37(4), 1–12 (2018)\\n[54] Liu, X., Han, Z., Liu, Y.-S., Zwicker, M.:\\nPoint2sequence: Learning the shape representa-\\ntion of 3d point clouds with an attention-based\\nsequence to sequence network. In: Proceedings\\nof the AAAI Conference on Artiﬁcial Intelli-\\ngence, vol. 33, pp. 8778–8785 (2019)\\n[55] Hassani, K., Haley, M.: Unsupervised multi-task\\nfeature learning on point clouds. In: Proceed-\\nings of the IEEE International Conference on\\nComputer Vision, pp. 8160–8171 (2019)\\n[56] Huang, Q., Wang, W., Neumann, U.: Recur-\\nrent slice networks for 3d segmentation of point\\nclouds. In: Proceedings of the IEEE Conference\\non Computer Vision and Pattern Recognition,\\npp. 2626–2635 (2018)\\n[57] Feng, Z., Zhou, Q., Gu, Q., Tan, X., Cheng,\\nG., Lu, X., Shi, J., Ma, L.: Dmt: Dynamic\\nmutual training for semi-supervised learning.\\narXiv preprint arXiv:2004.08514 (2020)\\n')])\n",
      "ResearchTopics(topic='Instance segmentation using modified UNet architectures and adaptive feature fusion', priority=2, query='unet instance segmentation', timestamp='2024-11-23 18:21:07', research_papers=[ResearchPaper(title='MGTUNet: An new UNet for colon nuclei instance segmentation and quantification', authors=[arxiv.Result.Author('Liangrui Pan'), arxiv.Result.Author('Lian Wang'), arxiv.Result.Author('Zhichao Feng'), arxiv.Result.Author('Zhujun Xu'), arxiv.Result.Author('Liwen Xu'), arxiv.Result.Author('Shaoliang Peng')], abstract='Colorectal cancer (CRC) is among the top three malignant tumor types in terms\\nof morbidity and mortality. Histopathological images are the gold standard for\\ndiagnosing colon cancer. Cellular nuclei instance segmentation and\\nclassification, and nuclear component regression tasks can aid in the analysis\\nof the tumor microenvironment in colon tissue. Traditional methods are still\\nunable to handle both types of tasks end-to-end at the same time, and have poor\\nprediction accuracy and high application costs. This paper proposes a new UNet\\nmodel for handling nuclei based on the UNet framework, called MGTUNet, which\\nuses Mish, Group normalization and transposed convolution layer to improve the\\nsegmentation model, and a ranger optimizer to adjust the SmoothL1Loss values.\\nSecondly, it uses different channels to segment and classify different types of\\nnucleus, ultimately completing the nuclei instance segmentation and\\nclassification task, and the nuclei component regression task simultaneously.\\nFinally, we did extensive comparison experiments using eight segmentation\\nmodels. By comparing the three evaluation metrics and the parameter sizes of\\nthe models, MGTUNet obtained 0.6254 on PQ, 0.6359 on mPQ, and 0.8695 on R2.\\nThus, the experiments demonstrated that MGTUNet is now a state-of-the-art\\nmethod for quantifying histopathological images of colon cancer.', url='http://arxiv.org/abs/2210.10981v2', pdf_path='./papers/2210.10981v2.MGTUNet__An_new_UNet_for_colon_nuclei_instance_segmentation_and_quantification.pdf', content=' \\nMGTUNet: An new UNet for colon nuclei instance \\nsegmentation and quantification \\n \\nLiangrui Pan  \\nCollege of Computer Science and \\nElectronic Engineering \\nHunanUniversity  \\nChang Sha, China \\npanlr@ hnu.edu.cn \\nZhujun Xu \\nCollege of Computer Science and \\nElectronic Engineering \\nHunanUniversity  \\nChang Sha, China \\nxzjcs1992@hotmail.com \\nLian Wang \\nCollege of Computer Science and \\nElectronic Engineering \\nHunanUniversity \\nChang Sha, China \\nlianwang@hnu.edu.cn \\nLiwen Xu \\nCollege of Computer Science and \\nElectronic Engineering \\nHunan University \\nChang Sha, China \\nxuliwen @hnu.edu.cn \\nZhichao Feng \\nDepartment of Radiology \\nThird Xiangya Hospital \\nCentral South University \\nChang Sha, China \\nfengzc2016@163.com \\nShaoliang Peng* \\nCollege of Computer Science and \\nElectronic Engineering \\nHunan University \\nChang Sha, China \\nslpeng@hnu.edu.cn  \\nAbstract—Colorectal cancer (CRC) is among the top three \\nmalignant tumor types in terms of morbidity and mortality. \\nHistopathological images are the gold standard for diagnosing \\ncolon cancer. Cellular nuclei instance segmentation and \\nclassification, and nuclear component regression tasks can aid \\nin the analysis of the tumor microenvironment in colon tissue. \\nTraditional methods are still unable to handle both types of \\ntasks end-to-end at the same time, and have poor prediction \\naccuracy and high application costs. This paper proposes a \\nnew UNet model for handling nuclei based on the UNet \\nframework, called MGTUNet, which uses Mish, Group \\nnormalization and transposed convolution layer to improve \\nthe segmentation model, and a ranger optimizer to adjust the \\nSmoothL1Loss values. Secondly, it uses different channels to \\nsegment and classify different types of nucleus, ultimately \\ncompleting the nuclei instance segmentation and classification \\ntask, \\nand \\nthe \\nnuclei \\ncomponent \\nregression \\ntask \\nsimultaneously. Finally, we did extensive comparison \\nexperiments using eight segmentation models. By comparing \\nthe three evaluation metrics and the parameter sizes of the \\nmodels, MGTUNet obtained 0.6254 on PQ, 0.6359 on mPQ, \\nand 0.8695 on R2. Thus, the experiments demonstrated that \\nMGTUNet is now a state-of-the-art method for quantifying \\nhistopathological images of colon cancer.  \\nKeywords—histopathology, images, nuclei, segmentation, \\nclassification \\nI. INTRODUCTION  \\nColorectal cancer (CRC) is the most common cancer in \\nthe world and a major contributor to mortality, accounting \\nfor more than 9% of all cancer incidences. Although early \\nCRC has a good prognosis with a five-year survival rate of \\nover 90%, the detection rates are relatively low and most \\npatients are diagnosed at an intermediate to late stage with a \\npoor prognosis [1], [2]. The common method like fecal \\nexamination cannot meet the needs of accuracy screening \\nand the supplementary methods are needed [3]. Usually, \\nhematoxylin-eosin (HE)-stained tissue slides of CRC \\npatients are used for detection and diagnosis. The \\nhistological slides highlight the nucleus and cytoplasm of \\ntissue cells, which characterize quantitative information of \\nthe tumor. While subjective evaluation of histological slides \\nby highly trained pathologists remains the gold standard for \\ncancer diagnosis and staging, this remains labor-intensive \\nand is limited by experience. \\nFurther, cancer cells are highly heterogeneous. They are \\ncapable of causing varying degrees of host inflammatory \\nresponse, angiogenesis and tumor necrosis. The spatial \\narrangement of these heterogeneous cell types has also \\nbeen shown to correlate with cancer staging [4], [5]. \\nTherefore, qualitative and quantitative analysis of different \\ntumor types at the cellular level can help pathologists to \\nbetter understand tumors and also to explore multiple \\noptions for treating cancer. The accurate localization of \\nclinically relevant structures is the first step in pathological \\nanalysis. For example, segmentation of each nucleus yields \\na characterization of the nucleus morphology, the results of \\nwhich can guide the prediction of cancer grade or survival \\nanalysis. Secondly, segmented cells reflecting nuclear \\nmorphological features need to be combined with the \\naccurate classification of different cell types to quantify \\nmost cellular information on the whole-slide images (WSI). \\nFor example, the ratio of the number of the tumor to \\nlymphocytes is often seen as a marker of prognosis in CRC. \\nHistopathological images are therefore the gold standard \\nfor the diagnosis of cancer. \\nHowever, machine learning can train predictive models \\nusing the vast amount of image data information contained \\nin billions of WSIs and has been widely applied in the field \\nof computational pathology [6], [7]. Traditional machine \\nlearning \\nalgorithms \\nrequire \\nmatrix \\neigenvalue \\ndecomposition, or singular value decomposition, and have a \\nhigh time complexity to handle large amounts of data. Some \\nalgorithms need to calculate the distance between all sample \\npoints, which has a high spatio-temporal complexity. More \\nimportantly, many traditional learning algorithms are \\ndifficult to combine with GPU parallel computing and are \\ntherefore unable to handle large scale data. Deep learning is \\nusually an unconstrained optimization problem, and the use \\nof SGD can significantly improve training efficiency, \\nbenefiting from GPU parallel computing acceleration, so it \\ncan then handle large amounts of data. However, deep \\nlearning using an end-to-end approach to predicting WSIs \\ncan lead to models with poor interpretability, making it \\ndifficult for diagnoses to aid doctors or make patients \\ncredible. Explainable and fair AI algorithms have always \\nbeen an aid to diagnosis that computational pathologists \\nwant. \\n \\nFig. 1. Full flow chart of MGTUNet for segmenting histopathology images. (The training data includes images and instance \\nground truth. The prediction results include nuclei instance map and class instance map. Second, MGTUNet can also segment \\ndifferent types of nucleus.)  \\n \\n \\nThe original intention of UNet model design is to solve \\nthe problem of medical image segmentation. At present, it \\nhas been widely used in imaging data processing tasks \\ncombined with deep learning [8]. It uses a U-shaped  \\nnetwork structure to obtain contextual information and \\nlocation information, and reconstructs and predicts \\npathological images through feature extraction and up-\\nsampling. The algorithm breakthrough of Transformer in \\nrecent years, combined with the UNet framework, has also \\nbeen initially applied in the field of image segmentation [9]. \\nHowever, for the segmentation task of the nucleus, this \\npaper verifies that the feature extraction effect of some \\nTransformers is not very significant. Secondly, the larger \\nmodel and computing resource requirements lead to the \\ninability to maximize the efficiency on the lizard dataset \\n[10]. Therefore, this paper proposes a new nuclei processing \\nUNet (MGTUNet) model to predict the type and location of \\nthe nucleus in pathology images. Our main contributions are \\nas follows: \\n1. Based on the UNet framework, we propose to use Mish, \\nGroup normalization and transposed convolution layer \\nto improve the segmentation model, and use the ranger \\noptimizer to tune the SmoothL1Loss value. \\n2. MGTUNet uses different channels to segment and \\nclassify \\ndifferent \\ntypes \\nof \\nnuclei, \\nultimately \\ncompleting the nuclei instance segmentation and \\nclassification task, and the nuclei component \\nregression task simultaneously. \\n3. We have done extensive comparison tests using the \\nCenet, PSPNet, UNet, Resnet50UNet, SegNet, \\nR2UNet, Ternausnet and MGTUNet segmentation \\nmodels and concluded that MGTUNet is the best \\nmethod for quantifying histopathological images. \\n \\nII. MATERIALS AND METHODS \\nA. Materials \\nThe histological images of colon tissue used in the \\nexperiments were obtained from six different datasets which \\nmake up the lizard dataset [11]. At a magnification of 20×, \\nthe pathologists annotated the complete segmentation of the \\ndifferent nuclei, which contained six different nuclei labels, \\nwhich were epithelial nucleus, connective tissue nucleus, \\nlymphocytes, plasma nucleus, neutrophils and this nucleus \\naccurately describe the colonic tumor microenvironment. \\nTo ensure that the lizard dataset contains a variety of \\ndifferent images of normal, inflammatory, dysplastic and \\ncancerous diseases of the colon, the dataset adds the \\npossibility of tagging unseen examples to generalize to other \\ntypes. 4,981 patches of 256×256 size are provided with the \\nlizard dataset. Each RGB image patch is associated with an \\ninstance segmentation map and a classification map, both of \\nwhich are also 256×256 in size. Instance segmentation maps \\nuniquely label each nucleus by containing values from 0 \\n(background) to N (number of nuclei). The classification \\nmap provides the category of each pixel within the patch. \\nSpecifically, the nuclei classification map includes values \\nfrom 0 (background) to C (number of classes). both the \\nRGB image and the segmentation/classification map are \\nstored as a single Numpy array. the RGB image array has \\ndimensions of 4981×256×256×3 and is of type unsigned 8-\\nbit integer, while the segmentation and classification map \\narrays have dimensions of 4981×256×256×2 and are of type \\nunsigned signed 16-bit integer [12]. Here, the first channel \\nis the instance segmentation map and the second channel is \\nthe classification map. 80% of the dataset is divided into a \\ntraining set and the remaining 20% is divided into a test set. \\nB. Methods \\nIn this paper, inspired by the UNet framework, we \\npropose to present a completely new segmentation network \\nfor processing nuclei, called MGTUNet. Fig. 1 depicts the \\nflowchart of MGTUNet segmentation. First, MGTUNet \\nuses the first half of the network to extract features from \\nhistopathology image patches and corresponding nuclei \\nlabels. The second half of MGTUNet is used to generate \\npredicted histopathology nuclei segmentation images and \\nclassification images, where each class of nucleus can be \\nsegmented and classified in a different channel. \\nMGTUNet consists of two parts, the encoder and the \\ndecoder. In particular, the encoder consists of four \\nConvBlock modules and four ConvPool modules. Each \\nConvBlock module is composed of two groups of \\nConvolution 2d, Mish, Group Normalization (GN); each \\nConvPool module is composed of one group of Convolution \\n2d, \\nMish, \\nGN. \\nThe \\ndecoder \\nconsists \\nof \\nfour  \\nTranspConvBlock modules, two ConvBlock modules and a \\nConvolution 2d layer. each TranspConvBlock module is \\ncomposed of transposed convolution(TC) layer and GN. \\nThe prediction results generated by the decoder are output \\nby the Convolution 2d layer. \\nFirstly, the histopathology image and labels are fed into \\nMGTUNet\\'s encoder before data augmentation such as \\nhorizontal/vertical \\nflip, \\nrotation, \\nscaling, \\ncropping, \\ncropping, panning, etc. is performed. The Convolution layer \\nin the ConvBlock module then performs feature extraction \\non the image. A convolution kernel of size 3×3 performs \\nmatrix computation on the image at a stride size of one. The \\npositions around the matrix generated by the convolution \\noperation that are zero are complemented by one. We \\nprocess the feature matrix using the Mish activation \\nfunction to incorporate non-linear factors to improve the \\nexpressiveness of the neural network, which can be \\nexpressed as: \\n\\x01\\x02\\x03\\x04 = \\x03 ∗tanh \\x02ln \\x021 + \\x0f\\x10\\x04\\x04                    (1) \\nWhere \\x03 is the input image feature matrix and \\x01\\x02\\x03\\x04 is the \\noutput image feature matrix. Compared with Swish and \\nReLu, Mish has better capabilities of generalization and \\nresult optimization, and can improve the quality of results. \\nThe group normalization can also solve the Internal, \\nCovariate and Shift problem with better results, and its \\ncomputational procedure is as follows [13]: \\n\\x11 =\\n\\x10\\x12\\x13\\x14\\x10\\x15\\n\\x16\\x17\\x18\\x19\\x14\\x10\\x15\\x1a\\x1b ∗\\x1c + \\x1d                         (2) \\n\\x03 in the input channel is divided into several groups, and \\nthe mean and standard deviation of each group are \\ncalculated separately; \\x1c and \\x1d are the sizes of the learnable \\nper-channel affine transformation parameter vectors; the \\nstandard deviation is calculated by biased estimation. This \\nlayer uses statistics computed from the input data in both \\ntraining and evaluation modes. After the pathological image \\nis processed by four ConvBlock modules, its feature matrix \\nwill be preliminarily learned. \\nThe ConvPool module will further process the \\npathological image features. The convolution nuclei in the \\nconvolution layer performs feature extraction with a stride \\nof two, and the image features after the convolution \\noperation are passed to Mish and GN for processing. So far, \\nthe work of the encoder part of MGTUNet has been \\ncompleted. \\nThe image feature matrix obtained by the encoder will \\nbe sent to the decoder. The TC layer in the \\nTranspConvBlock module is a transposed convolution \\noperation performed by a convolution kernel with a size of \\n2×2 with a stride size of two, which mainly plays the role of \\nup-sampling. Unlike the convolution operation, it can only \\nrestore the feature matrix to the original size value, but it is \\ndifferent from the original. The process is: fill some values \\nbetween the input feature map elements; fill some values \\naround the input feature map; flip the convolution nuclei \\nparameters up and down, left and right; do the classic \\nconvolution operation [14]. It can be expressed as: \\no = \\x1f\\n \\x12!\\x1a\"#\\n$\\n% + 1                            (3) \\nwhere & represents the size of the image input; \\x01 represents \\nthe size of the convolution nuclei; \\' represents the filled \\npixel value; ( represents the stride size; and o represents the \\nsize of the feature map, which is related to &, \\x01, \\', ( and also \\nto the way the matrix is filled. The transposed convolution \\noperation is followed by the transfer of the feature map to \\nthe GN. After feature extraction by the four ConvBlock \\nmodules, the feature map is finally output by the \\nconvolution layer. \\nThe experiments use SmoothL1Loss to adjust the \\naccuracy of the model. When the difference between the \\npredicted value and the ground truth is small (the absolute \\nvalue difference is less than 1), L2 Loss is actually used; \\nwhen the difference is large, it is the translation of L1 Loss. \\nSmoothL1Loss is actually a combination of L2Loss and \\nL1Loss. It also has some advantages of L2 Loss and L1 Loss \\n[15]. Its calculation can be expressed as: \\n)oss\\x02\\x03, \\x11\\x04 =\\n,\\n- ∑\\n/0.5 ∗\\x02\\x11 −\\x01\\x02\\x03 \\x04\\x04\",   &\\x01|\\x11 −\\x01\\x02\\x03 \\x04| < 1\\n|\\x11 −\\x01\\x02\\x03 \\x04| −0.5,   67ℎ\\x0f9:&(\\x0f \\n-\\n ;,\\n           (4) \\n\\x11 denotes the true image of the model; \\x01\\x02\\x03\\x04 denotes the \\nimage predicted by the model. A simple cross-validation \\nmethod allows us to obtain the final segmentation model. \\nSome hyper-parameters must be mentioned in the \\nexperiments. The experiments use the ranger optimizer; the \\nlearning rate is set to 0.001, which will decrease as time \\ngrows; the batch size is set to 8 and the epoch to 50, and the \\ntraining of the model stops early when the value of the loss \\nremains constant. All models were trained on a GPU \\n(NVIDIA V100) based on the PyTorch platform. \\nIII. RESULT AND DISCUSSION \\nA. Evalution \\nFor the nuclei instance segmentation and classification \\ntasks, we use a multi-class panoramic quality (PQ) to reflect \\nthe performance of the model, and for each type t, PQ> is \\ndefined as: \\n?@> =\\n|ABC|\\nDABCD\\x1aE\\nFDGBCD\\x1aE\\nF|GHC| ×\\n∑\\nJKL\\x02\\x10C,MC\\x04\\n\\x02NC,OC\\x04∈QR\\n|ABC|\\n         (5) \\nwhere \\x03 denotes a ground truth (GT) instance, \\x11 denotes the \\npredicted instance, and IoU denotes intersection over union. \\nIoU\\x02x, y\\x04 > 0.5 will uniquely match \\x03 and \\x11. Each type of \\nt  can be split into matched pairs (TP), unmatched GT \\ninstances (FN) and unmatched predicted instances (FP). We \\ndefine the multiclass PQ as the evaluation metric for the \\nexperiment, which has an all-class mean of: \\nmPQ =\\n,\\nA ∑?@>\\n>\\n                           (6) \\nFor the task of nuclei component regression, the \\nexperiments use a multi-class coefficient of determination \\nto determine the correlation between predicted and true \\ncounts. To do this, the statistics for each class are computed \\nindependently, and the results are then averaged. Therefore, \\nfor each nuclei class t, its correlation can be defined as: \\nY>\\n\" = 1 −\\nZ[[C\\nA[[C                              (7) \\nwhere Y\\\\\\\\ denotes the residual sum of squares and ]\\\\\\\\ \\ndenotes the total sum of squares. \\nB. Results \\nDue to the small sample size of the experiment, we used \\ndata augmentation to enrich the training set. The \\nexperiments use two datasets generated based on the lizard \\ndataset, the original one and the up-sampled one. First, we \\ntrained the Cenet, PSPNet, UNet, Resnet50UNet, SegNet, \\nR2UNet, Ternausnet and MGTUNet segmentation models \\nusing 80% of the data from the up-sampled dataset. The \\nremaining 20% of the data was used to test the models. To \\ncompare the performance of the models for nuclei instance \\nsegmentation and nuclei component regression, we counted \\nPQ>, mPQ and R2 in TABLE 1. the values of PQ and mPQ \\nfor UNet reached 0.6035 and 0.6409, 0.032 and 0.0112 \\nhigher than for MGTUNet, respectively. the value of R2 for \\nMGTUNet reached 0.7881, 0.0008 larger than for UNet. \\nHowever, the performance of the other UNet-based models \\nwas average. \\nTABLE I.  \\nPERFORMANCE COMPARISON OF MODELS TRAINED ON \\nTHE UPSAMPLED LIZARD DATASET. \\n \\nPQ \\nmPQ \\nR2 \\nCenet \\n0.5079 \\n0.5233 \\n0.2009 \\nPSPNet \\n0.5702 \\n0.604 \\n0.7088 \\nUNet \\n0.6035 \\n0.6409 \\n0.7872 \\nResnet50UNet \\n0.5989 \\n0.6322 \\n0.7396 \\nSegNet \\n0.4841 \\n0.4665 \\n0.1441 \\nR2UNet \\n0.5202 \\n0.5385 \\n1.1389 \\nTernausnet \\n0.5534 \\n0.6116 \\n0.4789 \\nMGTUNet \\n0.6003 \\n0.6297 \\n0.7881 \\n \\nFor the lizard dataset trained on the no up-sampling \\ndataset, we still used 80% of the data to train the Cenet, \\nPSPNet, \\nUNet, \\nResnet50UNet, \\nSegNet, \\nR2UNet, \\nTernausnet and MGTUNet segmentation models. The \\nremaining 20% of the data was used to test the models. \\nBased on the evaluation metrics PQ, mPQ and R2, we \\ntabulated the test results for the eight models in TABLE 2. \\nWe found that UNet still had values for PQ and mPQ that \\nwere 0.075 and 0.094 larger than MGTUNet. however, \\nMGTUNet had a value for R2 of 0.8695, which was 0.0011 \\nlarger than UNet. \\nTABLE II.  \\nPERFORMANCE COMPARISON OF MODELS TRAINED ON \\nTHE NO UPSAMPLING LIZARD DATASET. \\n \\nPQ \\nmPQ \\nR2 \\nCenet \\n0.489 \\n0.4947 \\n0.25891 \\nPSPNet \\n0.5357 \\n0.5563 \\n0.7637 \\nUNet \\n0.6329 \\n0.6445 \\n0.8684 \\nResnet50UNet \\n0.6117 \\n0.6251 \\n0.8548 \\nSegNet \\n0.4715 \\n0.4469 \\n0.3063 \\nR2UNet \\n0.5099 \\n0.5346 \\n-3.8478 \\nTernausnet \\n0.5663 \\n0.6043 \\n0.4942 \\nMGTUNet \\n0.6254 \\n0.6359 \\n0.8695 \\n \\nGiven that the size of the model affects computational \\nresources, training time and application costs, the parameter \\nsize of the model should not be ignored. In order to fairly \\ncompare the performance of the models, we used the \\nparameter sizes of the models for comparison. The \\nparameter size of UNet is 143.69 MB; the parameter size of \\nMGTUNet is 130.39 MB. therefore, using PQ, mPQ, R2 \\nand the parameter size of the model, we can determine that \\nMGTUNet is the nuclei instance segmentation and \\nclassification task with the nuclei component regression \\ntask as a standard method. \\nFurthermore, according to the predicted nuclei types, \\nour model quantified the cell count of six different cell types \\nin the tumor microenvironment in Fig. 2.  The \\ncomputational inference of cell count from WSI may \\neffectively supplement current methods in revealing spatial \\ntumor-infiltrating lymphocytes (TIL) patterns and cellular \\ncompositions. We expect this histopathological framework \\nto improve TME predictions and even construct promising \\nbiomarkers for tumor immunotherapy in the future. \\nC. Discussion \\nIn our experiments, we also used transformer-based \\nUNet frameworks such as SwinUNet, Transfuse and \\nTransUNet models for the nuclei instance segmentation and \\nclassification with nuclei component regression tasks [16]–\\n[18]. The training time of these models is approximately \\nfour times longer than that of the UNet model, and the \\nprediction performance of these models is lower than UNet. \\nTherefore, we did not use the transformer-based UNet \\nframework to design new models. Second, training the \\nmodel on the up-sampled dataset was generally not as \\neffective as training on the original dataset. It is possible that \\nsome of the same samples were present in the up-sampled \\ndataset, resulting in no improvement in the model training. \\nSecondly, the large number of samples may have caused the \\nmodel training to be biased towards identifying a particular \\ntype of nucleus. \\n \\nFig. 2. Quantified cell count in tumor microenvironment of \\none sample by the model. \\n \\nIV. CONCLUSION \\nThis paper proposes a new colon nuclei processing \\nmodel based on the UNet framework, named MGTUNet, \\nwhich uses Mish, Group normalization and transposed \\nconvolution layer to improve the segmentation model, and \\na ranger optimizer to adjust the SmoothL1Loss value. \\nSecondly, it uses different channels to segment and classify \\ndifferent types of nucleus, ultimately completing the \\nsegmentation \\ntask \\nand \\nthe \\nclassification \\ntask \\nsimultaneously. Finally, we did extensive comparison \\nexperiments \\nusing \\nthe \\nCenet, \\nPSPNet, \\nUNet, \\nResnet50UNet, \\nSegNet, \\nR2UNet, \\nTernausnet \\nand \\nMGTUNet segmentation models. The parameter sizes of the \\nmodels were compared by three evaluation metrics, PQ, \\nmPQ and R2, and MGTUNet obtained 0.6254, 0.6359 and \\n0.8695 for PQ, mPQ and R2 respectively. its model had the \\nsmallest parameter of all models at 130.39 MB. in summary, \\nthe experiments concluded that MGTUNet is the best \\nmethod for quantifying colon histopathological images. \\nACKNOWLEDGEMENTS \\nThis work was supported by NSFC Grants U19A2067; \\nScience Foundation for Distinguished Young Scholars of \\nHunan Province (2020JJ2009); National Key R&D \\nProgram of China 2017YFB0202602, 2018YFC0910405, \\n2017YFC1311003, 2016YFC1302500; Science Foundation \\nof \\nChangsha \\nZ202069420652, \\nkq2004010; \\nJZ20195242029, JH20199142034; The Funds of State Key \\nLaboratory of Chemo/Biosensing and Chemometrics and \\nPeng Cheng Lab. \\nREFERENCE \\n[1] \\nE. J. Kuipers et al., “Colorectal cancer,” Nat. Rev. Dis. Primer, \\nvol. 1, no. 1, p. 15065, Dec. 2015, doi: 10.1038/nrdp.2015.65. \\n[2] \\nP. Rawla, T. Sunkara, and A. Barsouk, “Epidemiology of \\ncolorectal cancer: incidence, mortality, survival, and risk factors,” \\nGastroenterol. Rev., vol. 14, no. 2, pp. 89–103, 2019, doi: \\n10.5114/pg.2018.81072. \\n[3] \\nJ. S. Lin, L. A. Perdue, N. B. Henrikson, S. I. Bean, and P. R. \\nBlasi, “Screening for Colorectal Cancer: Updated Evidence \\nReport and Systematic Review for the US Preventive Services \\nTask Force,” JAMA, vol. 325, no. 19, p. 1978, May 2021, doi: \\n10.1001/jama.2021.4417. \\n[4] \\nR. Lugano, M. Ramachandran, and A. Dimberg, “Tumor \\nangiogenesis: \\ncauses, \\nconsequences, \\nchallenges \\nand \\nopportunities,” Cell. Mol. Life Sci., vol. 77, no. 9, pp. 1745–1770, \\nMay 2020, doi: 10.1007/s00018-019-03351-7. \\n[5] \\nR.-Y. Ma, A. Black, and B.-Z. Qian, “Macrophage diversity in \\ncancer revisited in the era of single-cell omics,” Trends Immunol., \\nvol. \\n43, \\nno. \\n7, \\npp. \\n546–563, \\nJul. \\n2022, \\ndoi: \\n10.1016/j.it.2022.04.008. \\n[6] \\nN. Dimitriou, O. Arandjelović, and P. D. Caie, “Deep Learning \\nfor Whole Slide Image Analysis: An Overview,” Front. Med., vol. \\n6, p. 264, Nov. 2019, doi: 10.3389/fmed.2019.00264. \\n[7] \\nD. Komura and S. Ishikawa, “Machine Learning Methods for \\nHistopathological Image Analysis,” Comput. Struct. Biotechnol. \\nJ., vol. 16, pp. 34–42, 2018, doi: 10.1016/j.csbj.2018.01.001. \\n[8] \\nM. H. Hesamian, W. Jia, X. He, and P. Kennedy, “Deep Learning \\nTechniques for Medical Image Segmentation: Achievements and \\nChallenges,” J. Digit. Imaging, vol. 32, no. 4, pp. 582–596, Aug. \\n2019, doi: 10.1007/s10278-019-00227-x. \\n[9] \\nL. Pan et al., “Noise-reducing attention cross fusion learning \\ntransformer \\nfor \\nhistological \\nimage \\nclassification \\nof \\nosteosarcoma.” arXiv, Apr. 28, 2022. Accessed: May 18, 2022. \\n[Online]. Available: http://arxiv.org/abs/2204.13838 \\n[10] \\nJ. M. J. Valanarasu, P. Oza, I. Hacihaliloglu, and V. M. Patel, \\n“Medical transformer: Gated axial-attention for medical image \\nsegmentation,” 2021, pp. 36–46. \\n[11] \\nS. Basodi, C. Ji, H. Zhang, and Y. Pan, “Gradient amplification: \\nAn efficient way to train deep neural networks,” Big Data Min. \\nAnal., vol. 3, no. 3, pp. 196–207, Sep. 2020, doi: \\n10.26599/BDMA.2020.9020004. \\n[12] \\nS. Graham et al., “CoNIC: Colon Nuclei Identification and \\nCounting Challenge 2022.” arXiv, Nov. 29, 2021. Accessed: Aug. \\n08, 2022. [Online]. Available: http://arxiv.org/abs/2111.14485 \\n[13] \\nY. Wu and K. He, “Group Normalization,” p. 17. \\n[14] \\nH. Gao, H. Yuan, Z. Wang, and S. Ji, “Pixel Transposed \\nConvolutional Networks,” IEEE Trans. Pattern Anal. Mach. \\nIntell., pp. 1–1, 2019, doi: 10.1109/TPAMI.2019.2893965. \\n[15] \\nF. Lu, F. Qin, and J. Chen, “Blind Image Quality Assessment \\nBased On Multi-scale Spatial Pyramid Pooling,” in 2021 IEEE \\n23rd \\nInt \\nConf \\non \\nHigh \\nPerformance \\nComputing \\n& \\nCommunications; 7th Int Conf on Data Science & Systems; 19th \\nInt Conf on Smart City; 7th Int Conf on Dependability in Sensor, \\nCloud \\n& \\nBig \\nData \\nSystems \\n& \\nApplication \\n(HPCC/DSS/SmartCity/DependSys), Haikou, Hainan, China, Dec. \\n2021, pp. 1741–1747. doi: 10.1109/HPCC-DSS-SmartCity-\\nDependSys53884.2021.00256. \\n[16] \\nY. Zhang, H. Liu, and Q. Hu, “Transfuse: Fusing transformers and \\ncnns for medical image segmentation,” 2021, pp. 14–24. \\n[17] \\nJ. Chen et al., “TransUNet: Transformers make strong encoders \\nfor medical image segmentation,” ArXiv Prepr. ArXiv210204306, \\n2021. \\n[18] \\nH. Cao et al., “Swin-UNet: UNet-like pure transformer for \\nmedical image segmentation,” ArXiv Prepr. ArXiv210505537, \\n2021. \\n \\n \\n'), ResearchPaper(title='HoVer-UNet: Accelerating HoVerNet with UNet-based multi-class nuclei segmentation via knowledge distillation', authors=[arxiv.Result.Author('Cristian Tommasino'), arxiv.Result.Author('Cristiano Russo'), arxiv.Result.Author('Antonio Maria Rinaldi'), arxiv.Result.Author('Francesco Ciompi')], abstract='We present HoVer-UNet, an approach to distill the knowledge of the\\nmulti-branch HoVerNet framework for nuclei instance segmentation and\\nclassification in histopathology. We propose a compact, streamlined single UNet\\nnetwork with a Mix Vision Transformer backbone, and equip it with a custom loss\\nfunction to optimally encode the distilled knowledge of HoVerNet, reducing\\ncomputational requirements without compromising performances. We show that our\\nmodel achieved results comparable to HoVerNet on the public PanNuke and Consep\\ndatasets with a three-fold reduction in inference time. We make the code of our\\nmodel publicly available at https://github.com/DIAGNijmegen/HoVer-UNet.', url='http://arxiv.org/abs/2311.12553v3', pdf_path='./papers/2311.12553v3.HoVer_UNet__Accelerating_HoVerNet_with_UNet_based_multi_class_nuclei_segmentation_via_knowledge_distillation.pdf', content='“HOVER-UNET”: ACCELERATING HOVERNET WITH UNET-BASED MULTI-CLASS\\nNUCLEI SEGMENTATION VIA KNOWLEDGE DISTILLATION\\nCristian Tommasino1, Cristiano Russo1, Antonio Maria Rinaldi1∗, Francesco Ciompi2*\\n1Department of Electrical Engineering and Information Technology\\nUniversity of Napoli Federico II, Napoli, Italy\\n2Department of Pathology, Radboud University Medical Center, Nijmegen, The Netherlands\\nABSTRACT\\nWe present “HoVer-UNet,” an approach to distill the knowl-\\nedge of the multi-branch HoVerNet framework for nuclei in-\\nstance segmentation and classification in histopathology. We\\npropose a compact, streamlined single UNet network with a\\nMix Vision Transformer backbone, and equip it with a cus-\\ntom loss function to optimally encode the distilled knowledge\\nof HoVerNet, reducing computational requirements with-\\nout compromising performances. We show that our model\\nachieved results comparable to HoVerNet on the public Pan-\\nNuke and Consep datasets with a three-fold reduction in infer-\\nence time. We make the code of our model publicly available\\nat https://github.com/DIAGNijmegen/HoVer-UNet.\\nIndex Terms— Instance Nuclei Segmentation, Nuclei\\nClassification, Knowledge Distillation, HoVerNet\\n1. INTRODUCTION\\nNuclei panoptic segmentation, i.e., the simultaneous detec-\\ntion, segmentation, and classification of nuclear instances, is\\nat the core of the automation of several tasks in digital pathol-\\nogy, particularly in the analysis of routine Hematoxylin and\\nEosin (H&E) stained histology slides [1]. In recent years,\\nthis task has been increasingly addressed using deep learn-\\ning techniques, achieving accurate segmentation results that\\ncan power the design of computational biomarkers [2]. How-\\never, several studies have addressed the challenge of nuclei\\ninstance segmentation and classification, with a key concern\\nbeing the inference time required for accurate results [3].\\nHoVerNet. Common nuclei segmentation approaches in-\\nvolve a multi-task learning paradigm based on convolutional\\nneural networks with a single encoder and multiple decoders,\\neach dedicated to a specific task, followed by traditional\\nenergy-based methods such as the watershed algorithm to\\nprocess output maps generated by deep learning models.One\\nof the best incarnations of this approach is the HoVerNet\\n∗Authors jointly supervised this work.\\nThis work has been submitted to the IEEE for possible publication.\\nCopyright may be transferred without notice, after which this version may\\nno longer be accessible.\\nMixViT Block\\nU-Net Decoder\\nBlock\\nMixViT Block\\nMixViT Block\\nMixViT Block\\nU-Net Decoder\\nBlock\\nU-Net Decoder\\nBlock\\nU-Net Decoder\\nBlock\\nU-Net Decoder\\nBlock\\nMixViT Block\\nEncoder\\nHV Branch\\nNP Branch\\nTP Branch\\nEncoder\\nDecoder\\nWxHxC\\nWxHx2\\nWxHx2\\nStudent Loss\\nLoss\\nHoVerNet\\nFast\\nHoVerNet\\nDistill Loss\\nHoVerNet\\nFaster\\nHoVerNet\\nGround\\nTruth\\nLoss\\nComputation\\nInput\\nSkip\\nConnection\\nPooling\\nUpConv\\nWxHx(4+C)\\nWxHx(4+C)\\nFig. 1: HoVerNet distillation framework. Input image is given\\nto the teacher (yellow component) and the student (blue com-\\nponent). The distillation loss is computed between the teacher\\noutput and the student output, while the student loss is com-\\nputed between the generated ground truth (violet component)\\nand student output. Lastly, both losses are combined to have\\nthe final loss (red components).\\nmodel introduced by Graham et al. [4], which features a pre-\\nactivated ResNet50 as an encoder and three decoders, which\\nperform nuclei prediction (NP), horizontal and vertical maps\\nprediction (HV), and nuclei classification (NC). Afterward,\\nHoVerNet runs post-processing using NP and HV output to\\nobtain the nuclei instance and combines it with the TP output\\nto classify nuclei (Figure 1). The effectiveness of this net-\\nwork has been shown on several public datasets [5, 6], but its\\ndimension and complexity lead to high inference time.\\nKnowledge distillation. Introduced by Hinton et al. [7],\\nknowledge distillation enables the training of a smaller net-\\nwork (student) based on a more complex network (teacher)\\nby using the teacher’s predictions to guide and improve the\\nstudent’s learning process. For this, it uses a loss function that\\nmerges two hyperparameters: 1) temperature (T) to smooth\\nthe predictions, allowing the student to learn while reduc-\\ning the influence of biases from the teacher, and 2) alpha\\n(α), to combine the student loss, computed between the stu-\\ndent’s predictions and the ground truth, with the distillation\\nloss. The latter is calculated as the Kullback-Leibler diver-\\ngence between the scaled softmax logits of the student and\\nteacher, multiplied by 1/T 2. In this paper, we propose us-\\narXiv:2311.12553v3  [eess.IV]  4 Dec 2023\\ning KD to derive a lightweight panoptic segmentation model\\nfrom the original HoVerNet, aiming to maintain HoVerNet’s\\nperformance while speeding up inference time.\\n2. METHOD\\nIn this section, we present the details of HoVer-UNet, high-\\nlighting the distillation strategy utilized during the training\\nstep and introducing the proposed loss function.\\nDistillation Framework.\\nIn our KD framework, we\\nadopt an offline technique using HoVerNet as a pre-trained\\nteacher network. Given that HoVerNet performs nuclei in-\\nstance segmentation and classification through three branches,\\nour distillation strategy is based on the idea of combining all\\noutput branches of HoVerNet into a single branch network\\n(see Figure 1. Note that we aim to train a student that can\\nreplace only the HoVerNet backbone, not its post-processing\\nsteps, which we left unvaried. We employ a single-branch\\nUNet [8] as our student model and join all HoVerNet branch\\noutputs into a single branch with a number of output channels\\nequal to the total number of HoVerNet’s branches. In partic-\\nular, we used a Mix Vision Transformer (MixViT) [9] as the\\nbackbone for UNet, resulting in the best combination based\\non our experiments.\\nLoss function. We propose a custom loss function to dis-\\ntill HoVerNet into a single-branch UNet. As suggested by the\\nKD theory [7], our loss is a linear combination of two losses,\\nthe student loss between the student and the ground truth and\\nthe distillation loss between the student and the teacher regu-\\nlated by α parameter, and defined as:\\nL = α · Lstudent + (1 −α) · Ldistill.\\n(1)\\nWith regards to individual loss terms, we were inspired by\\nHoVerNet loss, and we specifically modified it for KD. In\\ndetail, the student and distillation losses, denoted as Lstudent\\nand Ldistill, respectively, are derived from a linear combina-\\ntion of losses across three branches: NP, HV, and TP. The\\ntotal loss for each branch, Lk, is defined as the sum of the\\nindividual losses from these branches, where k represents ei-\\nther the student or distillation case. In all formulas reported\\nin the following, x is student prediction, while y for distill\\nloss is HoVerNet prediction, and for student loss is ground\\ntruth. For the HV branch, the loss is formulated as LHV =\\nMSE(x, y) + MSGE(x, y), incorporating Mean Squared Er-\\nror (MSE) and Mean Squared Gradient Error (MSGE). For\\nthe NP and TP branches, the student loss is similarly de-\\nfined as LNP = LTP = CE(x, y) + DICE(x, y), combining\\nWeighted Cross-Entropy (CE) loss with Dice loss (DICE). In\\ncontrast, the distillation loss for these branches is expressed\\nas LNP = LTP = CE(x, y) + KLD(x, y), utilizing Weighted\\nCross-Entropy loss and Kullback-Leibler Divergence (KLD).\\n3. EXPERIMENTAL RESULTS\\nDatasets. In this work we used two datasets, namely Pan-\\nNuke [10] for training HoVer-UNet and CoNSeP [4] for val-\\nidating results on external data. PanNuke consists of 6078\\nH&E tiles, each of size 256 × 256, covering 19 different tis-\\nsue types. It was designed for nuclei instance classification\\nand segmentation and contains annotations of five types of\\nnuclei: neoplastic, inflammatory, connective/soft tissue, dead,\\nand healthy epithelial. CoNSeP consists of 41 H&E stained\\nimage tiles, each of size 1, 000×1, 000 pixels at 40× objective\\nmagnification, with annotations of seven types of nuclei: ma-\\nlignant, normal, endothelial, miscellaneous, Fibroblas, Mus-\\ncle, and inflammatory.\\nMetrics. To evaluate the efficacy of our framework, we\\nutilized the metrics as recommended in the studies of Graham\\net al. [4] and Gamper et al. [10]. Specifically, we employed\\nthe Panoptic Quality metric, initially proposed in [1], in con-\\njunction with the F-score, introduced by Graham et al. [4].\\nTraining. To train our network, we use the Adam op-\\ntimizer with an initial learning rate equal to 10−4 and betas\\nequal to (0.99, 0.9999). We also used a reduced on-plateau\\nlearning scheduler with a patience of 5, a scale factor of 10−1,\\na delta of 10−4, a minimum learning rate of 10−6, and an\\nearly stopping with the patience of 10. Furthermore, we used\\na depth of encoder and decoder of 5 and initialized the en-\\ncoders with ImageNet-pretrained weights.\\nHyperparameter definition.\\nOur framework incorpo-\\nrates three hyperparameters: the temperature coefficient (T),\\nthe parameter α, and the backbone. Throughout our experi-\\nments, we explored a spectrum of values for these parameters.\\nFor T, we examined values of 1, 3, and 5. For α, we consid-\\nered values of 0 and 0.5.\\nBackbone selection.\\nAs for the backbone, we exper-\\nimented with various architectures, including MixViT-B0,\\nMixViT-B1, MixViT-B2, and MixViT-B3. We selected these\\nmodels because they are substantially faster than HoVerNet\\nand exhibit fewer Multiply-Accumulate Operations (MACs).\\nSpecifically, the most complex model has 10.63 GMac and\\nan inference time of 0.065 seconds, compared to HoVerNet,\\nwhich has 149.73 GMac and an inference time of 0.835 sec-\\nonds. We computed the inference times as an average on a\\nbatch of 16 patches with shape 256x256.\\nResults on PanNuke. We evaluated several performance\\nmetrics including binary panoptic quality (PQB), multiclass\\npanoptic quality (PQM), panoptic quality for specific classes\\n(PQN,I,C,D,E), F-score detection (Fd), and F-score for in-\\ndividual classes (F N,I,C,D,E). The categories are denoted\\nas Neoplastic (N), Inflammatory (I), Connective/Soft Tissue\\n(C), Dead Cells (D), and Epithelial (E). To ensure robustness,\\nwe conducted the training using a 3-fold cross-validation\\n(3-CV) approach, following the recommendation of the Pan-\\nNuke authors [10]. All reported results represent the mean\\nacross the test outcomes. Considering the extensive amount\\nof data collected from our experiments, we present our best-\\nperforming results in comparison to DIST [11], Mask-RCNN\\n[12], Micro-Net [13], and HoVerNet [10] 1. Specifically, our\\noptimal result was achieved using the UNet model with a\\nMixViT-B2 backbone, employing α = 0.5 and T = 1 as the\\nhyperparameters.\\nTable 1: Panoptic Quality comparison on PanNuke dataset\\nModel\\nP QB\\nP QM\\nP QN\\nP QI\\nP QC\\nP QD\\nP QE\\nDIST\\n0.5346 0.3406 0.4390 0.3430 0.2750 0.0000 0.2900\\nMask-RCNN\\n0.5528 0.3688 0.4720 0.2900 0.3000 0.0690 0.4030\\nMicro-Net\\n0.6053 0.4059 0.5040 0.3330 0.3340 0.0510 0.4420\\nHoVerNet\\n0.6596 0.4629 0.5510 0.4170 0.3880 0.1390 0.4910\\nOurs\\n0.6286 0.4475 0.5239 0.4009 0.3794 0.0762 0.4779\\nTable 1 presents the PQ evaluation results.\\nCompared\\nwith HoVerNet, our solution achieved lower scores yet in line\\nwith HoVerNet’s performance, outperforming the other net-\\nworks listed in the table. However, when considering the re-\\nsults of the F −score, as shown in Table 2, our solution\\ndemonstrates comparable performance to HoVerNet and out-\\nperforms the other networks. Moreover, our proposed solu-\\ntion demonstrates a significant advantage in terms of process-\\ning speed compared to HoVerNet. This is primarily attributed\\nto our architecture designed to reduce inference time. Addi-\\ntionally, we have preserved the same post-processing method-\\nology as previously outlined, further solidifying our approach\\nas an efficient option for this task.\\nTable 2: F-Score comparison on PanNuke dataset\\nModel\\nFd\\nF N\\nF I\\nF C\\nF D\\nF E\\nDIST\\n0.73\\n0.50\\n0.42\\n0.39\\n0.00\\n0.35\\nMask-RCNN\\n0.72\\n0.59\\n0.50\\n0.42\\n0.22\\n0.52\\nMicro-Net\\n0.80\\n0.62\\n0.52\\n0.47\\n0.19\\n0.58\\nHoVerNet\\n0.80\\n0.62\\n0.54\\n0.49\\n0.31\\n0.56\\nOurs\\n0.79\\n0.64\\n0.53\\n0.48\\n0.18\\n0.62\\nResults on CoNSeP. We undertook a comparative anal-\\nysis between HoVerNet and HoVer-UNet, both pre-trained\\non PanNuke, leveraging the CoNSeP dataset [4] for external\\nevaluation purposes on out-of-domain data. Given that the\\nclassification of nuclei only showed partial correspondence of\\nclasses between the PanNuke and CoNSeP and consequently\\nbetween the trained HoVer-UNet and CoNSeP targets, we\\nremapped labels into several subclasses for a more detailed\\ncomparison. The newly defined classes were the Neoplastic,\\nInflammatory, Epithelial, and Miscellaneous.\\nWe mapped\\nthe Neoplastic class with PanNuke’s neoplastic and CoN-\\nSeP’s dysplastic/malignant epithelial, the Inflammatory class\\nwith PanNuke’s inflammatory and CoNSeP’s inflammatory,\\nthe Epithelial class with PanNuke’s epithelial and CoNSeP’s\\nhealthy epithelial, and the Miscellaneous class with Pan-\\n1HoVerNet trained on PanNuke is fast version.\\nSee https://github.com/vqdang/hover net\\nNuke’s dead and connective tissues as well as CoNSeP’s\\nother types, which include fibroblast, muscle, and endothelial\\ntissues. Table 3 shows the results demonstrating that our so-\\nlution outperforms HoVerNet in terms of PQ, though it falls\\nshort in terms of F-score detection. Regarding classification\\nmetrics, our solution outperforms HoVerNet across neoplastic\\nand epithelial nuclei; it is practically equal for miscellaneous\\nand worse for inflammatory.\\nLastly, the inference time is\\nabout three times lower.\\nFigure 2 shows visual examples\\nTable 3: Multiclass results on CoNSeP dataset\\nModel\\nFd\\nF N\\nF I\\nF E\\nF O\\nP Q\\nInf. Time(s)\\nHoVerNet\\n0.818\\n0.526 0.758 0.495 0.559\\n0.415\\n∼48\\nOurs\\n0.742\\n0.594 0.681 0.603 0.557\\n0.599\\n∼17\\nof the results of HoVerNet and HoVer-UNet compared with\\nthe CoNSeP reference standard. Overall, the similarity be-\\ntween the results supports the practical effectiveness of our\\napproach.\\nOurs\\nHoVerNet\\nGround T\\nruth\\nOurs\\nHoVerNet\\nGround T\\nruth\\nNeoplastic\\nInflammatory\\nEpithelial\\nMiscellaneous\\nFig. 2: Nuclei segmentation and classification comparison be-\\ntween CoNSeP ground truth, HoVerNet, and our predictions.\\n4. CONCLUSION\\nWe have presented a novel approach for panoptic segmen-\\ntation in histopathological analysis of H&E stained images.\\nOur method uses a KD offline approach to train a UNet net-\\nwork combined with a Mix Vision Transformer backbone.\\nOur approach yields results comparable to the state-of-the-\\nart HoVerNet model and considerably reduces inference time.\\nFurthermore, our approach does not increase training time\\nbecause our network has fewer parameters and MACs than\\nHoVerNet, and we make HoVerNet inferences only once and\\nthen use them in the distillation loss. Lastly, considering a\\nwhole-slide image, if HoVerNet takes 1 hour, we down it to\\n≈20 minutes with HoVer-UNet, with overall results in prac-\\ntice comparable. This approach performs similarly to leading\\nmodels and lays the groundwork for the progression of swifter\\ncomputational pathology solutions, such as biomarkers and\\nautomated quantification for subsequent tasks. Notably, our\\nmethod is approximately three times faster than HoVerNet.\\nWe have made our solution publicly available to promote its\\nuse in future developments in computational pathology.\\n5. COMPLIANCE WITH ETHICAL STANDARDS\\nThis research study was conducted retrospectively using hu-\\nman subject data available in open access by the Tissue Image\\nAnalytics (TIA) Centre. Ethical approval was not required, as\\nconfirmed by the license attached with the open-access data.\\n6. ACKNOWLEDGMENTS\\nThis work was partly supported by a research grant from\\nthe Netherlands Organization for Scientific Research (project\\nnumber 18388). F.C. was Chair of the Scientific and Med-\\nical Advisory Board of TRIBVN Healthcare, France, and\\nreceived advisory board fees from TRIBVN Healthcare,\\nFrance in the last five years. He is shareholder of Aiosyn\\nBV, the Netherlands.\\nWe also acknowledge financial sup-\\nport from the project Digital Health Solutions in Commu-\\nnity Medicine (DHEAL-COM) E63C22003790001 PNC-E3-\\n2022-23683267 PNC - HLS-DH.\\n7. REFERENCES\\n[1] A Kirillov, K He, R Girshick, C Rother, and P Dol-\\nlar,\\n“Panoptic segmentation. in 2019 ieee,”\\nin CVF\\nConference on Computer Vision and Pattern Recogni-\\ntion (CVPR), 2018, pp. 9396–9405.\\n[2] Simon Graham, Fayyaz Minhas, Mohsin Bilal, Mah-\\nmoud Ali, Yee Wah Tsang, Mark Eastwood, Noorul Wa-\\nhab, Mostafa Jahanifar, Emily Hero, Katherine Dodd,\\net al.,\\n“Screening of normal endoscopic large bowel\\nbiopsies with interpretable graph learning: a retrospec-\\ntive study,” Gut, 2023.\\n[3] Tomohiro Hayakawa, VB Surya Prasath, Hiroharu\\nKawanaka, Bruce J Aronow, and Shinji Tsuruoka,\\n“Computational nuclei segmentation methods in digital\\npathology: a survey,” Archives of Computational Meth-\\nods in Engineering, vol. 28, pp. 1–13, 2021.\\n[4] Simon Graham, Quoc Dang Vu, Shan E Ahmed Raza,\\nAyesha Azam, Yee Wah Tsang, Jin Tae Kwak, and\\nNasir Rajpoot, “Hover-net: Simultaneous segmentation\\nand classification of nuclei in multi-tissue histology im-\\nages,”\\nMedical Image Analysis, vol. 58, pp. 101563,\\n2019.\\n[5] Guillaume Jaume, Pushpak Pati, Valentin Anklin, An-\\ntonio Foncubierta, and Maria Gabrani,\\n“Histocartog-\\nraphy: A toolkit for graph analytics in digital pathol-\\nogy,” in MICCAI Workshop on Computational Pathol-\\nogy. PMLR, 2021, pp. 117–128.\\n[6] Richard J Chen, Ming Y Lu, Drew FK Williamson,\\nTiffany Y Chen, Jana Lipkova, Zahra Noor, Muham-\\nmad Shaban, Maha Shady, Mane Williams, Bumjin Joo,\\net al., “Pan-cancer integrative histology-genomic analy-\\nsis via multimodal deep learning,” Cancer Cell, vol. 40,\\nno. 8, pp. 865–878, 2022.\\n[7] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean, “Distill-\\ning the knowledge in a neural network,” arXiv preprint\\narXiv:1503.02531, 2015.\\n[8] Olaf Ronneberger, Philipp Fischer, and Thomas Brox,\\n“U-net:\\nConvolutional networks for biomedical im-\\nage segmentation,” in Medical Image Computing and\\nComputer-Assisted Intervention–MICCAI 2015: 18th\\nInternational Conference, Munich, Germany, October\\n5-9, 2015, Proceedings, Part III 18. Springer, 2015, pp.\\n234–241.\\n[9] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandku-\\nmar, Jose M Alvarez, and Ping Luo, “Segformer: Sim-\\nple and efficient design for semantic segmentation with\\ntransformers,” Advances in Neural Information Process-\\ning Systems, vol. 34, pp. 12077–12090, 2021.\\n[10] Jevgenij Gamper, Navid Alemi Koohbanani, Ksenija\\nBenes, Simon Graham, Mostafa Jahanifar, Syed Ali\\nKhurram, Ayesha Azam, Katherine Hewitt, and Nasir\\nRajpoot, “Pannuke dataset extension, insights and base-\\nlines,” arXiv preprint arXiv:2003.10778, 2020.\\n[11] Peter Naylor, Marick La´e, Fabien Reyal, and Thomas\\nWalter, “Segmentation of nuclei in histopathology im-\\nages by deep regression of the distance map,”\\nIEEE\\ntransactions on medical imaging, vol. 38, no. 2, pp.\\n448–459, 2018.\\n[12] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross\\nGirshick, “Mask r-cnn,” in Proceedings of the IEEE\\ninternational conference on computer vision, 2017, pp.\\n2961–2969.\\n[13] Shan E Ahmed Raza, Linda Cheung, Muhammad Sha-\\nban, Simon Graham, David Epstein, Stella Pelengaris,\\nMichael Khan, and Nasir M Rajpoot, “Micro-net: A\\nunified model for segmentation of various objects in mi-\\ncroscopy images,” Medical image analysis, vol. 52, pp.\\n160–173, 2019.\\n'), ResearchPaper(title='UNet#: A UNet-like Redesigning Skip Connections for Medical Image Segmentation', authors=[arxiv.Result.Author('Ledan Qian'), arxiv.Result.Author('Xiao Zhou'), arxiv.Result.Author('Yi Li'), arxiv.Result.Author('Zhongyi Hu')], abstract=\"As an essential prerequisite for developing a medical intelligent assistant\\nsystem, medical image segmentation has received extensive research and\\nconcentration from the neural network community. A series of UNet-like networks\\nwith encoder-decoder architecture has achieved extraordinary success, in which\\nUNet2+ and UNet3+ redesign skip connections, respectively proposing dense skip\\nconnection and full-scale skip connection and dramatically improving compared\\nwith UNet in medical image segmentation. However, UNet2+ lacks sufficient\\ninformation explored from the full scale, which will affect the learning of\\norgans' location and boundary. Although UNet3+ can obtain the full-scale\\naggregation feature map, owing to the small number of neurons in the structure,\\nit does not satisfy the segmentation of tiny objects when the number of samples\\nis small. This paper proposes a novel network structure combining dense skip\\nconnections and full-scale skip connections, named UNet-sharp (UNet\\\\#) for its\\nshape similar to symbol \\\\#. The proposed UNet\\\\# can aggregate feature maps of\\ndifferent scales in the decoder sub-network and capture fine-grained details\\nand coarse-grained semantics from the full scale, which benefits learning the\\nexact location and accurately segmenting the boundary of organs or lesions. We\\nperform deep supervision for model pruning to speed up testing and make it\\npossible for the model to run on mobile devices; furthermore, designing two\\nclassification-guided modules to reduce false positives achieves more accurate\\nsegmentation results. Various experiments of semantic segmentation and instance\\nsegmentation on different modalities (EM, CT, MRI) and dimensions (2D, 3D)\\ndatasets, including the nuclei, brain tumor, liver, and lung, demonstrate that\\nthe proposed method outperforms state-of-the-art models.\", url='http://arxiv.org/abs/2205.11759v1', pdf_path='./papers/2205.11759v1.UNet___A_UNet_like_Redesigning_Skip_Connections_for_Medical_Image_Segmentation.pdf', content='arXiv:2205.11759v1  [eess.IV]  24 May 2022\\nUNet#: A UNet-like Redesigning Skip Connections\\nfor Medical Image Segmentation\\nLedan Qian\\nCollege of Mathematics and Physics\\nWenzhou University\\nWenzhou, China 325025\\n00802005@wzu.edu.cn\\nXiao Zhou\\nInformation Technology Center\\nWenzhou University\\nWenzhou, China 325025\\n00802002@wzu.edu.cn\\nYi Li\\nCollege of Computer Science and Artiﬁcial Intelligence\\nWenzhou University\\nWenzhou, China 325025\\nliyi@wzu.edu.cn\\nZhongyi Hu\\nCollege of Computer Science and Artiﬁcial Intelligence\\nWenzhou University\\nWenzhou, China 325025\\nhuzhongyi@wzu.edu.cn\\nAbstract\\nAs an essential prerequisite for developing a medical intelligent assistant system,\\nmedical image segmentation has received extensive research and concentration\\nfrom the neural network community. A series of UNet-like networks with encoder-\\ndecoder architecture has achieved extraordinary success, in which UNet2+ and\\nUNet3+ redesign skip connections, respectively proposing dense skip connection\\nand full-scale skip connection and dramatically improving compared with UNet\\nin medical image segmentation. However, UNet2+ lacks sufﬁcient information\\nexplored from the full scale, which will affect the learning of organs’ location\\nand boundary. Although UNet3+ can obtain the full-scale aggregation feature\\nmap, owing to the small number of neurons in the structure, it does not satisfy the\\nsegmentation of tiny objects when the number of samples is small. This paper pro-\\nposes a novel network structure combining dense skip connections and full-scale\\nskip connections, named UNet-sharp (UNet#) for its shape similar to symbol #.\\nThe proposed UNet# can aggregate feature maps of different scales in the decoder\\nsub-network and capture ﬁne-grained details and coarse-grained semantics from\\nthe full scale, which beneﬁts learning the exact location and accurately segment-\\ning the boundary of organs or lesions. We perform deep supervision for model\\npruning to speed up testing and make it possible for the model to run on mobile\\ndevices; furthermore, designing two classiﬁcation-guided modules to reduce false\\npositives achieves more accurate segmentation results. Various experiments of se-\\nmantic segmentation and instance segmentation on different modalities (EM, CT,\\nMRI) and dimensions (2D, 3D) datasets, including the nuclei, brain tumor, liver\\nand lung, demonstrate that the proposed method outperforms state-of-the-art mod-\\nels.\\nPreprint. Under review.\\n1\\nIntroduction\\nComputer-aided diagnosis(CAD) system plays a vital role in disease diagnosis and treatment (Doi,\\n2007). Medical image segmentation is the ﬁrst step of all tasks for the medical image processing\\ntask based on a CAD system. Essentially, it classiﬁes the input medical images with pixel granular-\\nity, aiming to make the human tissue or pathological structure more extinct and intuitive. Medical\\nworkers can model the relevant organizations by using the segmentation results for subsequent appli-\\ncations. Medical image segmentation is a crucial signiﬁcance in providing noninvasive information\\nabout the human structure. It is also helpful for radiologists to visualize the anatomical structure,\\nsimulate biological processes, locate pathological tissues, track the progress of diseases, and provide\\nthe information needed to evaluate radiotherapy or surgery (Norouzi et al., 2014).\\nMedical images are the discrete image representation generated by sampling or reconstructing. Its\\nvalues mapping to different spatial locations can reﬂect the anatomical structure or functional orga-\\nnization of the human body. Medical images include computed tomography (CT), Ultrasound (US),\\nMagnetic Resonance Imaging (MRI), X-ray, etc. These images are different from natural images\\nwith obvious boundaries, showing human tissue or lesion area obtained from professional imaging\\ninstruments; accordingly, medical images have the unclear edge contour of organs and tissues and\\ncomplex brightness changes. The application of segmentation technology in these two images con-\\nsequently presents different performance requirements. It is worth mentioning that medical images\\nhave higher standards for segmentation details.\\nThe emergence and development of deep learning technology have effectively solved many issues\\nencountered in medical image segmentation. Using the powerful learning ability of the deep neural\\nnetwork can abstract lesions or organ targets in high-dimensional through nonlinear transformation\\nand in-depth extract the high-level semantic information of medical images layer by layer. This in-\\nformation describes the internal attributes of lesions or organs and can more accurately segment the\\nimage region of interest (ROI) automatically in medical images. UNet (Ronneberger et al., 2015)\\nis the most classic application case in medical image segmentation. Recently, it has been used as a\\nbenchmark network to investigate a series of novel strategies, such as the introduction of ResNet and\\nDesNet structural blocks (Drozdzal et al., 2016; Li et al., 2018), the development of attention mech-\\nanism (Gaál et al., 2020; Oktay et al., 2018), the reconstruction of convolution mode (Chen et al.,\\n2017), and the redesign of skip connection (Huang et al., 2020; Zhou et al., 2018, 2019).\\nBeneﬁting from the redesign of the skip connection, UNet2+ (Zhou et al., 2019) and UNet3+\\n(Huang et al., 2020) have brought signiﬁcant performance improvement compared with UNet. Nev-\\nertheless, it can not be ignored that UNet2+ lacks sufﬁcient, comprehensive information exploring\\nfrom the full scale, which makes it unable to learn the location and boundary of organs distinctly.\\nDue to the fewer neurons in the UNet3+ structure, the segmentation of small targets is not ideal\\nfor the training with a small number of samples. Inspired by these two networks, we propose a\\nnovel structure of UNet_like shown in Figure 1 (d), which is very similar to the grid structure of\\n# symbols, named UNet-sharp (UNet#). UNet# has the advantages of UNet2+ and UNet3+, which\\ncan aggregate feature maps of different scales in the encoder sub-network and capture ﬁne-grained\\ndetails incorporating coarse-grained semantics from the full-scale feature map.\\nIn addition, we add the loss function to the four decoder branch outputs, comparing with the post-\\ndownscale ground truth to supervise the model training, which further learn the hierarchical repre-\\nsentation from the full-scale aggregation feature map. Moreover, introducing deep supervision to\\nthe other four middle layers and the ﬁnal decoder branch with the same scale as the input image\\ncan realize the pruning function of the model. To suppress the false-positive problem further, we\\ndevelop a classiﬁcation-guided module (CGM) and apply it to all eight branch outputs to promote a\\nmore accurate segmentation for lesions and organs in medical images.\\nContributions. Our contributions are as follows:\\n1. Proposing A novel medical image segmentation structure UNet#.\\n2. Adding deep supervision into the decoder branches’ output to further capture coarse-grained\\nand ﬁne-grained semantic information from the full scale, and introducing deep supervision\\ninto the ﬁrst layer nodes (including the ﬁnal output node) can guide the model pruning.\\n3. Designing two classiﬁcation-guided modules for different output branches to achieve higher\\nprecision organ and lesion segmentation.\\n2\\nFigure 1: The model architecture comparison between UNet(a), UNet2+(b), UNet3+(c) and UNet#(d). The\\nelements of node matrix(e) composed of the UNet#’s units shown as (d) are expressed by Eq. 1, easily com-\\nprehending the calculation method of the proposed UNet#. The number of feature channels from each stage in\\nbackbone is 32, 64, 128, 256 and 512. The implementation of deep supervision in UNet# is different from that\\nin UNet2+ and UNet3+, which is detailed in Sec. 2.2.\\n4. Carrying out extensive experiments on the different data sets to verify the consistent improve-\\nment of the method proposed in this paper from multiple aspects.\\n2\\nMethods\\nThe redesigned nested and dense skip connections for UNet2+ make full use of the semantics from\\nthe feature maps at different layers. In UNet3+, the proposed full-scale skip connections can obtain\\nenough information from the full-scale feature map. The network we designed combines the dense\\nskip connection and full-scale skip connection to learn enough full-scale semantics while incorpo-\\nrating low-level details so that the model can yield more accurate segmentation results.\\n2.1\\nRe-designed skip connections\\nSkip connections (Wu et al., 2020) play an inﬂuential role in improving the model’s performance in\\nthe deep neural network. It can connect the shallow layer with the deep layer to retain the low-level\\nfeatures, avoiding the model performance degradation when adding multiple layers. We redesign\\nthe skip connections in the proposed network, interconnecting the features of each level encoder ex-\\ntracted through a dense revolution block, besides intraconnecting the deep and shallow features via\\nfull-scale skip connections. Furthermore, we also perform intraconnection through full-scale skip\\nconnections for decoders at different levels. Compared with the state-of-the-art network models\\n(UNet, UNet2+, UNet3+), this redesigned skip connection method has a stronger full-scale infor-\\nmation exploration ability and receptive ﬁelds of diverse sizes, which can realize high-precision\\nsegmentation organs with different sizes.\\nThe proposed model structure is shown in Figure 1 (d). As a UNet-like network, our model is sim-\\nilar to the UNet2+ design, adding six intermediate units, but the skip connections are redesigned.\\nTo better understand the model structure, a matrix, as shown in ﬁgure 1 (e), composing 5-row and\\n5-columns cell nodes is used to abstractly represent the model structure units. The ﬁrst column\\nnodes of the matrix are the model’s encoder, marked as Xi,0\\nEn, i ∈[0, 1, 2, 3, 4]. The calculation for\\nthe second column unit module is conducted as follows: ﬁrstly, upsamples the encoder feature of\\nthe deeper level with a two-scale factor; secondly, concatenates with the encoder of the same level\\n3\\nFigure 2: The schematic diagram of deep supervision in the proposed UNet#. Deep supervision in our model\\nhas tow functions: (a) model pruning and (b) improving the learning skill of hierarchical representations.\\nAdding loss function to the branches followed by 1 × 1 or 3 × 3 convolution operation produces the loss\\nLi, i ∈(1 ∼4). LIn is used for supervision of model pruning, and LDe is used to supervise the improvement\\nof the feature learning for the model. Most notably, the implementation methods of these two deep supervision\\n(a) and (b) are not the same. The ground truth (GT) is from BraTs19 dataset (brain tumor). The MaxPool2d\\noperation is used for down sampling (d x2, d x4, d x8, d x16).\\non the channel, and ﬁnally outputs the ﬁnal result of the unit using convolution operation. For in-\\nstance, the encoder unit X0,1\\nIn is obtained by the convolution calculation after operating the channel\\nconcatenation between the same level layer encoder X0,0\\nEn and the 2-times upsampling of the deeper\\nlayer encoder X1,0\\nEn. The other column nodes of the matrix are calculated from two aspects: on\\nthe one hand, the feature information of the same level is thoroughly obtained through dense skip\\nconnections (interconnections). On the other hand, a chain of intra-decoder skip connections (in-\\ntraconnections) transmits the high-level semantic information from the smaller-scale decoder layer.\\nFor example, the encoder X0,2\\nIn is calculated via the convolution operation following the channel\\nconcatenation of the four feature maps consisting of dense skip interconnections from X0,0\\nEn, X0,1\\nIn ,\\nfull-scale skip intraconnection from X2,0\\nEn, and the two-scale factor upsampling of X1,1\\nIn . The ﬁ-\\nnal output of the model, decoder X0,4\\nDe, is achieved by the convolution operation and the channel\\nconcatenation of the eight feature maps composed of dense skip interconnections from X0,0\\nEn, X0,1\\nIn ,\\nX0,2\\nIn , X0,3\\nIn full-scale skip intraconnections from X4,0\\nEn, X3,1\\nDe, X2,2\\nDe and the two-scale factor upsam-\\npling of X1,3\\nDe. The redesigned skip connections make the network more similar between encoder\\nand decoder features at the semantic level than UNet2+ and UNet3+. This similarity can make the\\noptimizer easier to optimize during computation, but also make the model more capable of full-scale\\nfeature information exploration from the full-scale aggregated feature maps.\\nThe calculation of each model unit in the matrix is expressed as Eq. 1.\\nAI,J =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f3\\nf 2 \\x00p\\n\\x00AI,0\\x01\\x01\\n,\\nJ = 0\\nf 2 \\x00\\x02\\nAI,0, u\\n\\x00AI+1,0\\x01\\x03\\x01\\n,\\nJ = 1\\nf 2 \\x10h\\x02\\nAI,j\\x03J−1\\nj=0 , u\\n\\x00AI+1,J−1\\x01\\n,\\n\\x02\\nf\\n\\x00uJ−j \\x00Ai,j\\x01\\x01\\x03I+2,J−2\\ni=I+J,j=0\\ni\\x11\\n,\\nJ > 1\\n(1)\\nWhere AI,J (I = [0, 1, 2, 3, 4]) represents the calculation results of each model unit. When J = 0,\\nthe ﬁrst sub-formula deﬁnes the calculation method for the ﬁrst column of node matrix, that is, the\\ncalculation method of the model encoder Xi,0\\nEn, i ∈[0, 1, 2, 3, 4]. When J = 1, the second sub-\\nformula is the calculation method of the second column of the matrix. When J > 1 (J ∈[2, 3, 4]),\\nthe third sub-formula is the calculation method of the matrix’s 3, 4, and 5 columns. P(·) indicates\\n2 × 2 Maxpool2d operation meaning downsampling. u(·) means to upsample on 2×, and un(·)\\n4\\nmeans 2n times upsampling. [·] denotes that the feature maps of each unit are concatenated on the\\nchannel; f(·) represents 1-time sequential operation including Conv2d, BatchNorm2d, and ReLU\\nactivation function, f n(·) indicates n-times sequential operations.\\n2.2\\nDeep supervision\\nAs a training trick, deep supervision was proposed in DSN (Deeply-Supervised Nets) (Lee et al.,\\n2015) in 2014, aiming to get more totally training in shallow layers, which can avoid gradient disap-\\npearance and slow convergence. Compared with the conventional deep learning mechanism, deep\\nsupervision outputs the result out at the terminal branch of the network while obtaining the result\\nout_m with the same size after operating deconvolution or upsampling on the middle feature map\\nof the network, sequentially, combining out_m and out to train the network jointly. According to\\nthis idea, deep supervision applied in UNet2+ and UNet3+ achieve better results than the original\\nstructure.\\nWe introduce deep supervision into the proposed model structure to make it operate in two modes\\n(Zhou et al., 2018): 1) accurate mode, wherein the branch output of the model will be averaged to\\ncalculate loss; 2) fast mode, which selects the branch with the best result, actually prunes the model\\n(see Sec. 2.4 for details) to improve the speed. As shown in Figure 2, we add the loss function to\\nthe eight branches’ output of the proposed UNet#. Speciﬁcally, implementation is as follows. The\\nﬁrst layer feature maps X0,j\\nIn , j ∈[1, 2, 3] and X0,4\\nDe generated by dense skip interconnections and\\nfull-scale skip intraconnections at multiple semantic levels, are followed by the operation of 1 × 1\\nconvolution and ReLU activation function, producing the output results. Then comparing the results\\nwith GT calculates the loss LIn used to supervise the model for pruning. Additionally, the decoder\\nX1,3\\nDe, X2,2\\nDe, X3,1\\nDe, X4,0\\nEn branches followed by a 3×3 convolution and ReLU activation function bring\\nthe output results. Subsequently, respectively comparing with the GT of the corresponding size after\\ndown-scaling (consistent with the size of each output result) calculates the loss value LDe, realizing\\nthe supervision of the model to the improvement of skills for learning hierarchical representations\\nfrom full-scale feature maps.\\nHere, we use a mixed loss to calculate the loss value of each branch, which is composed of fo-\\ncal loss (Lin et al., 2017), Laplace smoothing dice loss, and Lovasz hinge loss (Yu and Blaschko,\\n2018). Mixing loss can bring smooth gradient and handling of class imbalance, which can capture\\nboth large-scale and delicate structures with clear boundaries. The mixed segmentation loss ℓseg is\\ndeﬁned as follows:\\nℓseg = 0.5 ∗ℓfl + ℓlsd + 0.5 ∗ℓlh\\n(2)\\nWhere ℓfl is focal loss, deﬁned as follows:\\nℓfl = −α(1 −ˆY )β log( ˆY )\\n(3)\\nWhere α and β are parameters, α can control the shared weight of positive and negative samples to\\nthe total loss, β controls the weight of easy classify and difﬁcult classify samples, and Y represents\\nthe predicted samples. In the experiment, we specify α = 0.25 and β = 2 according to the setting\\nwith the best experimental effect in [4].\\nℓlsd from Eq. 2 is the soft Dice coefﬁcient loss after Laplace smoothing, which can avoid the\\nproblem of division by 0 and overﬁtting, deﬁned as follows:\\nℓlsd = 1 −2 · Y · ˆY + 1\\nY + ˆY + 1\\n(4)\\nWhere Y is the label, ˆY is the predicted sample.\\nℓlh from Eq. 2 is Lovasz hinge loss, deﬁned as follows:\\nℓlh = (1 −Y · ˆY )+\\n(5)\\n5\\nFigure 3: The principle of calculation for classiﬁcation-guided module. The ﬁrst layer nodes X0,j\\nIn , j ∈[1, 2, 3]\\napply CGM in the same way as X0,4\\nDe, , so it is not drawn separately in the picture. The channel number of each\\nnode is displayed below the cuboids.\\nWhere Y is the label, ˆY is the predicted sample, (·)+ = max(·, 0).\\nSo mathematically, the hybrid segmentation loss can eventually be expressed as Eq. 6:\\nℓseg(Y, ˆY ) = −1\\nN\\nN\\nX\\nn=1\\n\\x10\\n0.5 ∗\\n\\x12\\n−α\\n\\x10\\n1 −ˆYn\\n\\x11β\\nlog\\n\\x10\\nˆYn\\n\\x11\\x13\\n+\\n2 · Yn · ˆYn + 1\\nYn + ˆYn + 1\\n+ 0.5 ∗\\n\\x10\\nYn · ˆYn\\n\\x11\\n+\\n\\x11\\n(6)\\nWhere Yn ∈Y , ˆYn ∈ˆY , respectively represents the n−th tiling GT picture and the tiling prediction\\nprobability picture, N represents the batch size.\\n2.3\\nClassiﬁcation-guided module\\nIn the process of deep supervision calculation, some noisy information from the shallow layer back-\\nground will lead to false positives, which will lead to over-segmentation, reducing the segmentation\\naccuracy in the image segmentation task. Therefore, we refer to Huang et al. (2020) and propose\\na classiﬁcation-guided module (CGM) applied in deep supervision. This module can classify the\\ninput image and judge whether it includes the organs or tissues to be segmented, avoiding the wrong\\nsegmentation caused by false positives to a certain extent, and improving the segmentation accuracy.\\nDifferent from the CGM structure proposed in UNet3+ (Huang et al., 2020), we propose two dif-\\nferent CGM structures and apply them to the eight deep supervision branches of the model. As\\ndepicted in Figure 3, the green CGM structure is proposed to be applied in the deep supervision\\nbranches X4,0\\nEn and X3,1\\nDe. Speciﬁcally, ﬁrstly, concatenates the global semantic features obtained\\nthrough AdaptiveAvgPool2d and AdaptiveMaxPool2d operations. Then ﬂattens the feature map by\\nFlatten, followed by the sequential operation including BatchNorm1d, Dropout, Linear, ReLU, and\\none more time of the sequential operation, will get a 2-channel feature map. Finally, using a Soft-\\nmax activation function generates a 2-dimensional tensor indicating the possibility of whether the\\ninput image has organs, which realizes the classiﬁcation that is optimized by the BCE loss function.\\nApplying the Argmax function yields the ﬁnal output result composing of 0,1, which is multiplied\\nby the deep supervision branches, and ﬁnally gets the branch results after CGM operation. These\\nresults rectify some defects of over-segmentation caused by false positives. Considering that the\\nchannel number of X2,2\\nDe, X1,3\\nDe, X0,4\\nDe, and X0,j\\nIn , j ∈[1, 2, 3] six branch feature maps are relatively\\nsmall, we simplify the CGM structure shown with blue in Figure 3. It includes AdaptiveAvgPool2d,\\nAdaptiveMaxPool2d, Flatten, BatchNorm1d, Dropout, Linear, Softmax, Argmax operations.\\n2.4\\nModel pruning\\nThe introduction of deep supervision in our proposed network model makes the model have two\\noperation modes (see Sec. 2.2), in which the fast mode can realize the model pruning function. As\\nshown in Figure 4, the four pruning levels are signiﬁed as Li, i ∈[1, 2, 3, 4]. During the training\\n6\\nFigure 4: Illustration for the different levels of the proposed UNet# pruning. Training the model with deep\\nsupervision prunes the model to L4, L3, L2 and L1 at the inference time. Model pruning L4(a) indicates no\\npruning, whereas Model pruning L1(d) represents the maximum pruning level, and its structure is the same\\nas that of UNet2+ pruning of the same level. The calculation method of loss value L in each pruning level is\\nsimilar, which takes the average loss value of all branches in the training time. See Figure 2 and Sec. 2.2 for\\nspeciﬁc diagrams and calculation process.\\nphase of the model, the weight calculation includes both forward propagation and back-propagation,\\nwhile in the test phase, the weight calculation only includes forward propagation calculation. Bene-\\nﬁting from the added deep supervision, the pruned branches of the model will have an impact on the\\nremaining results due to back-propagation in the training stage; differently, they will not impact the\\noutput results in the inference stage. In other words, the L4 level model participates training in the\\ntraining phase, whose branches to be pruned contribute to the weight back-propagation. Meanwhile,\\nin the inference time, according to the validation results, the model is pruned into the L1, L2, or\\nL3 level to reduce the network parameters, improving the network speed (Pruning L4, a full model\\nmeans no pruning in the inference).\\n3\\nExperiments and results\\n3.1\\nDatasets\\nTo evaluate the performance of the proposed model more comprehensively, we consider using\\ndatasets with different modalities and organs, speciﬁcally including Dsb2018 (Nuclei segmentation)\\n, BraTs19 (Brain tumor segmentation), Lits17 (Liver segmentation) , LIDC-IDRI (Lung 2D segmen-\\ntation), Luna16 (Lung 3D segmentation), wherein the ﬁrst four datasets are used for the training and\\nvalidation of the 2D network model and Luna16 dataset is used for the 3D network model. All these\\ndatasets will be separately divided into training set, validation set, and test set in the experiment.\\n3.2\\nEvaluation metrics\\nIn our experiments, Dice-Coefﬁcient (DC) and Intersection over Union (IoU) are used as the evalua-\\ntion metrics of model performance. DC is a set similarity measurement function that calculates the\\npixel-wise similarity between the predicted image and the GT. IoU is a standard for measuring the\\naccuracy of detecting corresponding objects in a speciﬁc dataset, representing the pixel-wise overlap\\nrate or degree of the prediction and GT, namely, their intersection to union ratio.\\n3.3\\nImplementation details\\nWe preprocess the datasets with RandomRotate90, Flip, a random selection (HSV color space trans-\\nformation, brightness contrast change), and Normalization to train the models. Adam optimization\\nfunction with weight_decay 1e-4 and small batch sizes of 128 samples to calculate gradient updates\\nin training. The network’s weights are initialized by He et al. (2015), trained for 100 passes using\\nthe learning rate (initialized to 1e-3) adjusted by CosineAnnealing (Loshchilov and Hutter, 2016).\\nWe code the experiments on the PyTorch open-source framework and log the experiment data by\\nthe open-source tool Wandb. Both training and validation of all models are parallel conducted on\\ntwo NVIDIA Tesla A100 GPU with 80GB memory in a 24-core, 48 threads server equipped with an\\nIntel Xeon Silver 4214R 2.4GHz CPU(128GB RAM).\\n7\\nmodel\\nDS Params\\n2D segmentation\\nmodel\\nDS Params 3D segmentation\\nDsb2018 BraTs19 Lits17 LIDC-IDRI\\nLuna16\\nUNet\\n×\\n7.85M\\n90.15\\n89.96\\n91.43\\n72.59\\nV-Net\\n× 26.39M\\n71.21\\nWUNet\\n×\\n9.39M\\n90.27\\n90.06\\n91.52\\n72.93\\nWV-Net\\n× 33.40M\\n73.52\\nUNet2+ ×\\n9.16M\\n92.02\\n90.03\\n91.59\\n72.73\\nV-Net2+ × 27.37M\\n76.35\\nUNet2+ ✓\\n9.16M\\n91.98\\n91.01\\n91.64\\n73.43\\nV-Net2+ ✓27.37M\\n77.62\\nUNet3+ ×\\n9.79M\\n91.96\\n90.67\\n91.21\\n73.22\\nV-Net3+ × 27.84M\\n76.78\\nUNet3+ ✓\\n9.79M\\n92.15\\n91.69\\n92.14\\n73.48\\nV-Net3+ ✓27.84M\\n78.23\\nUNet#\\n×\\n9.71M\\n92.53\\n91.67\\n94.79\\n73.68\\nV-Net#\\n× 28.37M\\n77.69\\nUNet#\\n✓\\n9.71M\\n92.67\\n92.38\\n95.36\\n74.01\\nV-Net#\\n✓28.37M\\n79.45\\nTable 1: Evaluation metric IoU (%) of semantic segmentation results for UNet, WUNet, UNet2+, UNet3+, and\\nthe proposed UNet# on nuclei, brain tumor, liver, 2D lung node, and 3D lung node. The model backbone is set\\nas VGG11. DS is the abbreviation of deep supervision, × indicates without DS, and ✓means with DS. Among\\nall the evaluation results, the best result is shown in bold.\\nFigure 5: The boxplots of validation results for UNet, WUNet, UNet2+, UNet3+, and the proposed UNet#\\non 2D datasets Dsb2018(a), BraTs19(b), Lits17(c), and LIDC-IDRI(d). The IoU boxplots show the scores of\\nthe quartile ranges, whiskers and dots indicate outliers on the validation datasets. UNet2+_DS, UNet3+_DS,\\nUNet#_DS indicates the corresponding model with deep supervision.\\n3.4\\nResults\\nThe experimental results of all models are obtained by running in a fair and unbiased environment,\\nwhich are visually presented in pictures and tables.\\n3.4.1\\nComparison on semantic segmentation\\nThis section compares the semantic segmentation performance of the proposed UNet# with UNet,\\nwide-UNet (WUNet), UNet2+, and UNet3+ on the datasets. We analyze the experiment results from\\nboth qualitative and quantitative aspects.\\nQuantitative analysis: Table 1 detailly summarizes the parameters of each model with VGG11 as\\nthe backbone and their IoU results on the ﬁve datasets. The number of parameters for WUNet is\\n9.39M, reaching 1.54M more than UNet. Beneﬁtting from more parameters, WUNet gets more sig-\\nniﬁcant performance on each dataset than UNet. However, unexpectedly, this superiority does not\\nwork compared with UNet2+ and UNet3+. This situation shows that simply raising channels for\\nthe performance improvement of the model is not better than optimizing the model structure. The\\n8\\nFigure 6: Qualitative comparison of 2d segmentation results between UNet, WUNet, UNet2+, UNet3+, and\\nUNet# on Dsb2018, BraTs19, Lits17, and LIDC-IDRI datasets. The last image of segmentation result from the\\nﬁrst line, in which the red circle is added in anaphase, clearly indicates the place where the segmentation result\\nis better.\\nproposed UNet# has 9.71M parameters, 1.68M more than UNet, and 0.32M more than WUNet. Nev-\\nertheless, unlike WUNet, UNet# is optimized and improved at the model structure by redesigning\\nthe skip connections, resulting in heavier parameters but better segmentation metrics. In the nuclei\\ndataset (Dsb2018), UNet# respectively achieves an IoU gain of 0.51 and 0.57 points over UNet2+\\nand UNet3+. Similarly, it performs a satisﬁed IoU gain over UNet2+ and UNet3+ across all the\\nother four tasks of brain tumor (↑1.64, ↑1.0), liver (↑3.2, ↑3.58), 2D lung nodule (↑0.95, ↑0.46)\\nsegmentation. Adding deep supervision to UNet# further performs a distinct effect on all datasets.\\nFigure 5 shows the validation IoU score of each model on the four 2D datasets during 100 pass\\ntraining. It can be seen from the ﬁgure that the proposed model and the model with deep supervision\\nhave better segmentation performance for organs and lesions than other models, wherein Figure 5\\n(c) has particularly obvious advantages in liver segmentation.\\nFurthermore, we compare the IoU results of the V-Net (Milletari et al., 2016) model applied ++\\n(Zhou et al., 2019), +++ (Huang et al., 2020), and # (our method) schemes on the 3D lung dataset.\\nThe feature list of V-Net is [16,32,64,128,256], which is slightly expands to [18,36,72,144,288],\\ngetting wide V-Net (WV-Net) whose parameters are 33.4M. The heavier parameter cannot bring\\nthe best result, indicating again that simply adding features may inhibit the learning ability of the\\nmodel. V-Net# achieving the best IoU of 77.69% (with DS 79.45%) shows that integrating dense\\nskip connections and full-scale skip connections can promote the model performance at a small\\nincreased cost.\\nQualitative analysis: Figure 6 exhibits the segmentation results of the models in each dataset. The\\nﬁrst column is the original image for testing, followed by ground truth (GT), and the third to seventh\\ncolumns are the test results for UNet, WUNet, UNet2+, UNet3+, and the proposed UNet#. As shown\\nby the red circle in the last column of the ﬁrst row, testing in the Dsb2018 dataset, the segmentation\\nresults obtained by UNet# are closer to the GT than other models. Similarly, it is noticeable that\\nUNet achieves consistent high-performance results in the localization and segmentation of organs\\nand lesions on other datasets.\\n3.4.2\\nComparison on instance segmentation\\nIn addition to segmenting the boundary of each instance on the pixel, instance segmentation also\\nneeds to frame different object instances with the target detection method, so it is more challenging\\n9\\nmodel\\nBackbone\\nIoU\\nDice\\nBackbone\\nIoU\\nDice\\nUNet\\nVGG16\\n90.12\\n86.94\\nResNet101\\n91.52\\n89.43\\nUNet#\\nVGG16\\n92.82\\n89.60\\nResNet101\\n93.56\\n90.78\\nMask R-CNN\\nVGG16\\n92.34\\n87.68\\nResNet101\\n93.45\\n89.34\\nMask R-CNN2+†\\nVGG16\\n93.47\\n88.14\\nResNet101\\n94.96\\n90.16\\nMask R-CNN3+ †\\nVGG16\\n94.28\\n90.51\\nResNet101\\n95.26\\n91.38\\nMask R-CNN# †\\nVGG16\\n94.89\\n91.37\\nResNet101\\n95.87\\n92.25\\nTable 2: Comparison of IoU (%) and Dice (%) for the semantic and instance segmentation tasks. The upper part\\nof the table is the validation results of nuclear semantic segmentation for UNet and UNet# on Dsb2018 dataset;\\nThe lower part is the validation results of nuclear instance segmentation for Mask R-CNN (He et al., 2017),\\nMask R-CNN2+, Mask R-CNN3+ and Mask R-CNN# on Dsb2018. † Mask R-CNN with ++ (Zhou et al.,\\n2019), +++ (Huang et al., 2020), and # (our method) schemes design in its feature pyramid. The best results\\nare shown as bold\\nthan semantic segmentation. Since the images in the Dsb2018 dataset contain multiple nuclei of\\ncells, suitable for the challenge of instance segmentation, this section details the instance segmenta-\\ntion results of different methods on the Dsb2018.\\nIn this experiment, to illustrate that the proposed method is a backbone-agnostic extension to UNet,\\nwe perform the models with VGG16 and ResNet101 as the backbone. Mask R-CNN (He et al.,\\n2017) is a classic model in instance segmentation, including feature pyramid network (FPN) struc-\\ntural blocks, which can generate target recommendations on multiple scales. Taking it as the bench-\\nmark network [6], we redesign the plain skip connections of FPN with ++ (Zhou et al., 2019), +++\\n(Huang et al., 2020), and # (our method) schemes, generating Mask R-CNN2+, Mask R-CNN3+,\\nMask R-CNN# for the instance segmentation experiment in this section.\\nThe second and third rows of Table 2 compare semantic segmentation experimental results of UNet\\nand UNet# on the Dsb2018 dataset. Based on the backbone of VGG16, the proposed UNet# achieves\\n92.82% IoU and 89.60% Dice, which are respectively enhanced by 2.7 points (vs. 90.12%) and 2.66\\npoints (vs. 86.94%) compared with UNet; Furthermore, the experimental results based on resnet101\\nbackbone demonstrate that our method achieves consistent performance. The fourth to seventh rows\\nof Table 2 compares the experimental results of the instance segmentation on Dsb2018. It can be\\nseen that the Mask R-CNN of our scheme (Mask R-CNN#) has achieved the best segmentation\\nresults. In conclusion, as expected, our method is a backbone-agnostic optimization scheme, outper-\\nforming UNet, ++, +++ method in semantic segmentation and instance segmentation.\\n3.4.3\\nComparison on model pruning\\nThe proposed model designs two types of deep supervision for branch pruning. One is adding a\\nloss function to the output of four decoder branches for deep supervision to learn the hierarchical\\nrepresentation from the full-scale aggregation feature map. The other is performing deep supervision\\non the four branches at the ﬁrst level layer, whose scale is the same as the input image size, to realize\\nthe branch pruning.\\nActually, there are two ways to train the model in implementing model pruning. The ﬁrst method\\nis called embedded training, speciﬁcally comprehended as training a high pruning-level UNet# and\\nthen pruning the model to get a low pruning-level model. For example, the model calculates X0,1\\nIn ,\\nX0,2\\nIn , X0,3\\nin and X0,4\\nde node loss, supervising training the full UNet# (equivalent to pruning-level L4),\\nand then pruning at level i to get UNet# Li, i ∈[1, 2, 3]; Or for another example, the model calculates\\nX0,1\\nIn , X0,2\\nIn , X0,3\\nin node loss supervising training UNet# L3, and then pruning model to obtain UNet#\\nLi, i ∈[1, 2]. Embedded training can be regarded as integrating sub-networks training at different\\ndepths. The second method is isolated training, speciﬁcally understood as training the pruning Li\\nmodel independently and using it for inference. For example, in the training phase, UNet# pruning\\nL3 is trained without interactions with the deeper encoder and decoder nodes from L4 and is used\\nfor inference without further pruning. Comparison of this two training methods are detailed in ??.\\nThe experimental results in this section are performed with the ﬁrst training method.\\n10\\nFigure 7: Illustration of different model pruning level. Model complexity, inference speed represented by\\nFrame Per Second (FPS), and inference IoU (%) of UNet# after pruning on Dsb2018(a), BraTs19(b), Lits17(c),\\nLIDC-IDRI(d) datsets. The inference speed of the experiments is carried out in the same hardware enviroment\\ndetailed in Sec. 3.3.\\nFigure 7 shows the test results of four different levels of pruning models on four various datasets.\\nAs seen, branch pruning generates a wide margin in the number of model parameters at different\\nlevels; speciﬁcally, L1 is 0.1M, 5 times less than L2 (vs. 0.56m), 25 times less than L3 (vs. 2.53m),\\nand 97 times less than L4 (vs. 9.71m). L1 parameters are the least but with the worst results,\\ndemonstrating that a too shallow model can not achieve satisfactory results. Nevertheless, more\\nimportantly, considering the expensive calculation cost and memory intensive of the existing deep\\nconvolution neural networks, transplanting lighter pruned models to mobile devices can promote the\\ndaily application and comprehensive development of computer-aided diagnosis (CAD).\\n3.4.4\\nComparison with the State-of-the-Art\\nWe make a quantitative comparison between the proposed method and other state-of-the-art methods\\nincluding PSPNet (Zhao et al., 2017), DeepLabV3+ (Chen et al., 2018), ENet(Paszke et al., 2016),\\nnnU-Net (Isensee et al., 2018), AttnUNet (Schlemper et al., 2019), DUNet (Jin et al., 2019), Ra-\\nUNet (Jin et al., 2020), and TransUNet (Chen et al., 2021) on BraTs19 and Lits17 datasets according\\nto the fair comparison of super parameters and training environmental conﬁguration.\\nmodel\\nBraTs19\\nLits17\\nIoU\\nDice\\nIoU\\nDice\\nPSPNet (Zhao et al., 2017)\\n83.85\\n91.01\\n87.75\\n93.23\\nDeepLabV3+ (Chen et al., 2018)\\n87.1\\n92.9\\n91.07\\n95.17\\nEnet (Paszke et al., 2016)\\n81.62\\n89.9\\n83.36\\n90.9\\nnnU-Net (Isensee et al., 2018)\\n88.52\\n91.68\\n91.31\\n95.3\\nAttnUNet (Schlemper et al., 2019)\\n91.32\\n95.32\\n94.61\\n97.15\\nDUNet (Jin et al., 2019)\\n90.86\\n95.19\\n94.47\\n97.21\\nRa-Unet (Jin et al., 2020)\\n84.28\\n89.12\\n91.94\\n96.1\\nTransUNet (Chen et al., 2021)\\n88.85\\n94.04\\n92.59\\n96.16\\nUNet#\\n91.68\\n95.6\\n94.79\\n97.29\\nUNet#(CGM)\\n92.08\\n96.14\\n95.31\\n98.24\\nTable 3: Comparison of UNet# and other 8 state-of-the-art models on BraTs19 and Lits17 datasets. UNet#\\nwith calssiﬁcation-guided module (CGM) achieves the best results highlighted in bold.\\nAs seen from Table 3, Our method earns better IoU (91.68%, 94.79%) and Dice (95.60%, 97.29%)\\nmetrics than other methods in the two datasets. Embedding the CGM module can achieve 0.4, 0.52\\ngains on IoU, and 0.54, 0.95 gains on Dice. These demonstrate that the mean of improving UNet\\nin this paper is feasible, and the improvement of skip-connections is signiﬁcant. Meanwhile, it also\\nconﬁrms that compared with Separable pyramid pooling methods (Chen et al., 2018; Zhao et al.,\\n2017), NLP Transformers combination UNet (Chen et al., 2021), Attention UNet (Jin et al., 2020;\\nSchlemper et al., 2019), and deformable convolution combining with UNet (Jin et al., 2019), the\\nproposed UNet# is sophisticated in medical image segmentation.\\n11\\n4\\nConclusion\\nInspired by the successful improvement of skip connections in UNet2+ and UNet3+, this paper\\nproposes a new model structure named UNet-sharp (UNet#). The proposed model aggregates the\\nfeatures of different semantic scales on the encoder and decoder subnet through nested and dense\\nskip connections. Furthermore, it utilizes full-scale connections to maximize the fusion of high-\\nlevel semantics and low-level details in multi-scale feature maps to realize accurate organ or lesion\\nlocation perception and segmentation. Introducing deep supervision into the eight branches can\\nmake the model have a pruning function and learn the hierarchical representation from the full-scale\\naggregation feature map. Additionally, two kinds of CGM are designed for the model to avoid over-\\nsegmentation. Experiments from semantic and instance segmentation on different organ and pattern\\ndatasets demonstrate that the proposed model outperforms all the state-of-the-art methods.\\nAcknowledgments\\nThis work was supported in part by Wenzhou Association For Science and Technology under [grant\\nno. kjfw39], in part by the Major Project of Wenzhou Natural Science Foundation under [grant\\nno. ZY2019020], in part by the Project of Wenzhou Key Laboratory Foundation under [grant no.\\n2021HZSY0071], in part by the Department of Education of Zhejiang Province under [grant no.\\nY202146494], in part by the Soft Science Key Research Project of Zhejiang Province under [grant\\nno. 2022C25033] and in part by the Key Project of Zhejiang Provincial Natural Science Foundation\\nunder [grant no. LD21F020001].\\n12\\nReferences\\nJieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan L Yuille,\\nand Yuyin Zhou. Transunet: Transformers make strong encoders for medical image segmentation.\\narXiv preprint arXiv:2102.04306, 2021.\\nLiang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.\\nDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and\\nfully connected crfs. IEEE transactions on pattern analysis and machine intelligence, 40(4):\\n834–848, 2017.\\nLiang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-\\ndecoder with atrous separable convolution for semantic image segmentation. In Proceedings of\\nthe European conference on computer vision (ECCV), pages 801–818, 2018.\\nKunio Doi. Computer-aided diagnosis in medical imaging: historical review, current status and\\nfuture potential. Computerized medical imaging and graphics, 31(4-5):198–211, 2007.\\nMichal Drozdzal, Eugene Vorontsov, Gabriel Chartrand, Samuel Kadoury, and Chris Pal. The impor-\\ntance of skip connections in biomedical image segmentation. In Deep learning and data labeling\\nfor medical applications, pages 179–187. Springer, 2016.\\nGusztáv Gaál, Balázs Maga, and András Lukács. Attention u-net based adversarial architectures for\\nchest x-ray lung segmentation. arXiv preprint arXiv:2003.10304, 2020.\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing\\nhuman-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international\\nconference on computer vision, pages 1026–1034, 2015.\\nKaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of the\\nIEEE international conference on computer vision, pages 2961–2969, 2017.\\nHuimin Huang, Lanfen Lin, Ruofeng Tong, Hongjie Hu, Qiaowei Zhang, Yutaro Iwamoto, Xianhua\\nHan, Yen-Wei Chen, and Jian Wu. Unet 3+: A full-scale connected unet for medical image\\nsegmentation. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and\\nSignal Processing (ICASSP), pages 1055–1059. IEEE, 2020.\\nFabian Isensee, Jens Petersen, Andre Klein, David Zimmerer, Paul F Jaeger, Simon Kohl, Jakob\\nWasserthal, Gregor Koehler, Tobias Norajitra, Sebastian Wirkert, et al. nnu-net: Self-adapting\\nframework for u-net-based medical image segmentation. arXiv preprint arXiv:1809.10486, 2018.\\nQiangguo Jin, Zhaopeng Meng, Tuan D Pham, Qi Chen, Leyi Wei, and Ran Su. Dunet: A de-\\nformable network for retinal vessel segmentation.\\nKnowledge-Based Systems, 178:149–162,\\n2019.\\nQiangguo Jin, Zhaopeng Meng, Changming Sun, Hui Cui, and Ran Su. Ra-unet: A hybrid deep\\nattention-aware network to extract liver and tumor in ct scans. Frontiers in Bioengineering and\\nBiotechnology, page 1471, 2020.\\nChen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu.\\nDeeply-\\nsupervised nets. In Artiﬁcial intelligence and statistics, pages 562–570. PMLR, 2015.\\nXiaomeng Li, Hao Chen, Xiaojuan Qi, Qi Dou, Chi-Wing Fu, and Pheng-Ann Heng. H-denseunet:\\nhybrid densely connected unet for liver and tumor segmentation from ct volumes. IEEE transac-\\ntions on medical imaging, 37(12):2663–2674, 2018.\\nTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense\\nobject detection. In Proceedings of the IEEE international conference on computer vision, pages\\n2980–2988, 2017.\\nIlya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv\\npreprint arXiv:1608.03983, 2016.\\n13\\nFausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural net-\\nworks for volumetric medical image segmentation. In 2016 fourth international conference on\\n3D vision (3DV), pages 565–571. IEEE, 2016.\\nAlireza Norouzi, Mohd Shafry Mohd Rahim, Ayman Altameem, Tanzila Saba, Abdolvahab Ehsani\\nRad, Amjad Rehman, and Mueen Uddin. Medical image segmentation methods, algorithms, and\\napplications. IETE Technical Review, 31(3):199–213, 2014.\\nOzan Oktay, Jo Schlemper, Loic Le Folgoc, Matthew Lee, Mattias Heinrich, Kazunari Misawa,\\nKensaku Mori, Steven McDonagh, Nils Y Hammerla, Bernhard Kainz, et al. Attention u-net:\\nLearning where to look for the pancreas. arXiv preprint arXiv:1804.03999, 2018.\\nAdam Paszke, Abhishek Chaurasia, Sangpil Kim, and Eugenio Culurciello. Enet: A deep neural net-\\nwork architecture for real-time semantic segmentation. arXiv preprint arXiv:1606.02147, 2016.\\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedi-\\ncal image segmentation. In International Conference on Medical image computing and computer-\\nassisted intervention, pages 234–241. Springer, 2015.\\nJo Schlemper, Ozan Oktay, Michiel Schaap, Mattias Heinrich, Bernhard Kainz, Ben Glocker, and\\nDaniel Rueckert. Attention gated networks: Learning to leverage salient regions in medical im-\\nages. Medical image analysis, 53:197–207, 2019.\\nDongxian Wu, Yisen Wang, Shu-Tao Xia, James Bailey, and Xingjun Ma.\\nSkip connections\\nmatter: On the transferability of adversarial examples generated with resnets. arXiv preprint\\narXiv:2002.05990, 2020.\\nJiaqian Yu and Matthew B Blaschko. The lovász hinge: A novel convex surrogate for submodular\\nlosses. IEEE transactions on pattern analysis and machine intelligence, 42(3):735–748, 2018.\\nHengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing\\nnetwork. In Proceedings of the IEEE conference on computer vision and pattern recognition,\\npages 2881–2890, 2017.\\nZongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, and Jianming Liang. Unet++:\\nA nested u-net architecture for medical image segmentation. In Deep learning in medical image\\nanalysis and multimodal learning for clinical decision support, pages 3–11. Springer, 2018.\\nZongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, and Jianming Liang. Unet++:\\nRedesigning skip connections to exploit multiscale features in image segmentation. IEEE trans-\\nactions on medical imaging, 39(6):1856–1867, 2019.\\n14\\n')])\n",
      "ResearchTopics(topic='Weakly supervised image segmentation using point annotations and region growing algorithms', priority=4, query='weakly supervised segmentation point', timestamp='2024-11-23 18:21:07', research_papers=[ResearchPaper(title='Weakly-Supervised 3D Medical Image Segmentation using Geometric Prior and Contrastive Similarity', authors=[arxiv.Result.Author('Hao Du'), arxiv.Result.Author('Qihua Dong'), arxiv.Result.Author('Yan Xu'), arxiv.Result.Author('Jing Liao')], abstract='Medical image segmentation is almost the most important pre-processing\\nprocedure in computer-aided diagnosis but is also a very challenging task due\\nto the complex shapes of segments and various artifacts caused by medical\\nimaging, (i.e., low-contrast tissues, and non-homogenous textures). In this\\npaper, we propose a simple yet effective segmentation framework that\\nincorporates the geometric prior and contrastive similarity into the\\nweakly-supervised segmentation framework in a loss-based fashion. The proposed\\ngeometric prior built on point cloud provides meticulous geometry to the\\nweakly-supervised segmentation proposal, which serves as better supervision\\nthan the inherent property of the bounding-box annotation (i.e., height and\\nwidth). Furthermore, we propose contrastive similarity to encourage organ\\npixels to gather around in the contrastive embedding space, which helps better\\ndistinguish low-contrast tissues. The proposed contrastive embedding space can\\nmake up for the poor representation of the conventionally-used gray space.\\nExtensive experiments are conducted to verify the effectiveness and the\\nrobustness of the proposed weakly-supervised segmentation framework. The\\nproposed framework is superior to state-of-the-art weakly-supervised methods on\\nthe following publicly accessible datasets: LiTS 2017 Challenge, KiTS 2021\\nChallenge, and LPBA40. We also dissect our method and evaluate the performance\\nof each component.', url='http://arxiv.org/abs/2302.02125v1', pdf_path='./papers/2302.02125v1.Weakly_Supervised_3D_Medical_Image_Segmentation_using_Geometric_Prior_and_Contrastive_Similarity.pdf', content='1\\nWeakly-Supervised 3D Medical Image\\nSegmentation using Geometric Prior and\\nContrastive Similarity\\nHao Du, Qihua Dong, Yan Xu, Jing Liao\\nAbstract—Medical image segmentation is almost the most\\nimportant pre-processing procedure in computer-aided diagnosis\\nbut is also a very challenging task due to the complex shapes\\nof segments and various artifacts caused by medical imaging,\\n(i.e., low-contrast tissues, and non-homogenous textures). In this\\npaper, we propose a simple yet effective segmentation framework\\nthat incorporates the geometric prior and contrastive similarity\\ninto the weakly-supervised segmentation framework in a loss-\\nbased fashion. The proposed geometric prior built on point\\ncloud provides meticulous geometry to the weakly-supervised\\nsegmentation proposal, which serves as better supervision than\\nthe inherent property of the bounding-box annotation (i.e.,\\nheight and width). Furthermore, we propose the contrastive\\nsimilarity to encourage organ pixels to gather around in the\\ncontrastive embedding space, which helps better distinguish low-\\ncontrast tissues. The proposed contrastive embedding space can\\nmake up for the poor representation of the conventionally-\\nused gray space. Extensive experiments are conducted to verify\\nthe effectiveness and the robustness of the proposed weakly-\\nsupervised segmentation framework. The proposed framework\\nare superior to state-of-the-art weakly-supervised methods on\\nthe following publicly accessible datasets: LiTS 2017 Challenge,\\nKiTS 2021 Challenge and LPBA40. We also dissect our method\\nand evaluate the performance of each component.\\nIndex Terms—Weakly-supervised Segmentation, Medical Im-\\nage Segmentation, Contrastive Similarity, Geometric Prior, Point\\nCloud\\nI. INTRODUCTION\\nS\\nEGMENTATION is of fundamental importance for the\\nunderstanding and interpretation of medical images, as\\nit is essential for the diagnostic, treatment, and follow-up\\nrehabilitation of various diseases. This task has been widely\\nstudied with the recent advent of deep convolutional neural\\nnetworks (CNNs) [1], [2]. Nevertheless, there exists the main\\nlimitation that their methods require a large number of training\\nimages with pixel-wise annotations. The extremely high cost\\nof collecting and annotating these training images largely\\nHao\\nDu,\\nQihua\\nDong\\nand\\nJing\\nLiao\\nare\\nwith\\nDepartment\\nof\\nComputer Science, City University of Hong Kong, Hong Kong, China\\n(e-mail:\\nhaodu8-c@my.cityu.edu.hk,qihuadong2-c@my.cityu.edu.hk\\nand\\njingliao@cityu.edu.hk).\\nYan Xu are with School of Biological Science and Medical Engineering,\\nBeihang University, Beijing, China (e-mail: xuyan04@gmail.com).\\nHao Du and Qihua Dong contributed equally to this work. Cor-\\nresponding\\nauthors:\\nJing\\nLiao\\n(jingliao@cityu.edu.hk)\\nand\\nYan\\nXu\\n(xuyan04@gmail.com).\\nThis work was supported by the HKSAR Innovation and Technology\\nCommission (ITC) under ITF Project MHP/109/19 and by the National\\nNatural Science Foundation in China under Grant 62022010, the 111 Project in\\nChina under Grant B13003, the high performance computing (HPC) resources\\nat Beihang University.\\nInput Image\\nBounding-Box\\nAnnotation\\nTemplate\\nGeometric\\nPrior\\nContrastive\\nSimilarity\\nPositive Queue\\nNegative Queue\\nEmbedding\\nSpace\\nPoint Cloud\\nMin. Chamfer \\nDistance\\n…\\n…\\nEncoder\\nDecoder\\nFig. 1. The illustration of the proposed weakly-supervised segmen-\\ntation framework. As shown in the ﬁgure, we propose geometric\\nprior and contrastive similarity for weakly-supervised segmentation.\\nThe top row indicates the geometric prior of our method. We ﬁrst\\nconvert the conventionally-used volume representation to point cloud\\nrepresentation and register the template organ to the predicted organ.\\nThen we minimize their Chamfer Distance. The bottom row explains\\nthe core idea of the proposed contrastive similarity. By dividing pixels\\ninto positive and negative pixels, we encourage organ pixels to gather\\naround in the embedding space to better segment the low-contrast\\norgan.\\nhampers the performance and limits the scalability of deep\\nCNNs in the medical domain. A popular paradigm to alleviate\\nthe need for pixel-wise annotations is the weakly-supervised\\nsegmentation with bounding-box annotations [3]–[7]. They\\nemploy bounding-box annotations to generate\\nproposals,\\nwhich are fake labels and thereby mimic full supervision.\\nNevertheless, despite the good performances achieved by\\nthese works in certain practical scenarios, their applicability\\nmight be limited for two reasons: 1) complex shapes: Some\\norgans have delicate structures , i.e., intra-kidney variabilities,\\nwhich are difﬁcult to be precisely segmented without pixel-\\nwise supervision; 2) imaging artifacts: as discussed in previous\\nworks [8]–[10], various medical imaging artifacts caused by\\ntechnical or physical problems make low-contrast tissues and\\nnon-homogenous textures hard to distinguish, especially in the\\nconventionally widely-used gray space. The complex shapes\\nand imaging artifacts largely limit the applicability of the\\nweakly-supervised segmentation models in many scenarios,\\nespecially when segmenting complex structures.\\nTo conquer the challenge of complex shapes we propose to\\nlearn the geometric prior of the organ by a standard organ\\ntemplate. Instead of using volume representation, we ﬁrst\\nleverage the gridding reverse [11] to convert the segmentation\\narXiv:2302.02125v1  [eess.IV]  4 Feb 2023\\n2\\nresult from volume representation to point cloud representation\\nand then compare it with the template in the point cloud space.\\nThe basic unit in point cloud representation is much more\\nﬁne-grained and ﬂexible than the volume representation (, i.e.,\\nﬂexible point v.s. uniform voxel grids), which helps better\\ndescribe delicate geometric structures. On the other hand,\\nunlike the conventionally-used gray space [8], we leverage\\nthe contrastive learning [12] to encode the pixels to high-\\ndimensional embedding space and encourage pixels of the\\nsame labels to gather around. This helps alleviate the imag-\\ning artifacts for richer expressivity in the embedding space\\ncompared to the gray space.\\nIn this paper, we present a novel weakly-supervised seg-\\nmentation framework, which makes the earliest effort to\\nincorporate geometric prior and contrastive similarity. And\\nthe framework is general as well that can be easily applied\\nto improve multiple weakly-supervised segmentation models\\nwith bounding-box annotations, i.e., Ai+L [13], BoxInst [7].\\nBy learning geometry prior from the given template and dis-\\ntinguishing low-contrast tissues by the contrastive similarity,\\nour method can generate high-quality results with bounding\\nbox supervision only.\\nOur method consists of two major components. In the\\ngeometric prior component, the shapes of proposals are con-\\nstrained by a given template represented by point cloud. Both\\nthe external boundaries and internal structures of the proposal\\nwill be optimized by minimizing the distance according to a\\ngiven template. The second component is the contrastive sim-\\nilarity, addressing the issues raised by medical imaging arti-\\nfacts. By pre-training a contrastive head, we successfully learn\\nthe difference between organ pixels and non-organ pixels.\\nThis component can better distinguish low-contrast tissues and\\nnon-homogenous texture than conventionally widely-used gray\\nspace. Through extensive experiments, we demonstrate that\\nour method can generate a high-quality segments, along with\\ndelicate internal details and accurate boundaries. We show\\nthat our method outperforms other bounding-box weakly-\\nsupervised methods\\n[5], [7] under similar settings. We also\\nconduct extensive experiments to verify the effectiveness of\\ncomponents in our method.\\nIn summary, our major contributions are three folds.\\n• We propose a simple yet effective weakly-supervised\\nsegmentation framework with bounding-box annotations,\\nwhich can be easily applied to many weakly-supervised\\nsegmentation models and improve their performances.\\n• We propose the geometric prior in point cloud represen-\\ntation to better guide the learning of shapes, especially\\nfor those organs with complex structures.\\n• The proposed contrastive similarity makes up for the poor\\nrepresentation of the conventional gray space and thus can\\nbetter distinguish tissues with medical imaging artifacts.\\nOur code and data will be made publicly available for further\\nresearch.\\nII. RELATED WORK\\nI\\nN this section, we ﬁrst review existing weakly-supervised\\nmedical segmentation methods with bounding-box annota-\\ntions in both natural and medical image segmentation, then we\\ndiscuss recent works with geometric prior and ﬁnally present\\nthe trends in contrastive similarity.\\nA. Weakly-supervised medical semantic segmentation\\nGenerally, methods in weakly-supervised segmentation are\\nclassiﬁed into four categories by the type of their weak anno-\\ntations: scribbles [14], points [15], [16], image-level tags [17],\\n[18] and bounding-box annotations [5]. Scribbles and points\\nsupervision at least label one scribble or point for each region,\\nand the annotated areas will be directly incorporated into the\\ncalculation of segmentation loss. Wang et al. [19] propose to\\nleverage a random walker algorithm [20] to generate initial\\nproposals for the unlabeled regions and then supervise the\\ntraining of segmentation models by the initial segments. Qu\\net al. [16] uses a similar training pipeline but a different\\nlabel generation method for label generation which combines\\nK-means clustering and Voronoi partition diagram. Xu et\\nal. [18] enrich the image-level labels to instance-level labels\\nby multiple instance learning (MIL) and segment images using\\nonly volume-level labels.\\nWeakly-supervised segmentation with bounding box annota-\\ntions earns increasing interest in medical image segmentation\\nfor its simplicity and low-annotation cost. We can deﬁne\\nthe bounding boxes with two corner coordinates that are\\neasy to store in real scenarios. In addition, the bounding\\nbox annotations are location-aware so that they provide the\\nspatial relationship of the target object, which is a popular\\ndirection in recent researches [21]–[24]. In the early stages,\\nresearchers [22], [25] propose to consider pixels within the\\nbounding box as foreground pixels and train the segmentation\\nframework by these noisy labels. Despite the good perfor-\\nmance achieved by such a scheme, they may accumulate\\nerrors during the alternative generation process. Most recently,\\nresearchers [7] tried to directly generate the segmentation\\nresult instead of the error-prone alternative way. Generally,\\nthey build a mask head to produce the segmentation result, and\\nthe bounding box annotation is employed to train this mask\\nhead. In this work, we follow this segmentation scheme where\\nthe segmentation result is directly generated by the mask\\nhead. Furthermore, to address the fore-mentioned complex\\nshapes and imaging artifacts we propose geometric prior and\\ncontrastive similarity, respectively.\\nB. Geometric prior\\nDifferent from natural images, there exists obvious anatom-\\nical prior (, i.e., atlas prior) in medical images, speciﬁcally in\\norgans of human bodies (i.e., shape and position). Existing\\nworks incorporating such anatomical prior mostly fall into\\ntwo categories: loss-based methods and graph-based meth-\\nods. Generally, graph-based methods [26]–[30] leverage the\\nprobability maps of occurring anatomy chances to construct\\ngraph models and estimate the foreground probability from\\nthe input image gray space. An appearance model of basic\\nforms is employed to improve the segmentation accuracy [29].\\nGao et al [30] proposes to apply an appearance ConvNet\\nto characterize the foreground. Despite the high accuracy\\nachieved by these methods, the graphical models bring heavy\\n3\\nand expensive computational burdens to the segmentation\\nframework, which makes it infeasible in certain scenarios.\\nAnother popular direction in combining the prior with\\nthe segmentation framework is loss-based methods. Re-\\nsearchers [31]–[36] mostly minimize the distance between the\\nsegmentation network output and the pre-deﬁned anatomical\\npriors. Several works [31], [32] pose regularization terms on\\nthe training objective (, i.e., anatomical adjacency or boundary\\nconditions). Distance between predictions and atlas prior are\\nalso calculated in latent feature space [35], [36].\\nCompared with graph-based methods, loss-based methods\\nprovide a versatile fashion to incorporate anatomical priors\\nwith a wider range of scales while maintaining the compu-\\ntational efﬁciency of the segmentation framework. However,\\nprevious loss-based methods fail to address the aforementioned\\ntwo issues for two aspects: 1) Previous works generally slice\\nthe volume into 2D or 3D patches, which are then processed\\nsequentially to save memory cost. However, such a partitioning\\nmethod breaks the global geometric relationships, resulting in\\ninferior segmentation performance. Different from them, we\\nlearn the geometric prior in 3D embedding space to capture the\\noverall geometry and proposed completeness head to ensure\\nthe shape completeness of the proposal. 2) Unlike previous\\nworks using a volume representation, we leverage the Gridding\\nReverse [11] to convert the volume representation to point\\ncloud representation. Compared to the volume representation\\nconstrained by uniform voxel grids, point cloud without grids\\nis more ﬂexible in representing delicate structures.\\nC. Contrastive learning\\nContrastive Learning aims to attract the positive and reverse\\nthe negative by dividing the feature space into positive and\\nnegative data pairs. As for semantic segmentation, it has been\\nmainly used as pre-training [37]–[39]. Van et al. [40] apply\\nit to distinguish features from various salient masks, showing\\nits superiority in unsupervised set-ups. Wang et al. [41] have\\nshown advantages of contrastive learning by learning in both\\npixel and region levels. Some researchers leverage contrastive\\nlearning to address the time-consuming pixel-wise labeling in\\nmedical image segmentation. Chaitanya et al. [12] propose\\na two-stage self-supervised contrastive learning framework to\\nlearn the feature matching both in global and local mechanisms\\nfrom unlabeled data in the pre-training stage. Hu et al. [42]\\nproposes a semi-supervised scheme to learn self-supervised\\nglobal contrast and supervised local contrast. In our work, we\\nobserve that the conventionally-used gray space is not enough\\nto distinguish positive and negative pixels (, i.e., organ pixels\\nand non-organ pixels), especially in Magnetic Resonance\\nImaging. We thus leverage contrastive learning to calculate the\\ncontrastive similarity between pixels. By encoding pixels to\\nhigh-dimensional features and encouraging pixels of the same\\nlabel to gather around in the embedding space, it enhances\\nthe discriminability and thus alleviates the poor performance\\nof the gray space in handling medical imaging artifacts, i.e.,\\nartifacts in ultrasound imaging and similar surrounding tissues.\\nIII. METHOD\\nA. Overall framework\\nGiven an input image I ∈RS×H×W (S indicates the slice\\nnumber, H indicates height and W represents width) and\\nits corresponding bounding-box annotation B1×6 (constrained\\nby its upper left coordinates and bottom right coordinates),\\nour weakly-supervised framework F(·) obtains the pixel-wise\\nsegmentation mask M = F(I) and the training goal is to\\nminimize the loss function Lframe:\\nmin\\nF Lframe(I, B, F)\\n(1)\\nPipeline Following nnUNet [2], we randomly sample an\\ninput patch p ∈RS′×H′×W ′ from the original input image\\nand encode the patches by a ConvNet encoder E. Similar\\nto [1], we adopt a multi-layer ConvNet as the decoder G to\\nobtain the feature maps P ∈RS′×H′×W ′, where the decoder\\nshares the same layer number as the encoder. As shown in\\nFig. 2, the proposed geometric prior and contrastive learning\\nare incorporated in the training of the mask head to address the\\naforementioned complex shapes and imaging artifacts issues.\\nThe training loss Lframe is composed of two components: Lori\\nand Lmask\\nLframe = Lori + Lmask\\n(2)\\nwhere Lori indicates the original training loss of a standard\\nweakly-supervised framework (i.e., Lfcos in BoxInst [7]) and\\nLmask stands for the training loss of the mask head. In the\\nfollowing paragraphs, we mainly discuss the training of the\\nmask head. The training of the mask head can be formulated\\nas Eq.(3).\\nLmask = Lgeo + Lcons\\n(3)\\nThe mask head produce binary segmentation masks which is\\nfurther optimized by our proposed geometric prior Lgeo and\\ncontrastive similarity Lcons. More speciﬁcally, for geometric\\nprior loss, we build a completeness head to predict the com-\\npleteness score for every proposal, indicating the conditional\\nprobability that the object is complete inside the input. Each\\ncomplete proposal is converted into point cloud and registered\\nwith the point cloud of the template organ. The Chamfer\\nDistance loss is applied to minimize their distance. In the\\naspect of the contrastive similarity loss, we build a contrastive\\nhead to obtain the contrastive similarity by the feature maps\\nthat the contrastive head assigns positive and negative labels to\\neach position in the feature maps. In the following paragraphs,\\nwe ﬁrst introduce the geometric prior. Then we elaborate on\\nthe technical details of the proposed contrastive similarity.\\nB. Geometric Prior\\nThe proposed geometric prior is applied in 3D point cloud\\nspace for two reasons: 1) we observe that 2D slices cannot\\nwell-preserve the geometric continuity of 3D organs. Thus,\\nwe learn the geometric shape of the organ in 3D embed-\\nding space. 2) The conventionally-used volume representation\\ncannot handle the segmentation of meticulous structures. The\\nexpressivity of the volume representation is largely limited\\nby the uniform voxel grids. Instead, we leverage gridding\\nreverse [11] to process the segmentation of complex shapes\\nin point cloud embedding space.\\n4\\nAs shown in Fig. 2, we propose the geometric prior to\\nbetter weakly supervise the training of the mask head. The\\ngeometric prior refers to the template organ’s boundary shape\\nand internal distribution. More speciﬁcally, we introduce grid-\\nding reverse [11] to building a bridge between the volume\\nrepresentation and point cloud representation conversion. This\\nhelps us to get rid of the representation constraint in the\\nvolume representation. After converting both template organ\\nT and proposal S into the point cloud representation, we then\\nregister the template organ to the proposal by a widely-used\\nICP registration tool [43]. We ﬁnally minimize the geometric\\nprior loss of the Chamfer Distance between the template organ\\nand the proposal in the point cloud embedding space. Below\\nwe ﬁrst introduce the conversion and registration of point\\ncloud and then the deﬁnition of geometric prior loss.\\nConversion & Registration To utilize the rich expressivity\\nin point cloud representation, we introduce the gridding re-\\nverse [11] to help the transition between the volume represen-\\ntation and the point cloud representation. For each voxel grid,\\nthe gridding reverse calculates the weighted sum of the eight\\nvertices of the corresponding grid and assigns the weighted\\nsum to coordinates of a new point. Unlike uniform voxel\\ngrids, the high ﬂexibility of points’ coordinates enable the\\npoint cloud representation to describe meticulous and complex\\narchitectures. This helps better learn the difﬁcult intra-organ\\nvariabilities in the weakly-supervised segmentation. Further-\\nmore, we propose the sparse registration, which is applied\\nbefore calculating the Chamfer Distance between S and T.\\nTiny rotation of the template greatly impacts the calculation\\nof the Chamfer Distance, especially when the object structure\\nis much more complex. We thus conduct registration [43]\\nbetween the general shape of the proposal and the template.\\nSpeciﬁcally, we sample 20% points uniformly across the\\ninterval for the template and the proposal respectively, and\\nthen calculate the transform matrix between them by the ICP\\nregistration tool [43]. The template point cloud is registered\\naccording to the transform matrix.\\nLoss The geometric prior is then applied in the loss function\\nof the mask head training that we optimize the Chamfer\\nDistance between the proposal S and the registered template\\norgan T. This can be formulated as follows:\\nLgeo = 1\\n|S|\\nX\\nx∈S\\nmin\\ny∈T ||x −y||2 + 1\\n|T|\\nX\\ny∈T\\nmin\\nx∈S ||y −x||2.\\n(4)\\nSpeciﬁcally, the mask head produces binary segmentation\\nmasks for each proposal. The proposal is a probabilistic\\nsegmentation mask consisting of the segmented instances.\\nThis segmentation mask is further processed by the Gumbel-\\nSoftmax [44] to obtain the binary voxel. Locations of low\\nprobability are assigned to 0 and vice versa. We then adopt\\nthe gridding reverse [11] to obtain the point cloud proposal\\nS from the binary voxel. Finally, we calculate the Chamfer\\nDistance between S and T as the geometric prior loss.\\nC. Contrastive Similarity\\nGray space performs poorly for the artifacts in medical\\nimaging and similar surrounding tissues. It is not enough to\\ndistinguish between positive and negative pixels (, i.e., organ\\npixels and non-organ pixels). We thus leverage contrastive\\nlearning to calculate the contrastive similarity between pixels.\\nEncoding pixels to high-dimensional features and encouraging\\npixels of the same label to gather around in the embedding\\nspace helps to increase the distinguishability.\\nTo calculate the proposed contrastive similarity, we ﬁrst\\nbuild a ConvNet contrastive head after the decoder. Following\\nprevious contrastive learning works [42], [45], we build a two-\\nlayer point-wise convolution h(·) to extract distinct represen-\\ntations from feature maps P. More speciﬁcally, we ﬁrst pre-\\ntrain the proposed contrastive head in a coarse-to-ﬁne fashion\\nwhere only bounding box annotations are included in the\\npre-training stage. We encode pixels into embedding features\\nC = h(P) and encourage pixels of the same label to gather\\naround in the embedding space. The contrastive similarity\\nbetween two pixels is deﬁned as the distance in the embedding\\nspace. To calculate the contrastive similarity loss for the\\nwhole image, an undirected graph is constructed where the\\nvertices correspond to the pixels and edges are links between\\nneighboring pixels. The contrastive similarity associated with\\neach edge is then summarized for the calculation of the overall\\ncontrastive similarity loss. Below we ﬁrst introduce the pre-\\ntraining of the contrastive head and then the deﬁnition of\\ncontrastive similarity loss.\\nPre-training The pre-training of the contrastive head is\\nconducted in a weakly-supervised fashion that only bounding\\nbox annotations are included in the pre-training stage. To be\\nmore speciﬁc, there are two sub-stages in the pre-training\\nstage: coarse and reﬁne. In the coarse stage, we ﬁrst take\\npixels within the bounding box as positive labels and pixels\\noutside the bounding box as negative labels. Then we train the\\ncontrastive head by such labeling. However, the performance\\nof the contrastive head is largely limited for the noisy labels.\\nThus, we propose the reﬁne stage to further improve the\\nperformance of the contrastive head. In the reﬁne stage, we\\nﬁrst take random K negative pixels as referring pixels. And for\\neach pixel within the bounding box, we calculate the distance\\nD between all K referring pixels:\\nDu,v,z =\\nK\\nX\\ni=1\\n1{{Cu,v,z · Ci} ≥τ}\\n(5)\\nwhere Cu,v,z ∈RS×H×W indicates the feature at the (u, v, z)\\nof the embedding features, 1 stands for 1 if the distance is\\ngreater than τ and 0 if less, and τ is the threshold to decide\\nwhether pixels are positive or negative. If Du,v,z is greater than\\nK/2, the pixel at location (u, v, z) is considered positive, and\\nvice versa. Then, we train the contrastive head using the same\\ntraining loss as in the coarse stage, which is formulated as:\\nloss = −1\\n|Ω|\\nX\\n(u,v,z)∈Ω\\n1\\n|P(u, v, z)| ·\\nlog\\nP\\n(up,vp,zp)∈P(u,v,z) exp(Cu,v,z · Cup,vp,zp/τ)\\nP\\n(un,vn,zn)∈N (u,v,z) exp(Cu,v,z · Cun,pn,zn/τ)\\n(6)\\nwhere Cu,v,z ∈RS×H×W indicates the feature at the (u, v, z)\\nof the feature map, and Ωstands for all points inside input.\\n5\\nP(u, v, z) denotes the set of points with the same label as the\\npixel at (u, v, z) and N denotes the set of points with different\\nlabels. τ is the temperature constant.\\nLoss Considering an undirected graph G = (V, E) built on\\nthe input image I, where V corresponds pixels and E indicates\\nedges between neighboring pixels, the predicted segmentation\\nmask can be viewed as the probability of pixel (u, v, z) being\\nforeground. Then the probability of pixel (u1, v1, z1) and pixel\\n(u2, v2, z2) being the same label is:\\nProb(ye = 1) = ˜\\nMu1,v1,z1 · ˜\\nMu2,v2,z2\\n+ (1 −˜\\nMu1,v1,z1) · (1 −˜\\nMu2,v2,z2),\\n(7)\\nwhere\\n˜\\nM indicates the foreground probability mask and ye\\nrepresents the label of the edge.\\nThus, we can deﬁne an indicator on each edge to indicate\\nwhether they belongs to the same label. If the contrastive\\nsimilarity between two neighboring pixels is above the pre-\\ndeﬁned threshold τ, the indicator on the edge linking them\\nis assigned to 1, and 0 vice versa. We discard the edges\\nwith 0 and further summarize the contrastive similarity loss\\nof positive edges, which can be formulated as:\\nLcons = −1\\nN\\nX\\ne∈Ein\\n1{Cestart·Ceend≥τ} log Prob(ye = 1). (8)\\nThis serves as the contrastive similarity loss of the whole\\nimage.\\nIV. EXPERIMENTS\\nA. Datasets\\n• LiTS: The public liver LiTS [49] dataset comprises 201\\nCT scans from various CT scanners and devices. The\\nresolution of images in this dataset is from 0.56mm to\\n1.0mm in axial and 0.45mm to 6.0mm in z direction.\\nSlices in z range from 42 to 1026. We split 131 cases\\ninto training and evaluation sets by a ratio of 4:1.\\n• KiTS21: The publicly accessible KiTS21 [50] dataset\\nconsists of 300 cases during the period from 2010 to\\n2020. Each CT scan in the dataset is annotated by three\\nexpert annotators for the following semantic classes:\\nKidney, Tumor, and Cyst. We split the provided CT scans\\ninto training and validation sets by a ratio of 4:1.\\n• LPBA40: This dataset consists of 40 T1-weighted image\\nvolumes from randomly selected cases (among 40 scans:\\n20 males, 20 females, and 29.2 ± 6.3 years). The scans\\nwere acquired with a spatial resolution of 0.86 × 1.5 ×\\n0.86 mm3. Here, we conduct experiments on the subset\\nof the hippocampus in LPBA40. .\\nB. Implementation details and evaluation metrics\\nThe proposed framework with geometric prior and con-\\ntrastive similarity can be easily incorporated with any weakly-\\nsupervised segmentation models with bounding box annota-\\ntions. To verify the robustness of our method, we conduct ex-\\nperiments with two models: Ai+L [13] and BoxInst [7]. Here,\\nwe take the latest BoxInst [7] as our baseline, and experiments\\nare conducted based on this model unless otherwise speciﬁed.\\nThe training setting is mostly based on BoxInst’s training\\nsettings: The basic learning rate is 0.01 with weight decay 1e-\\n4, and a MultiStepLR scheduler with warmup is adopted. Our\\nframework is trained on GeForce RTX 3090 GPU. During the\\ninference stage, we set the threshold of the completeness head\\nto 0.6 and the threshold of the class head to 0.5 empirically.\\nWe randomly select a training sample as the template for each\\nspeciﬁc dataset unless otherwise speciﬁed. We evaluate our\\nresults by two widely adopted metrics: the Dice score (DSC)\\nin percentages and the Hausdorff Distance (95%). 1) DSC\\nscore is calculated as the overlap area of two masks divided\\nby their summation. A higher DSC score corresponds to better\\noverlap with GT. 2) The Hausdorff Distance mainly measures\\nthe boundary distance between the segmentation result and the\\npixel-wise segmentation masks. Better segmentation results are\\nof a smaller value than inferior results.\\nPre-processing & Post-processing Following nnUNet [2],\\nthe pre-processing includes downsampling, patching, and data\\naugmentation. Here, we downsample the data to reduce mem-\\nory use and ensure the existence of complete instances inside\\none patch. Then in the post-processing stage, the pipeline con-\\ntains resampling, patching, and patch-NMS (Non-Maximum\\nSuppression between patches). We ﬁrst adjust the spacing and\\npatch the data to the same size as training, with a certain step\\nsize. Then we predict a segmentation mask for each patch and\\nreturn all the predictions to the original space. Since there\\nare overlapping areas between patches, we use patch-NMS in\\nthese areas to eliminate the duplicates.\\nBounding box annotation We conduct experiments on\\nLiTS, KiTS, and LPBA40 datasets. All these three datasets\\nhave pixel-wise segmentation annotations. We utilize the cor-\\nresponding pixel-wise annotations to obtain the correspond-\\ning bounding-box annotations. In the training stage, only\\nbounding-box annotations are included. During the inference\\nstage, we evaluate the performance by pixel-wise annotations.\\nC. Results\\nQuantitative Results. We report the quantitative results\\nof three datasets (LiTS17, KiTS21, and LPBA40) in Tab. I.\\nOur method aims to integrate the geometric prior and con-\\ntrastive similarity to give better supervision in the training\\nof the weakly-supervised segmentation framework. Compared\\nwith BoxInst [7], which is the baseline of our method, the\\nsegmentation performance of our method is largely improved\\nover all three datasets. This is because the geometric prior\\ncan supervise the learning of both outer shape and the inner\\nstructure, and the contrastive similarity better distinguishes\\norgan and non-organ pixels in the embedding space. Simi-\\nlarly, ours outperforms the most recent two weakly-supervised\\nmethods [42], [47] supervised by bounding-box annotations,\\nwhich are based on the Multiple Instance Learning (i.e., MIL).\\nThe reason behind this is that MIL mainly focuses on the\\nboundary regression of the proposal, but additionally, we learn\\nthe internal details of organs from the geometric prior. We also\\nevaluate the upper bound of our method in which we supervise\\nthe framework training with ground truth.\\nQualitative Results. We present the qualitative compari-\\nson with two state-of-the-art weakly-supervised segmentation\\n6\\n...\\nMask \\nHead\\nEncoder \\nFeature \\nMaps\\nInputs\\n...\\nContrastive\\nHead\\nContrastive Similarity\\nFeature \\nMaps\\nLcons\\nSegmentation\\nMasks\\nPositive \\nQueue\\nNegative \\nQueue\\nLgeo\\nEmbedding\\nFeatures\\nProposal\\nTemplate\\n...\\n...\\nPositive \\nQueue\\nNegative \\nQueue\\nEmbedding\\nFeatures\\nDecoder \\nGeometric Prior\\nOrgan\\nTemplate\\nSparse \\nRegistration\\nGridding\\nReverse\\nChamfer \\nDistance Loss\\nSegmentation \\nMasks\\nRegistered\\nTemplate\\nPoint Cloud\\nProposal\\nPre-training Stage\\nInference Stage\\n...\\nCompleteness \\nHead\\n...\\nContrastive \\nHead\\n...\\nIf Complete\\nSegmentation \\nMasks\\nForeground\\nProb.\\n0.93\\n0.91\\n0.79\\n0.21\\n0.07\\n0.11\\n0.49\\n0.89\\n...\\n...\\nMaximize\\nFig. 2. The overall framework of our weakly-supervised segmentation framework with geometric prior and contrastive similarity. The training of our framework\\nis organized as follows: We ﬁrst pre-process the input images and obtain the embedding feature maps. Then we jointly supervise the mask head’s training by\\nthe proposed geometric prior and contrastive similarity. In the aspect of geometric prior, we ﬁrst convert the segmentation result to point cloud if complete.\\nThen we calculate the Chamfer Distance between the result and the registered template. In the aspect of contrastive similarity, we ﬁrst pre-train a contrastive\\nhead using only bounding box annotations. Then we minimize the distance between pixels of the same labels by the pre-trained contrastive head.\\nTABLE I\\nPERFORMANCE COMPARISON WITH OTHER METHODS. WE EVALUATE THE PERFORMANCE USING TWO METRICS: DICE SCORE AND HD95. † WE OBTAIN\\nTHEIR DICE SCORE BY THEIR OPEN SOURCE CODE. ‡: WE REBUILD THE 3D FRAMEWORK BASED ON ITS 2D OPEN SOURCE CODE. Ours w/o geometric\\nINDICATES WE REMOVE THE GEOMETRIC PRIOR COMPONENT. Ours w/o contrastive INDICATES WE REMOVE THE CONTRASTIVE SIMILARITY COMPONENT.\\nMethod\\nBackbone\\nLiTS17\\nKITS21\\nLPBA40\\n↑DSC(%)\\n↓HD95\\n↑DSC(%)\\n↓HD95\\n↑DSC(%)\\n↓HD95\\nFully Supervised\\nUNet [1]\\n95.5\\n5.3\\n96.0\\n3.2\\n83.7\\n2.1\\nDeepCut (2016) [25]\\n-\\n37.1\\n15.2\\n36.2\\n14.7\\n-\\n-\\nSDI (2017) [23]\\nVGG-16 [46]\\n49.2\\n11.7\\n-\\n-\\n38.7\\n9.0\\nMIL (2020) [47]†\\nENet [48]\\n69.4\\n9.4\\n72.3\\n8.7\\n-\\n-\\nGMIL (2021) [42]†\\nENet [48]\\n71.1\\n8.8\\n71.7\\n5.9\\n58.2\\n4.7\\nBoxInst (2021) [7]‡\\nUNet [1]\\n47.1\\n11.3\\n48.4\\n11.6\\n37.9\\n8.9\\nOurs w/o geometric\\nUNet [1]\\n52.9\\n10.9\\n54.2\\n10.7\\n44.9\\n7.1\\nOurs w/o contrastive\\nUNet [1]\\n69.7\\n9.4\\n68.5\\n9.1\\n57.3\\n4.9\\nOurs\\nUNet [1]\\n79.8\\n8.7\\n80.2\\n5.3\\n65.4\\n4.2\\nmethods in Fig 3. Compared with BoxInst [7], which is the\\nbaseline of our framework, we well preserve the proposal’s in-\\nner structure detail and outer shape. By comparing Column.#4\\nand Column.#7, the segmentation accuracy is largely improved\\ncompared to the baseline in Fig. 3. This is because the\\nproposed geometric prior and the contrastive similarity help\\nbetter segment organs. Additionally, the quality of our result\\nis much better than other state-of-the-art methods, especially in\\nterms of internal details. Their methods are based on Multiple\\nInstance Learning (i.e., MIL, GMIL), which mainly focuses\\non the outer shape of the proposal ignoring the hollows inside\\nthe organs. In contrast to their methods, we learn the inner\\nstructure and geometric details from the given template organ\\nalong with the outer shape that boosts the training of the mask\\nhead.\\nD. Ablation Study\\nIn this subsection, we ﬁrst present the component-wise\\nanalysis of our framework and then discuss the effectiveness\\nof the proposed geometric prior and contrastive similarity.\\n1) Robustness: In this subsection, we mainly discuss the\\nrobustness of the proposed weakly-supervised segmentation\\n7\\nTABLE II\\nROBUSTNESS ANALYSIS. WE MEASURE THE CORRESPONDING\\nDICE SCORE ON THE KITS21 DATASET. WE IMPLEMENT THE 3D\\nVERSION OF BOXINST [7] BASED ON ITS 2D VERSION PUBLIC\\nCODE.\\n#\\nBackbone\\nModel\\nWithout\\nWith\\nOur Loss\\nOur Loss\\n1\\nResNet [51]\\nBoxInst 3D [7]\\n47.8\\n77.1\\n2\\nUNet [1]\\nBoxInst 3D [7]\\n49.1\\n80.2\\n3\\nResNet [51]\\nAi+L [13]\\n67.2\\n72.1\\nframework. Our framework is built on BoxInst [7], which\\nis composed of Backbone and FPN. We analyze the im-\\npact of the backbone on the weakly-supervised segmentation\\nperformance. As shown in Tab. II, our method is robust to\\nother backbones (i.e., ResNet [51] and U-Net [1]). Further-\\nmore, we verify the generality of the proposed framework by\\nextending to other weakly-supervised segmentation models.\\nFollowing Chu et al. [13], we use the Ai+L [13] model with\\na ResNet50 [51] backbone that is widely used in both non-\\nmedical [52] and medical [53] segmentation tasks. We con-\\ncatenate the geometric prior branch and contrastive similarity\\nbranch after the decoder. As shown in Tab. II, the baseline\\nmodel obtains 67.2% in the KiTS dataset. The segmentation\\nperformance is largely improved by 4.9% after adding the\\nproposed geometric prior and contrastive similarity losses.\\n2) Geometric prior: In the proposed geometric prior, we\\nminimize the Chamfer Distance between the template organ\\nand the proposal in the point cloud embedding space. The\\npoint cloud representation is much more ﬂexible than the vol-\\nume representation. This is because the volume representation\\nis constrained by the uniform voxel grids. The representative\\npoint of each grid is ﬁxed to the center of each corresponding\\ngrid. In contrast, there is no such constraint in the point\\ncloud representation. The coordinates of points are much\\nmore precise and ﬂexible elastic (i.e., {0.17, 0.18, ...} v.s.\\n{1.00, 2.00, ...}). This helps the point cloud representation\\nto better describe delicate and complex structures than the\\nconventionally-used volume representation. Thus, to validate\\nthis point, we conduct an ablation of registration and optimiza-\\ntion with the volume representation. More speciﬁcally, we ﬁrst\\nregister the organ by the widely-used SimpleITK [54]. Here,\\nthe registration settings are set as follows: Mean Square metric,\\nrandom sampling strategy with a percentage of 0.01, shrink\\nfactors [4, 2, 1], smoothing sigmas [2, 1, 0], and 100 iterations.\\nAfter the registration, we employ the Dice loss to optimize the\\ndistance between the segmentation result and the geometric\\nprior. As shown in Fig. 4, the result shows that the point\\ncloud representation can handle more complex architectures\\nand precise geometric shapes, by improving 4.3% in the Dice\\nScore Coefﬁcient.\\nInternal details We further analyze the impact of the tem-\\nplate from the perspective of outer shape and inner structures,\\nrespectively. The experiments in this subsection are organized\\nas follows: We ﬁrst fulﬁll the inner structure of the template\\nand only retain the outer shape. Then we train the mask head\\nwith the deformed template. Normally, there exists intra-organ\\nvariabilities, i.e., as shown in Fig. 5, especially meticulous\\narchitectures inside the kidney. And the segmentation per-\\nformance drastically degrades (3.9%) for the lack of internal\\nprior.\\nAnalysis of the completeness head As discussed in\\nSec. III-A, following nnUNet [2], for an input image, we\\nﬁrst partition it into patches and sequentially process these\\npatches. However, during the partitioning, organs may not be\\ncomplete, which may cause the failure of the learning from\\nthe template. Thus, to ensure the success of learning from\\nthe template, the proposed completeness head detects whether\\nthe proposal in the sampled patch is complete. Only complete\\nproposals are further processed. As shown in Tab. III, if we\\nremove the completeness head, the performance degrades by\\n1.1% compared with our framework.\\n3) Architectures of contrastive head: We conduct exper-\\niments on the design of the proposed contrastive head in\\nthis subsection. Following previous works [42], [45], the\\ncontrastive head is composed of a two-layer point-wise con-\\nvolution that helps distinguish organ tissues from non-organ\\ntissues. To further analyze the structure of the contrastive\\nhead, we conduct ablation experiments on the dimension of\\nthe embedding space. As shown in Tab. IV, we evaluate both\\nthe performance and the efﬁciency of different embedding\\ndimension that by considering the trade-off between efﬁciency\\nand performance, we empirically set the embedding dimension\\nto 32.\\nContrastive similarity We propose the contrastive simi-\\nlarity in the embedding space. To compare the embedding\\nspace with the commonly used gray space, we select the\\nfollowing two similarity metrics: MSE and SSIM [55]. Both\\nMSE and SSIM are calculated on gray values since medical\\nimages are in grayscale. As shown in Tab. IV, the MSE\\nmetric fails to distinguish low-contrast tissues, and the SSIM\\nTABLE III\\nANALYSIS OF GEOMETRIC PRIOR. † INDICATES THE INNER STRUCTURE OF\\nTHE TEMPLATE.\\n#\\nAblation Setting\\nDSC on\\nKiTS\\n1\\nw/o Completeness Head\\n79.1\\n2\\nw/o Internal Details†\\n76.3\\n3\\nBaseline\\n80.2\\nTABLE IV\\nANALYSIS OF THE CONTRASTIVE HEAD. †: C STANDS FOR THE\\nCONTRASTIVE SIMILARITY, S INDICATES THE SSIM SIMILARITY AND G\\nSTANDS FOR THE GRAYSCALE SIMILARITY. THE INFERENCE SPEED\\nINDICATES THE TOTAL TIME OF 1000 RUNS.\\n#\\nComponent Setting\\nEmbedding\\nEmbedding\\nInference\\nDSC (%)\\nDimension\\nSpace†\\nSpeed\\non KiTS\\n1\\nContrastive Head\\n8\\nC\\n3s\\n78.4\\n2\\n16\\nC\\n5s\\n79.1\\n3\\n32\\nC\\n8s\\n80.2\\n4\\n64\\nC\\n15s\\n80.4\\n5\\nSimilarity Metrics\\n32\\nC\\n8s\\n80.2\\n6\\n32\\nG\\n8s\\n73.4\\n7\\n32\\nS\\n12s\\n47.2\\n8\\nmetric is inferior to the contrastive similarity in terms of both\\nsegmentation performance and evaluation efﬁciency.\\nV. CONCLUSION\\nI\\nN this work, we propose a novel weakly-supervised seg-\\nmentation framework with bounding-box annotations. We\\nintroduce the geometric prior and the contrastive similarity\\nto address the challenges of complex shapes and imaging\\nartifacts, enhancing the practicability and robustness of the\\nweakly-supervised segmentation framework. The geometric\\nprior enables the learning of delicate and complex structures,\\nand the contrastive similarity helps better distinguish organ\\npixels from non-organ pixels.\\nOur framework is general, which can be easily applied\\nto many weakly-supervised segmentation models and thus\\nimprove their performances. It will probably serve as a base\\nfor possible future studies on weakly-supervised segmentation,\\nespecially for medical image segmentation on organs that are\\nwith a speciﬁc shape. Extensive experiments are conducted to\\nverify the effectiveness and the superiority of our proposed\\ngeometric prior and contrastive similarity.\\nREFERENCES\\n[1] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks\\nfor biomedical image segmentation,” in International Conference on\\nMedical image computing and computer-assisted intervention. Springer,\\n2015, pp. 234–241.\\n[2] F. Isensee, P. F. Jaeger, S. A. Kohl, J. Petersen, and K. H. Maier-Hein,\\n“nnu-net: a self-conﬁguring method for deep learning-based biomedical\\nimage segmentation,” Nature methods, vol. 18, no. 2, pp. 203–211, 2021.\\n[3] D. Pathak, P. Krahenbuhl, and T. Darrell, “Constrained convolutional\\nneural networks for weakly supervised segmentation,” in Proceedings of\\nthe IEEE international conference on computer vision, 2015, pp. 1796–\\n1804.\\n[4] Z. Jia, X. Huang, I. Eric, C. Chang, and Y. Xu, “Constrained deep weak\\nsupervision for histopathology image segmentation,” IEEE transactions\\non medical imaging, vol. 36, no. 11, pp. 2376–2388, 2017.\\n[5] H. Kervadec, J. Dolz, M. Tang, E. Granger, Y. Boykov, and I. B. Ayed,\\n“Constrained-cnn losses for weakly supervised segmentation,” Medical\\nimage analysis, vol. 54, pp. 88–99, 2019.\\n[6] M. Bateson, H. Kervadec, J. Dolz, H. Lombaert, and I. B. Ayed,\\n“Constrained domain adaptation for segmentation,” in International\\nConference on Medical Image Computing and Computer-Assisted In-\\ntervention.\\nSpringer, 2019, pp. 326–334.\\n[7] Z. Tian, C. Shen, X. Wang, and H. Chen, “Boxinst: High-performance\\ninstance segmentation with box annotations,” in Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\\n2021, pp. 5443–5452.\\n[8] T. Budrys, V. Veikutis, S. Lukosevicius, R. Gleizniene, E. Monastyreck-\\niene, and I. Kulakiene, “Artifacts in magnetic resonance imaging:\\nhow it can really affect diagnostic image quality and confuse clinical\\ndiagnosis?” Journal of Vibroengineering, vol. 20, no. 2, pp. 1202–1213,\\n2018.\\n[9] F. E. Boas, D. Fleischmann et al., “Ct artifacts: causes and reduction\\ntechniques,” Imaging Med, vol. 4, no. 2, pp. 229–240, 2012.\\n[10] S. N. Sarkar, D. B. Hackney, R. L. Greenman, B. A. Vachha, E. A. John-\\nson, S. Nagle, and G. Moonis, “A subjective and objective comparison\\nof tissue contrast and imaging artifacts present in routine spin echoes\\nand in iterative decomposition of asymmetric spin echoes for soft tissue\\nneck mri,” European journal of radiology, vol. 102, pp. 202–207, 2018.\\n[11] H. Xie, H. Yao, S. Zhou, J. Mao, S. Zhang, and W. Sun, “Grnet: Grid-\\nding residual network for dense point cloud completion,” in European\\nConference on Computer Vision.\\nSpringer, 2020, pp. 365–381.\\n[12] K. Chaitanya, E. Erdil, N. Karani, and E. Konukoglu, “Contrastive\\nlearning of global and local features for medical image segmentation\\nwith limited annotations,” Advances in Neural Information Processing\\nSystems, vol. 33, pp. 12 546–12 558, 2020.\\n[13] T. Chu, X. Li, H. V. Vo, R. M. Summers, and E. Sizikova, “Improving\\nweakly supervised lesion segmentation using multi-task learning,” in\\nMedical Imaging with Deep Learning.\\nPMLR, 2021, pp. 60–73.\\n[14] D. Lin, J. Dai, J. Jia, K. He, and J. Sun, “Scribblesup: Scribble-\\nsupervised convolutional networks for semantic segmentation,” in Pro-\\nceedings of the IEEE conference on computer vision and pattern\\nrecognition, 2016, pp. 3159–3167.\\n[15] A. Bearman, O. Russakovsky, V. Ferrari, and L. Fei-Fei, “What’s the\\npoint: Semantic segmentation with point supervision,” in European\\nconference on computer vision.\\nSpringer, 2016, pp. 549–565.\\n[16] H. Qu et al., “Weakly Supervised Deep Nuclei Segmentation Using\\nPartial Points Annotation in Histopathology Images,” IEEE Transactions\\non Medical Imaging, pp. 1–1, 2020.\\n[17] G. Patel and J. Dolz, “Weakly supervised segmentation with cross-\\nmodality equivariant constraints.” Medical Image Analysis, p. 102374,\\n2022.\\n[18] G. Xu et al., “CAMEL: A Weakly Supervised Learning Framework for\\nHistopathology Image Segmentation,” in 2019 IEEE/CVF International\\nConference on Computer Vision (ICCV), 2019, pp. 10 681–10 690.\\n[19] X. Wang et al., “Weakly Supervised Deep Learning for Whole Slide\\nLung Cancer Image Analysis,” IEEE Transactions on Cybernetics, 2019.\\n[20] L. Grady, “Random Walks for Image Segmentation,” IEEE Transactions\\non Pattern Analysis and Machine Intelligence, vol. 28, pp. 1768–1783,\\n2006.\\n[21] J. Dai, K. He, and J. Sun, “Boxsup: Exploiting bounding boxes to super-\\nvise convolutional networks for semantic segmentation,” in Proceedings\\nof the IEEE international conference on computer vision, 2015, pp.\\n1635–1643.\\n[22] G. Papandreou, L.-C. Chen, K. P. Murphy, and A. L. Yuille, “Weakly-\\nand semi-supervised learning of a deep convolutional network for\\nsemantic image segmentation,” in Proceedings of the IEEE international\\nconference on computer vision, 2015, pp. 1742–1750.\\n[23] A. Khoreva, R. Benenson, J. Hosang, M. Hein, and B. Schiele, “Simple\\ndoes it: Weakly supervised instance and semantic segmentation,” in\\nProceedings of the IEEE conference on computer vision and pattern\\nrecognition, 2017, pp. 876–885.\\n[24] M. Pu, Y. Huang, Q. Guan, and Q. Zou, “Graphnet: Learning image\\npseudo annotations for weakly-supervised semantic segmentation,” in\\nProceedings of the 26th ACM international conference on Multimedia,\\n2018, pp. 483–491.\\n[25] M. Rajchl, M. C. Lee, O. Oktay, K. Kamnitsas, J. Passerat-Palmbach,\\nW. Bai, M. Damodaram, M. A. Rutherford, J. V. Hajnal, B. Kainz et al.,\\n“Deepcut: Object segmentation from bounding box annotations using\\nconvolutional neural networks,” IEEE transactions on medical imaging,\\nvol. 36, no. 2, pp. 674–683, 2016.\\n[26] B. Patenaude, S. M. Smith, D. N. Kennedy, and M. Jenkinson, “A\\nbayesian model of shape and appearance for subcortical brain segmen-\\ntation,” Neuroimage, vol. 56, no. 3, pp. 907–922, 2011.\\n[27] M. R. Sabuncu, B. T. Yeo, K. Van Leemput, B. Fischl, and P. Golland,\\n“A generative model for image segmentation based on label fusion,”\\nIEEE transactions on medical imaging, vol. 29, no. 10, pp. 1714–1729,\\n2010.\\n[28] B. Fischl, D. H. Salat, E. Busa, M. Albert, M. Dieterich, C. Haselgrove,\\nA. Van Der Kouwe, R. Killiany, D. Kennedy, S. Klaveness et al., “Whole\\nbrain segmentation: automated labeling of neuroanatomical structures in\\nthe human brain,” Neuron, vol. 33, no. 3, pp. 341–355, 2002.\\n[29] J. E. Iglesias and M. R. Sabuncu, “Multi-atlas segmentation of biomed-\\nical images: a survey,” Medical image analysis, vol. 24, no. 1, pp. 205–\\n219, 2015.\\n[30] M. Gao, Z. Xu, L. Lu, A. Wu, I. Nogues, R. M. Summers, and\\nD. J. Mollura, “Segmentation label propagation using deep convolutional\\nneural networks and dense conditional random ﬁeld,” in 2016 IEEE 13th\\nInternational Symposium on Biomedical Imaging (ISBI).\\nIEEE, 2016,\\npp. 1265–1268.\\n[31] P.-A. Ganaye, M. Sdika, and H. Benoit-Cattin, “Semi-supervised learn-\\ning for segmentation under semantic constraint,” in International Confer-\\nence on Medical Image Computing and Computer-Assisted Intervention.\\nSpringer, 2018, pp. 595–602.\\n[32] A. BenTaieb and G. Hamarneh, “Topology aware fully convolutional net-\\nworks for histology gland segmentation,” in International conference on\\nmedical image computing and computer-assisted intervention. Springer,\\n2016, pp. 460–468.\\n[33] H. Chen, X. Qi, L. Yu, Q. Dou, J. Qin, and P.-A. Heng, “Dcan: Deep\\ncontour-aware networks for object instance segmentation from histology\\nimages,” Medical image analysis, vol. 36, pp. 135–146, 2017.\\n9\\n[34] Y. Zhou, Z. Li, S. Bai, C. Wang, X. Chen, M. Han, E. Fishman, and\\nA. L. Yuille, “Prior-aware neural network for partially-supervised multi-\\norgan segmentation,” in Proceedings of the IEEE/CVF International\\nConference on Computer Vision, 2019, pp. 10 672–10 681.\\n[35] O. Oktay, E. Ferrante, K. Kamnitsas, M. Heinrich, W. Bai, J. Caballero,\\nS. A. Cook, A. De Marvao, T. Dawes, D. P. O‘Regan et al., “Anatomi-\\ncally constrained neural networks (acnns): application to cardiac image\\nenhancement and segmentation,” IEEE transactions on medical imaging,\\nvol. 37, no. 2, pp. 384–395, 2017.\\n[36] A. V. Dalca, J. Guttag, and M. R. Sabuncu, “Anatomical priors in\\nconvolutional networks for unsupervised biomedical segmentation,” in\\nProceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, 2018, pp. 9290–9299.\\n[37] S. Xie, J. Gu, D. Guo, C. R. Qi, L. Guibas, and O. Litany, “Pointcon-\\ntrast: Unsupervised pre-training for 3d point cloud understanding,” in\\nEuropean conference on computer vision.\\nSpringer, 2020, pp. 574–\\n591.\\n[38] Z. Xie, Y. Lin, Z. Zhang, Y. Cao, S. Lin, and H. Hu, “Propagate yourself:\\nExploring pixel-level consistency for unsupervised visual representation\\nlearning,” in Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition, 2021, pp. 16 684–16 693.\\n[39] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast\\nfor unsupervised visual representation learning,” in Proceedings of the\\nIEEE/CVF conference on computer vision and pattern recognition, 2020,\\npp. 9729–9738.\\n[40] W. Van Gansbeke, S. Vandenhende, S. Georgoulis, and L. Van Gool,\\n“Unsupervised semantic segmentation by contrasting object mask pro-\\nposals,” in Proceedings of the IEEE/CVF International Conference on\\nComputer Vision, 2021, pp. 10 052–10 062.\\n[41] W. Wang, T. Zhou, F. Yu, J. Dai, E. Konukoglu, and L. Van Gool,\\n“Exploring cross-image pixel contrast for semantic segmentation,” in\\nProceedings of the IEEE/CVF International Conference on Computer\\nVision, 2021, pp. 7303–7313.\\n[42] X. Hu, D. Zeng, X. Xu, and Y. Shi, “Semi-supervised contrastive\\nlearning for label-efﬁcient medical image segmentation,” in International\\nConference on Medical Image Computing and Computer-Assisted Inter-\\nvention.\\nSpringer, 2021, pp. 481–490.\\n[43] Q.-Y. Zhou, J. Park, and V. Koltun, “Open3D: A modern library for 3D\\ndata processing,” arXiv:1801.09847, 2018.\\n[44] E. Jang, S. Gu, and B. Poole, “Categorical reparameterization with\\ngumbel-softmax,” arXiv preprint arXiv:1611.01144, 2016.\\n[45] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework\\nfor contrastive learning of visual representations,” in International\\nconference on machine learning.\\nPMLR, 2020, pp. 1597–1607.\\n[46] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\\nlarge-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.\\n[47] H. Kervadec, J. Dolz, S. Wang, E. Granger, and I. B. Ayed, “Bounding\\nBoxes for Weakly Supervised Segmentation: Global Constraints Get\\nClose to Full Supervision,” arXiv Preprint ArXiv:2004.06816, 2020.\\n[48] A. Paszke, A. Chaurasia, S. Kim, and E. Culurciello, “Enet: A deep\\nneural network architecture for real-time semantic segmentation,” arXiv\\npreprint arXiv:1606.02147, 2016.\\n[49] P. Bilic, P. F. Christ, E. Vorontsov, G. Chlebus, H. Chen, Q. Dou, C.-W.\\nFu, X. Han, P.-A. Heng, J. Hesser et al., “The liver tumor segmentation\\nbenchmark (lits),” arXiv preprint arXiv:1901.04056, 2019.\\n[50] N. Heller, F. Isensee, K. H. Maier-Hein, X. Hou, C. Xie, F. Li, Y. Nan,\\nG. Mu, Z. Lin, M. Han et al., “The state of the art in kidney and\\nkidney tumor segmentation in contrast-enhanced ct imaging: Results of\\nthe kits19 challenge,” Medical Image Analysis, p. 101821, 2020.\\n[51] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\\nrecognition,” in Proceedings of the IEEE conference on computer vision\\nand pattern recognition, 2016, pp. 770–778.\\n[52] F. Lateef and Y. Ruichek, “Survey on semantic segmentation using deep\\nlearning techniques,” Neurocomputing, vol. 338, pp. 321–348, 2019.\\n[53] B. M. Anderson, E. Y. Lin, C. E. Cardenas, D. A. Gress, W. D. Erwin,\\nB. C. Odisio, E. J. Koay, and K. K. Brock, “Automated contouring of\\ncontrast and noncontrast computed tomography liver images with fully\\nconvolutional networks,” Advances in radiation oncology, vol. 6, no. 1,\\np. 100464, 2021.\\n[54] B. C. Lowekamp, D. T. Chen, L. Ib´a˜nez, and D. Blezek, “The design\\nof simpleitk,” Frontiers in neuroinformatics, vol. 7, p. 45, 2013.\\n[55] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image\\nquality assessment: from error visibility to structural similarity,” IEEE\\ntransactions on image processing, vol. 13, no. 4, pp. 600–612, 2004.\\n10\\nInput\\nGT\\nFully\\nBoxInst\\nMIL\\nGMIL\\nOurs\\nKidney\\nSurface X\\nKidney\\nSurface Y\\nKidney\\nSurface Z\\nLiver\\nSurface X\\nLiver\\nSurface Y\\nLiver\\nSurface Z\\nHippocampus\\nSurface X\\nHippocampus\\nSurface Y\\nHippocampus\\nSurface Z\\nFig. 3. Qualitative results of our framework over three datasets. As shown in the ﬁgure, four colors exist in the segmentation results. red\\nindicates ground-truth. green indicates correct predictions. blue indicates predictions where should have been predicted. yellow indicates\\nwrongly-predicted pixels. Our framework is built on BoxInst [7] which is the baseline method. We then compare several methods, including\\nfully-supervised results, the proposed method and demonstrate the high quality of the proposal.\\n11\\nInput\\nvolume\\npoint cloud\\nFig. 4.\\nVolume v.s. Point cloud. We optimize the learning of the\\ngeometric shape in the form of volume representation by Dice Loss.\\nAs shown in the ﬁgure, the proposed point cloud representation\\nlargely improves the segmentation result.\\nInput\\nw/o internal\\nw internal\\nFig. 5.\\nInternal details. To verify the effectiveness of the internal\\ndetails of the template, we erase the intra-organ variabilities by\\nfulﬁlling the template organ. As shown in the ﬁgure, the segmentation\\nperformance drastically degrades especially for organs with delicate\\nand complex structures. Figures from left to right are more and more\\ndelicate and complex.\\n'), ResearchPaper(title='Adaptive Superpixel for Active Learning in Semantic Segmentation', authors=[arxiv.Result.Author('Hoyoung Kim'), arxiv.Result.Author('Minhyeon Oh'), arxiv.Result.Author('Sehyun Hwang'), arxiv.Result.Author('Suha Kwak'), arxiv.Result.Author('Jungseul Ok')], abstract=\"Learning semantic segmentation requires pixel-wise annotations, which can be\\ntime-consuming and expensive. To reduce the annotation cost, we propose a\\nsuperpixel-based active learning (AL) framework, which collects a dominant\\nlabel per superpixel instead. To be specific, it consists of adaptive\\nsuperpixel and sieving mechanisms, fully dedicated to AL. At each round of AL,\\nwe adaptively merge neighboring pixels of similar learned features into\\nsuperpixels. We then query a selected subset of these superpixels using an\\nacquisition function assuming no uniform superpixel size. This approach is more\\nefficient than existing methods, which rely only on innate features such as RGB\\ncolor and assume uniform superpixel sizes. Obtaining a dominant label per\\nsuperpixel drastically reduces annotators' burden as it requires fewer clicks.\\nHowever, it inevitably introduces noisy annotations due to mismatches between\\nsuperpixel and ground truth segmentation. To address this issue, we further\\ndevise a sieving mechanism that identifies and excludes potentially noisy\\nannotations from learning. Our experiments on both Cityscapes and PASCAL VOC\\ndatasets demonstrate the efficacy of adaptive superpixel and sieving\\nmechanisms.\", url='http://arxiv.org/abs/2303.16817v2', pdf_path='./papers/2303.16817v2.Adaptive_Superpixel_for_Active_Learning_in_Semantic_Segmentation.pdf', content=\"Adaptive Superpixel for Active Learning in Semantic Segmentation\\nHoyoung Kim1\\nMinhyeon Oh2\\nSehyun Hwang2\\nSuha Kwak1,2\\nJungseul Ok1,2*\\nGraduate School of AI, POSTECH1,\\nDept. of CSE, POSTECH2\\n{cskhy16, minhyeonoh, sehyun03, suha.kwak, jungseul}@postech.ac.kr\\nAbstract\\nLearning semantic segmentation requires pixel-wise an-\\nnotations, which can be time-consuming and expensive. To\\nreduce the annotation cost, we propose a superpixel-based\\nactive learning (AL) framework, which collects a dominant\\nlabel per superpixel instead.\\nTo be specific, it consists\\nof adaptive superpixel and sieving mechanisms, fully ded-\\nicated to AL. At each round of AL, we adaptively merge\\nneighboring pixels of similar learned features into super-\\npixels.\\nWe then query a selected subset of these super-\\npixels using an acquisition function assuming no uniform\\nsuperpixel size. This approach is more efficient than ex-\\nisting methods, which rely only on innate features such as\\nRGB color and assume uniform superpixel sizes. Obtain-\\ning a dominant label per superpixel drastically reduces an-\\nnotators’ burden as it requires fewer clicks. However, it\\ninevitably introduces noisy annotations due to mismatches\\nbetween superpixel and ground truth segmentation. To ad-\\ndress this issue, we further devise a sieving mechanism that\\nidentifies and excludes potentially noisy annotations from\\nlearning. Our experiments on both Cityscapes and PAS-\\nCAL VOC datasets demonstrate the efficacy of adaptive su-\\nperpixel and sieving mechanisms.\\n1. Introduction\\nWith the advent of deep learning, many computer vi-\\nsion tasks including semantic segmentation have dramati-\\ncally evolved in recent years. Such advances are thanks to\\ncomplex deep network models that can learn huge datasets.\\nHowever, labeling such large datasets is prohibitively time-\\nconsuming and labor-intensive, in particular, for semantic\\nsegmentation tasks that demand a dense annotation on each\\npixel [8, 11]. Active learning (AL) offers an approach to al-\\nleviate the annotation cost by selectively querying only the\\nmost informative samples to annotators.\\nDesigning an effective form of annotation query is crit-\\n*Corresponding author.\\n(a) Over-segmented (t = 0)\\n(b) Adaptive merged (t = 2)\\n(c) Adaptive merged (t = 4)\\n(d) Oracle\\nFigure 1: Examples of adaptive superpixels. (a) We begin\\nactive learning with over-segmented superpixels. (b, c) In\\neach round t, we merge superpixels in an adaptive manner\\nusing the model from the previous round. (d) As the round\\nprogresses, adaptive superpixels look similar to oracle ones.\\nical in practice as it determines the actual annotation cost\\nsuch as the number of clicks required and the informative-\\nness per annotation query. For semantic segmentation, an\\nimage-wise query can be asked for a complete annotation on\\nthe semantic of every pixel in an image [9, 10, 34, 38, 40].\\nThis is a daunting task requiring an enormous amount of\\nclicks to indicate boundaries (using polygons or contours)\\nfor each semantic segment or to annotate semantic pixel-\\nwisely, while the diversity of contexts which we can observe\\nin a single image is restricted. Alternatively, one can design\\na region-based query enquiring only about the dominant la-\\nbel of a small region such as rectangle patch [5, 27, 37] or\\nsuperpixel [4, 33]. This is known to be simple yet effective\\nas it requires only a single click per query while enabling\\nAL to put more focus on significant regions and to avoid\\nannotation wastes.\\nAL with the region-based query needs a delicate genera-\\ntion of candidate regions to be queried. A small region size\\ndilutes the budget efficiency, whereas the dominant label-\\ning even by a perfect annotator is prone to give noisy labels\\narXiv:2303.16817v2  [cs.CV]  21 Aug 2023\\nMerge (Sec. 3.2)\\nSieve (Sec. 3.3)\\nModel \\nupdate\\nSieve (Sec. 3.3)\\nSuperpixel \\nselection\\nDominant\\nlabeling\\nOriginal labels\\nEq. (6)\\nSuperpixel-wise confidence threshold (knee point)\\nMerge (Sec. 3.2)\\nNode merging\\nConnectivity graph\\nBase superpixels\\n𝜃𝑡\\n𝜃𝑡−1\\nOracle\\nDenoise\\nconfidence\\nCDF\\nSieved labels\\nMerged superpixels\\nFigure 2: An overview of the proposed framework. In each round t, we merge superpixels with a graph using the latest model,\\nand obtain dominant labels for selected superpixels. The dominant labels are selectively propagated to pixels with confidence\\nabove the detected knee point, resulting in the creation of a sieved dataset. Finally, we train a model with the sieved one.\\nwhen regions are too large to be consisting of pixels with a\\nsingle class. However, the previous works [5, 27, 33] rely\\non a fixed candidate set of regions of uniform size, while we\\ncould adjust the size and shape of candidate regions as we\\ntrain the semantic segmentation model over rounds of AL.\\nThis limitation remains even in recent work [4] with super-\\npixel candidates providing less risk of noisy labels than rect-\\nangle ones since the superpixels are produced, only at the\\nbeginning, by a conventional superpixel algorithm, where\\nconventional superpixel algorithms [1, 32, 35] cluster ad-\\njacent pixels of similar innate features (e.g., color) with\\nimplicit or explicit regularization to make similar sizes or\\nshapes of superpixels, i.e., limited freedom of query region.\\nIn this paper, to fully enjoy the benefit in terms of anno-\\ntation cost while suppressing the risk of noisy labels, we de-\\nvise an AL framework, illustrated in Figure 2, consisting of\\nadaptive merging and sieving methods. The adaptive merg-\\ning method repeatedly evolves the candidate superpixels for\\ndominant labeling at every round with the latest model and\\nno explicit regularization on the size and shape of superpix-\\nels. This indeed enables the continual improvement of the\\nsuperpixels’ ability to accurately capture the boundaries of\\nsemantic objects (Figure 1b and 1c), and a proper variation\\nin the sizes and shapes of superpixels, i.e., larger superpix-\\nels being attached to larger semantic objects (e.g., road and\\nbuilding) and smaller ones to smaller objects (e.g., human\\nand vehicle) as shown in the ideal ones (Figure 1d).\\nGiven the adaptive superpixels, we establish a corre-\\nsponding acquisition function being aware of irregular su-\\nperpixel sizes. It prioritizes uncertain superpixels of rare\\nclasses in order to query the most informative superpixels\\nwhile balancing class distributions in the entire annotations.\\nIn addition, to alleviate the inevitable noise in the dominant\\nlabeling, we propose a sieving technique that excludes la-\\nbeled pixels of high potential risks of being different classes\\nthan the dominant one. To be specific, we identify such pix-\\nels of potentially noisy labels by per-superpixel sieving with\\ndistinct thresholds over superpixels. This provides stabler\\ndenoising than uniform sieving with a constant threshold,\\nwhich might aggravate class imbalance in the sieved anno-\\ntations.\\nThrough the integration of adaptive merging and siev-\\ning into an AL framework, we achieve improved accuracy\\nand budget-efficiency over a baseline method. Notably, the\\nmerging demonstrates effectiveness under small-sized su-\\nperpixels, while the sieving plays a critical role given large-\\nsized superpixels.\\nMoreover, we show a consistent im-\\nprovement over existing methods in various settings. We\\nprovide a thorough justification of the proposed method\\nusing various quantitative measures, where we introduce\\na new evaluation metric for superpixel algorithms that as-\\nsesses both (achievable) accuracy and recall, where the re-\\ncall is overlooked in the existing one, the achievable seg-\\nmentation accuracy (ASA) [22] but important in the context\\nof AL. This may give new insights into developing super-\\npixel algorithms.\\nOur main contributions are summarized as follows:\\n• We propose an adaptive merging algorithm where su-\\nperpixels are updated at each round (Section 3.2), and\\nshow the effectiveness of adaptive merging rather than\\nonly merging once (Section 4.2).\\n• We alleviate the side effect of noisy labels via a sieving\\ntechnique (Section 3.3), and demonstrate especially ef-\\nficient under large superpixels (Section 4.2).\\n• In various realistic experiments, we demonstrate the\\nconsistent improvement of the proposed AL frame-\\nwork, consisting of the adaptive merging and sieving\\nmethods with the dedicated acquisition function, over\\nexisting ones (Section 4.2).\\n• We provide an insightful analysis on proper superpix-\\nels for AL with the new evaluation metric of superpixel\\nalgorithms being aware of usage in AL (Section 5.1).\\n2. Related work\\nActive learning for segmentation. To reduce the label-\\ning cost of semantic segmentation, active learning for seg-\\nmentation selectively collects labels among unlabeled sam-\\nples, and they utilize different predefined labeling units.\\nEarly approaches [34, 40] perform image-wise selection\\nand mask labeling. Patch-based methods [5, 7, 16, 25, 37]\\ndivide images into rectangular patches and provide mask\\nlabel [5, 16, 37] or polygon overlay of an object [7, 25]\\nwithin the selected patch. Recently, superpixel-based ap-\\nproaches [4, 33] split images to perceptually meaningful\\nregions called superpixel by running an off-the-shelf over-\\nsegmentation algorithm [1, 28, 35]. Each superpixel is la-\\nbeled with a single dominant class, and thus it can be ob-\\ntained efficiently [4], while a label noise may occur depend-\\ning on the quality of the superpixel. We present a new ef-\\nficient labeling unit, that is initialized with the superpixel\\nbut its quality continuously improves by the proposed merg-\\ning algorithm. To the best of our knowledge, the proposed\\nmethod is the first approach to improve the labeling units\\nduring active learning for segmentation.\\nLearning from noisy labels for segmentation. Consid-\\nering the difficulty in acquiring high-quality labels [8],\\nsemantic segmentation often suffers from noisy annota-\\ntions.\\nPrevious studies address the label noise by using\\ngradient similarity to the clean label [41], structural con-\\nstraints [2, 21], and noise-aware loss [26, 39]. A recent ap-\\nproach captures the moment when different classes memo-\\nrize noisy labels [23]. Most of these methods [21, 26, 39]\\nutilize a single confidence threshold to detect label noise\\nwithin data. Unlike previous approaches, we propose to de-\\ntect an adaptive confidence threshold per every superpixel\\nqueried, using the Kneedle algorithm [30] (Section 3.3).\\nFiltering with the sample-adaptive threshold prevents super-\\npixels with low overall confidence or superpixels containing\\nminor classes from being ignored.\\nSuperpixel mechanisms and their evaluation metric.\\nNumerous studies segment an image into superpixels to\\nreduce the computation burden of pixels. Cut-based ap-\\nproaches [22, 32, 36, 42] create superpixels by adding mul-\\ntiple minimum cuts into a graph with pixel nodes. Other\\nmethods evolve homogeneous clusters from the initial set\\nof points [1, 20]. For real-time applications, a simple hill-\\nclimbing optimization is utilized to enforce color similar-\\nity [35]. Most of methods aim at generating superpixels of\\npredefined size or shape, and the generated superpixels are\\nevaluated by achievable segmentation accuracy and bound-\\nary recall compared with ground truth [22, 35] or by ex-\\namining the regularity in superpixel shape [15, 24, 31]. To\\nsave labeling costs in active learning, it is more important\\nto obtain superpixels as close to the ground-truth segments\\nas possible without such constraints on the shape or size of\\nsuperpixels. To this end, we propose the merging method\\n(Section 3.2), and a new evaluation metric of superpixel\\nmechanism, that also takes account of the size of ground-\\ntruth segments (Section 5.1). The proposed metric not only\\nhighlights the difference of the ideal superpixel required in\\nactive learning than that in the previous context, but also\\ngives a guideline to develop superpixel algorithms for ac-\\ntive learning.\\n3. Proposed framework\\nGiven an unlabeled image set I, we consider an active\\nlearning scenario with dominant labeling, where a query\\nasks an oracle annotator for the dominant class label D(s) ∈\\nC := {1, 2, ..., C} of an associated superpixel s, and we is-\\nsue a batch Bt of B queries for each round t. Once we\\nenquire the batch Bt, we train a model θt based on the an-\\nnotations obtained so far. Recalling a superpixel s is a clus-\\nter of neighboring pixels, the dominant labeling demands\\nmuch less annotation effort than the pixel-wise labeling on\\nevery individual pixel x in the same superpixel s or manual\\nsegmentation to indicate boundaries separating semantics.\\nThe benefit becomes greater with larger superpixels. Mean-\\nwhile, it is prone to noisy labeling as superpixels can be\\nblunt, i.e., including pixels of different semantics.\\nIn order to fully enjoy the benefit in terms of annotation\\ncost while suppressing the risk of noisy labels, our frame-\\nwork begins with a warm-up round (t = 0; Section 3.1; line\\n1-2 in Algorithm 1) to prepare an initial model from random\\nquerying and iterates subsequent rounds (t = 1, 2, . . . ) with\\nthe adaptive merging (Section 3.2; line 4-5 in Algorithm 1)\\nand sieving (Section 3.3; line 6-7 in Algorithm 1) methods\\nto evolve superpixels for dominant labeling round by round\\nand filter out annotations with the high risk of noisy labels\\ngiven the latest model. The overall procedure is summa-\\nrized in Figure 2 and Algorithm 1.\\n3.1. Warm-up round\\nThe adaptive merging and sieving methods demand a\\ntrained model. To obtain an initial model, we start with\\na canonical warm-up round, which is identical to the first\\nround of previous work [4]. We first use an off-the-shelf\\nsuperpixel algorithm, namely SEEDS [35], to partition the\\npixels in each image i ∈I into a set S0(i) of superpix-\\nels, and to produce a base segmentation S0 := S\\ni∈I S0(i).\\nQuerying a batch B0 of B superpixels randomly selected\\nfrom S0, we then train a model θ0 using the dominant la-\\nbels for B0. Specifically, to obtain θ0, we first initialize θ at\\na model pretrained on ImageNet, and then train it to mini-\\nmize the following cross-entropy (CE) loss:\\n  \\\\hat {\\\\mathbb {E}}_{( x, y) \\\\sim {\\\\mathcal {D}}_0} [ \\\\text {CE}(y, f_\\\\theta (x))] \\\\;, \\n(1)\\nwhere D0 := {(x, y) : ∃s ∈B0, x ∈s, y(c) = 1[c = D(s)]\\nAlgorithm 1 Proposed Framework\\nRequire: Image set I, batch size B, and final round T.\\n1: Produce base superpixels S0 := S\\ni∈I S0(i)\\n2: Obtain model θ0 training with D0\\n3: for t = 1, 2, . . . , T do\\n4:\\nAdaptively merge the base superpixels and obtain\\nSt ←S\\ni∈I AM(S0(i), θt−1)\\n5:\\nSelect and query B superpixels Bt ⊂St with (7)\\n6:\\nSieve s ∈St\\nt′=0 Bt′ and obtain Dt in (9)\\n7:\\nObtain model θt training with the sieved Dt\\n8: return θT\\n∀c ∈C} is the training data for round t = 0 without sieving,\\nandfθ(x) ∈R|C|isθ’s estimate of class probability on pixel\\nx.\\nWe remark that we use the initial model θ0 for round\\nt = 1. In our framework, SEEDS to generate S0 can be re-\\nplaced with any other unless S0 is a fair over-segmentation\\nof semantics with a low risk of noisy labeling while partially\\nenjoying the benefit of low annotation cost. We note that\\nSEEDS clusters neighboring pixels of similar colors while\\na semantic consists of multiple colors, typically. SEEDS,\\nready-to-use in OpenCV [3], easily provides the desired\\nover-segmentation [4] and a decent performance of θ0. In\\naddition, the warm-up round with SEEDS corresponds to\\nthat in existing work [4]. Hence, this also enables a fair\\ncomparison of our main contributions, i.e., adaptive merg-\\ning and sieving methods, to existing works.\\n3.2. Adaptive merging\\nIn advance of dominant labeling in round t ≥1, we\\nfirst merge the base superpixels in S0 to obtain St using\\nthe model θt−1 from the previous round. We then select a\\nbatch Bt of B superpixels from St to be annotated using an\\nacquisition function that prioritizes uncertain superpixels of\\nrare class labels. For simplicity, we often omit the subscript\\nt −1 and write θ for θt−1.\\nAdaptive merging. To obtain St := S\\ni∈I St(i), the merg-\\ning process converts base superpixels S0(i) into merged\\nones St(i) for each image i ∈I. We hence focus on how\\nwe merge given base superpixels S for an image. To begin\\nwith, we convert the superpixels S into a connected graph\\nG(S)=(S, E(S)) where S is the set of nodes, each of which\\ncorresponds to a base superpixel s ∈S, and E(S) is the\\nedge set such that (s, n) ∈E(S) if a pair of superpixels\\ns, n ∈S are adjacent. Starting from a root node s ∈S, we\\nthen merge neighboring superpixels of similar class predic-\\ntions with the root s along the breadth-first search tree. To\\nbe specific, a neighbor n is amalgamated with root s only if\\n  d\\n_\\n\\\\text  {JS} \\n\\\\\\nb i g ( f_\\\\theta (s) \\\\parallel f_\\\\theta (n) \\\\big ) < \\\\epsilon \\\\;, \\\\label {eq:jsd} \\n(2)\\nAlgorithm 2 Adaptive Merging (AM)\\nRequire: Base superpixels S, model θ, and threshold ϵ.\\n1: Set S′ ←∅and G(S) ←(S, E(S))\\n2: Mark s as unexplored for each s ∈S\\n3: for s ∈S in descending order of uθ(s) do\\n4:\\nif s is unexplored then\\n5:\\nS′ ←S′ ∪{MERGE(s, fθ(s); G, θ)}\\n6: return S′\\n7: procedure MERGE(s, f; G, θ)\\n8:\\nMark s as explored and set s′ ←s\\n9:\\nfor each neighbor n of s in G do\\n10:\\nif n is unexplored and dJS(f ∥fθ(n)) < ϵ then\\n11:\\ns′ ←s′ ∪MERGE(n, f; G, θ)\\n12:\\nreturn s′\\nwhere fθ(s) :=\\nP\\nx∈s fθ(x)\\n|{x:x∈s}| is the averaged class prediction\\nof superpixel s ∈S, and dJS is a symmetric measure of\\ndiscrepancy between two distributions, namely the square\\nroot of Jensen-Shannon (JS) divergence. More formally,\\n  d_\\\\ t ex t \\n{\\nJS}(p  \\\\pa\\nr a l lel q )  :=\\n \\\\\\ns\\nqrt {\\\\frac {d_\\\\text {KL}(p \\\\parallel \\\\frac {p+q}{2}) + d_\\\\text {KL}(q \\\\parallel \\\\frac {p+q}{2})}{2}} \\\\;, \\n(3)\\nwhere dKL is the Kullback-Leibler divergence. Once every\\nnode has been either merged to a root or played as a root,\\nwe collect the merged superpixels into St(i). The merging\\nprocess is formally described in Algorithm 2.\\nRecalling (2) and the fact that dJS is a distance metric,\\nwe can guarantee that any pair of superpixels s and n has\\nthe prediction discrepancy at most 2ϵ and thus similar un-\\ncertainty and predicted label if they are merged. Hence, the\\nthreshold ϵ governs the impurity of predictions in a merged\\nsuperpixel. We also remark that the merging process is fully\\ndedicated to collecting pixels of similar predictions as a part\\nof saving the annotation budget for querying similar pixels\\nrepeatedly. Hence, the merged superpixels can have var-\\nious sizes differently from existing superpixel algorithms\\nthat regularize the superpixel size to be even [15, 24, 31].\\nAcquisition function. From the merged superpixels St, we\\nthen select a batch Bt ⊂St of size B to be labeled, ac-\\ncording to an acquisition function that estimates the benefit\\nfrom labeling a merged superpixel, where the benefit would\\nbe huge for uncertain superpixels of rare class labels. In\\nwhat follows, we define an uncertainty measure of super-\\npixel in (5) and a popularity estimate of class in (6), and\\nthen introduce an acquisition function in (7).\\nRecalling fθ(x) ∈R|C| is the probability such that\\nfθ(c; x) is the estimated probability that the class c of pixel\\nx, we adapt best-versus-second-best [17] for uncertainty\\nmeasures of pixel x and superpixel s as follows:\\n  u_\\\\ th eta (x) &:= \\\\f rac { \\\\m\\nax _{c  \\\\in \\\\m\\na\\nthc\\nal {C } \\n\\\\\\nset minus\\n \\\\{ y _ \\\\ the ta (x)\\\\}} {f_\\\\theta (c; x)}}{\\\\max _{c \\\\in \\\\set {C} }f_\\\\theta (c;x)}\\\\;, \\\\\\\\ u_\\\\theta (s) &:= \\\\frac {\\\\sum _{x \\\\in s} u_\\\\theta (x)}{|\\\\{x: x \\\\in s\\\\}|} \\\\;, \\\\label {eq:uncertainty}\\n(5)\\nwhere yθ(x) := arg maxc∈C fθ(c; x) is the estimated dom-\\ninant label of pixel x in a given model θ.\\nWe then define a popularity estimate p(c; θ) of class c ∈\\nC given θ as follows:\\n  p( c; \\\\t het a  )  := \\\\frac  {| \\\\ {  x \\n: \\\\ e xi s ts s  \\\\in\\n \\\\mathcal {S}_t, \\\\text {D}_\\\\theta (s) = c, x \\\\in s\\\\}|}{|\\\\{x : \\\\exists s \\\\in \\\\mathcal {S}_t, x \\\\in s \\\\}|} \\\\;, \\\\label {eq:size-aware-class-balance} \\n(6)\\nwhere Dθ(s) := arg maxc∈C |{x ∈s : yθ(x) = c}| is the\\nmajority of predicted labels in superpixel s. We note that\\nlow p(c; θ) implies that class c is rare in the prediction of\\nθ. It is noteworthy that we compute the class popularity in\\npixel-level due to the various sizes of our merged superpix-\\nels, while the previous work [4] proposes a superpixel-wise\\nclass popularity, |{s:Dθ(s)=c,s∈St}|\\n|{s:s∈St}|\\n, assuming superpixels\\nof uniform size.\\nUsing the uncertainty uθ(s) in (5) and the class pop-\\nularity p(c; θ) in (6), we define the following acquisition\\nfunction a(s; θ) prioritizing uncertain superpixels of rare\\nclasses:\\n  a( s;  \\\\ theta  ):\\n=\\n u_\\\\theta  (\\ns\\n) \\\\exp \\\\big ({-p(\\\\text {D}_\\\\theta (s) ; \\\\theta )} \\\\big )\\\\;. \\\\label {acquisition_function} \\n(7)\\nWe select B superpixels of highest values of a(s; θt−1)\\nfrom the merged St for query batch Bt.\\nRemarks. We note that it is possible to produce St from\\nscratch rather than from base segmentation S0.\\nTo re-\\nduce the computational cost for the adaptive merging pro-\\ncess, we however compose St by merging base superpixels\\nin S0 from SEEDS, which is known to generate an over-\\nsegmentation of semantics. Moreover, it is computation-\\nally expensive to explore all the possible mergers and ob-\\ntain St followed by the query selection. We hence conduct\\nthe merging process only for a certain portion of base super-\\npixels with the highest values of uncertainty (c.f., line 3 in\\nAlgorithm 2) and then select Bt to be queried since the ac-\\nquisition function would select merged superpixels of high\\nuncertainty in the end. Further details are presented in Ap-\\npendix B.\\n3.3. Sieving\\nDespite the sophisticated design of the adaptive merging,\\na queried superpixel can inevitably include pixels of classes\\ndifferent from the dominant one, in particular, as we select\\nsuperpixels of which model predictions are unsure. Hence,\\nthe dominant labeling is liable to make noisy annotations.\\nTo alleviate such side effects of the dominant labeling, we\\npropose a simple sieving technique that filter out pixels that\\nhave high potential risks of being different classes than the\\ndominant one. We observe that for a queried superpixel s\\nand given model θ, the risk of mismatch between the dom-\\ninant label D(s) and the true label of pixel x ∈s would be\\nhigh when fθ\\n\\x00D(s); x\\n\\x01\\nis low. From this observation, we\\ndefine\\n  h( s; \\\\t he t a  ) \\n:\\n= \\\\{ x\\n \\n\\\\ in s : f _\\\\theta \\\\big ( \\\\text {D}(s); x \\\\big ) \\\\geq \\\\phi (s; \\\\theta ) \\\\} \\\\;, \\\\label {eq:sieving} \\n(8)\\nwhere ϕ(s; θ) is a knee point of the cumulative distribution\\nfunction of values of fθ\\n\\x00D(s); x\\n\\x01\\nin superpixel s, detected\\nby Kneedle algorithm [30]. In addition, the knee point de-\\ntection allows us to have a tailored sieving threshold to each\\nsuperpixel. This is important to avoid the case that the re-\\nmained pixels are heavily biased to relatively easy labels\\nafter sieving. Further details are in Appendix C.\\nWe revisit all the queried superpixel s ∈St\\nt′=0 Bt′ and\\nsieve them using (8) with the latest model θt−1 since the\\nmodel evolves round by round. We finally obtain the fol-\\nlowing sieved dataset Dt for round t ≥1:\\n  {\\\\\\nm\\nath ca l  { D }}\\n_{t} := \\\\ l eft \\\\{(x, \\ny) :  \\\\be g in {a li g n\\ne\\nd} &\\\\exists s \\\\in \\\\cup _{t'=0}^{t} \\\\mathcal {B}_{t'}, \\\\ x \\\\in h(s;\\\\theta _{t-1}), \\\\\\\\ &y(c) = \\\\mathbbm {1}{[c = \\\\text {D}(s)]} \\\\ \\\\forall c\\\\in \\\\set {C} \\\\end {aligned} \\\\right \\\\} \\\\;. \\\\label {sieved-superpixel} \\n(9)\\nAnalogously to the warm-up round, initializing model θ at\\na model pretrained on ImageNet, we obtain θt trained to\\nmainly minimize the following CE loss:\\n  \\\\hat {\\\\mathbb {E}}_{( x,y) \\\\sim {\\\\mathcal {D}}_t} [ \\\\text {CE}(y, f_\\\\theta (x))] \\\\;. \\\\label {eq:final-loss} \\n(10)\\n4. Experiments\\n4.1. Experimental setup\\nDatasets.\\nWe use two semantic segmentation datasets:\\nCityscapes [8] and PASCAL VOC 2012 (PASCAL) [13].\\nCityscapes comprises 2,975 training and 500 validation im-\\nages with 19 classes, while PASCAL consists of 1,464 train-\\ning and 1,449 validation images with 20 classes.\\nImplementation details. We adopt DeepLab-v3+ architec-\\nture with Xception-65 [6] as our segmentation backbone.\\nDuring training, we use the SGD optimizer with a momen-\\ntum of 0.9 and set a base learning rate to 7e-3. We decay\\nthe learning rate by polynomial decay with a power of 0.9.\\nFor Cityscapes, we resize training images to 769 × 769 and\\ntrain a model for 60k iterations with a mini-batch size of 4.\\nSimilarly, for PASCAL, we resize training images to 513 ×\\n513 and train a model for 30k iterations with a mini-batch\\nsize of 12. Unless specified, we set the value of ϵ to 0.1.\\nBaseline methods. We compare our algorithm to SP [4],\\nthe state-of-the-art superpixel-based active segmentation\\nmethod. Our algorithm applies two proposed processes in\\neach round: merging and sieving. We call our complete\\n100k\\n150k\\n200k\\n250k\\n64\\n66\\n68\\n70\\n72\\n74\\nThe number of clicks\\nmIoU (%)\\nOracle\\nAMSP+S (Ours)\\nMSP+S\\nSP [4]\\n(a) Cityscapes\\n10k\\n15k\\n20k\\n25k\\n60\\n62\\n64\\n66\\n68\\n70\\nThe number of clicks\\nmIoU (%)\\n(b) PASCAL\\n64\\n256\\n1024\\n4096\\n56\\n58\\n60\\n62\\n64\\n66\\n68\\nSize of base superpixels\\nmIoU (%)\\nOracle\\nAMSP+S (Ours)\\nSP+S\\nSP [4]\\n(c) Cityscapes\\n4\\n16\\n64\\n256\\n54\\n56\\n58\\n60\\n62\\n64\\n66\\nSize of base superpixels\\nmIoU (%)\\n(d) PASCAL\\nFigure 3: Effect of adaptive superpixels. (a, b) mIoU versus the number of clicks as budget. (c, d) mIoU versus the size of\\nbase superpixels. Each experiment is conducted with three trials and the shaded region indicates ranges.\\nmethod including adaptive merging as AMSP+S, while the\\npartial version that only uses the sieving without the merg-\\ning is called SP+S. Additionally, we evaluate the modified\\nversion of our method that performs merging only once in\\nthe second round, called MSP+S. Note that AMSP+S and\\nMSP+S are identical until the second round.\\nOracle baseline. The adaptive superpixel aims to merge\\nevery connected region with the same class labels. Thus,\\nthe upper bound of it is to consider each region separated\\nby the ground truth mask as a superpixel. We refer to such\\nideal regions as oracle superpixels in Figure 4c. An active\\nlearning model trained using the oracle superpixels is called\\nOracle. Details are in Appendix D. As the number of ora-\\ncle superpixels is limited, all of them are eventually labeled\\nas the round progresses, and the performance of the trained\\nmodel becomes equivalent to that of the pixel-wise fully su-\\npervised model. We report 100% and 90% of the Oracle\\nperformance for Cityscapes and PASCAL, respectively.\\nEvaluation protocol. We set the average size of the su-\\nperpixels to 256 and 64 pixels on Cityscapes and PASCAL,\\nrespectively, for all experiments except for one where we\\nadjust the size. Following SP [4], we use the number of\\nclicks as the labeling budget. We conduct 5 rounds of data\\nsampling, where we allocate a budget of 50k and 5k for\\neach round on Cityscapes and PASCAL, respectively. In the\\nfirst round, we randomly select superpixels to train a model,\\nensuring that all methods start at the same performance.\\nWe evaluate the trained model with mean Intersection-over-\\nUnion [11] on the validation images. We emphasize that\\nthe average size of superpixels containing 64 pixels is more\\nefficient on Cityscapes, as detailed in Appendix A.\\n4.2. Effect of adaptive superpixels\\nMulti-round scenario. In Figures 3a and 3b, we compare\\nthe performance of the proposed method to SP [4] vary-\\ning budget for both of Cityscapes and PASCAL. Note that\\nthe performance for round 0, i.e., 50K budget, is omitted\\nas each method has the same performance at the warm-\\nup round. The results show that our adaptive superpixel\\n(AMSP+S) clearly outperforms the previous art in every\\nbudget setting on both of the datasets. In particular, the\\nAMSP+S with only 150k clicks outperforms the previous\\nart with 250k clicks in Cityscapes. In the final round, the\\nproposed method recovers 97% and 92% of the Oracle per-\\nformance for Cityscapes and PASCAL, respectively.\\nTo\\nshow the effectiveness of our adaptive approach, we com-\\npare AMSP+S to its one-shot merging version MSP+S in\\nFigures 3a and 3b. On both datasets, adaptive feature of\\nAMSP+S shows performance gain especially for the last\\ntwo rounds.\\nThe experiments conducted for additional\\nrounds can be found in Appendix A.\\nMulti-size scenario. The size of superpixels is an essen-\\ntial hyperparameter in superpixel-based AL, affecting both\\nthe quantity and quality of labels. In Figures 3c and 3d, we\\ncompare the proposed method to SP [4] varying the base\\nsuperpixel size for both of Cityscapes and PASCAL, in the\\nsecond round. Our adaptive superpixel (AMSP+S) outper-\\nforms the previous art in various superpixel sizes on both of\\nthe datasets. We also evaluate sieving only version (SP+S)\\nof our method, which quantifies contribution of each com-\\nponents in our method. The performance improvement be-\\ntween SP and SP+S shows our sieving is especially help-\\nful for large superpixels, and the performance gap between\\nSP+S and AMSP+S shows our merging is especially effec-\\ntive for small superpixels. Thanks to the proposed sieving\\nand merging, AMSP+S are comparably robust to the change\\nof the superpixel size than SP.\\nQualitative results. The quality of the proposed adaptive\\nsuperpixel is illustrated in Figure 4. As shown in Figure 4a,\\nsuperpixels used in the previous study [4] have uniform\\nsizes for all areas regardless of their content. In contrast,\\nFigure 4b demonstrates that adaptive superpixels accurately\\n(a) Base superpixels [35]\\n(b) Merged superpixels (Ours)\\n(c) Oracle superpixels\\nFigure 4: Qualitative results of adaptive superpixels. (a) Base superpixel generated by SEEDS [35] with size 256. (b)\\nSuperpixels generated with proposed adaptive merging at round 4. (c) Oracle superpixels generated from the ground truth.\\nreflect the actual size of the content in images, carefully cap-\\nturing small object classes while efficiently covering large\\nbackground classes. More examples are in Appendix F.\\n5. Analyses of adaptive superpixels\\nWe propose new evaluation metrics to measure the qual-\\nity of superpixel as a labeling unit for active segmenta-\\ntion, and utilize it to analyze our adaptive superpixels (Sec-\\ntion 5.1). We also conduct analyses about the effect ϵ to our\\nadaptive superpixels (Section 5.2). All analyses are con-\\nducted on Cityscapes with an average superpixel size of 256\\npixels.\\n5.1. Achievable metrics\\nWhile various evaluation metrics for superpixel are pre-\\nsented [15, 22, 24, 31, 35], most of them aims to measure\\nthe quality of over-segmentation. For instance, achievable\\nsegmentation accuracy (ASA) [22] measures the segmenta-\\ntion accuracy when each superpixel s ∈S is associated with\\nthe oracle superpixel with the largest overlap. The ASA is\\ncalculated as follows:\\n  \\\\tex t {A\\nS\\nA}( S; G) :=  \\\\f\\nr\\nac {\\\\s\\num _{s \\\\in S} \\\\max _{g \\\\in G} |s \\\\cap g|}{\\\\sum _{s \\\\in S} |s|} \\\\;, \\n(11)\\nwhere S and G represent the generated and oracle super-\\npixels from the same image, respectively. As an image be-\\ncomes more over-segmented, i.e., the superpixel size be-\\ncomes smaller, the ASA value increases. However, active\\nlearning (AL) aims to achieve the maximum benefit with\\nthe least amount of labeling effort, and therefore, the num-\\nber of labels should be taken into account. In addition, the\\nASA is heavily biased towards classes with a large number\\nof pixels.\\nIn order to measure the suitability of superpixels for AL,\\nwe introduce precision and recall between generated and\\noracle superpixels. A generated superpixel can be viewed\\nas positive on the inside and negative on the outside, and\\nits precision and recall with respect to the corresponding\\noracle one can be calculated. For all generated superpixels,\\nwe define the achievable precision (AP) as follows:\\n  \\\\te xt  { A\\nP}(\\nS\\n;G)\\n := \\\\f ra c  {\\n1}{\\n|S|} \\\\sum _{s \\\\in S} \\\\frac {\\\\max _{g \\\\in G} |s \\\\cap g|}{|s|} \\\\;, \\n(12)\\nwhere the summation is performed in superpixels, unlike in\\nASA, which implies pixel-wise precision. As we put the\\nsame weight on each superpixel, AP is less influenced by\\nlarge objects than ASA. We note that AP is different to av-\\nerage precision [12, 29], used in object detection, which\\nutilize the precision and recall curve. We also define the\\nachievable recall (AR) and F1-score (AF) as:\\n  \\\\te xt  { A\\nR}(\\nS\\n;G)\\n := \\\\f ra c  {\\n1}{|S| } \\\\\\nsum _{s \\\\in S} \\\\frac {\\\\max _{g \\\\in G} |s \\\\cap g|}{|g'(s; G)|} \\\\;, \\n(13)\\n  \\\\te xt  { A\\nF}(\\nS\\n;G)\\n := \\\\f ra c  {\\n2}{ | S|} \\\\s um _{s \\\\in S} \\\\frac {\\\\max _{g \\\\in G} |s \\\\cap g|}{|s| + |g'(s; G)|} \\\\;, \\n(14)\\nwhere g′(s; G) := arg maxg∈G |s ∩g| refers to the corre-\\nsponding oracle superpixel. Details are in Appendix E.\\nAll the metrics evaluate generated superpixels in com-\\nparison to oracle ones. However, the size of superpixels is\\nalso important besides their quality in AL. Therefore, it is\\nnecessary to evaluate the oracle superpixels against the gen-\\nerated superpixels, i.e., ASA(G; S), AP(G; S), AR(G; S)\\nand AF(G; S). We hence propose AF(G; S) defined as:\\n  \\\\te xt  {\\nA\\nF}(\\nG\\n;S)\\n := \\\\f ra c  {\\n2}{ | G|} \\\\s um _{g \\\\in G} \\\\frac {\\\\max _{s \\\\in S} |g \\\\cap s|}{|g| + |s'(g; S)|} \\\\;, \\n(15)\\nMethods\\nASA(S; G)\\nASA(G; S)\\nAP(S; G)\\nAR(S; G)\\nAF(S; G)\\nAP(G; S)\\nAR(G; S)\\nAF(G; S)\\nmIoU\\nSLIC4096\\n0.887\\n0.082\\n0.897\\n0.046\\n0.066\\n0.695\\n0.259\\n0.185\\n53.18\\nSEEDS4096\\n0.909\\n0.082\\n0.900\\n0.050\\n0.070\\n0.665\\n0.309\\n0.221\\n57.61\\nSLIC256\\n0.956\\n0.013\\n0.958\\n0.007\\n0.012\\n0.400\\n0.622\\n0.278\\n58.04\\nSEEDS256\\n0.961\\n0.014\\n0.960\\n0.007\\n0.012\\n0.395\\n0.647\\n0.297\\n58.97\\nMerged2\\n0.898\\n0.515\\n0.883\\n0.042\\n0.063\\n0.553\\n0.472\\n0.333\\n60.00\\nMerged4\\n0.898\\n0.496\\n0.883\\n0.042\\n0.062\\n0.548\\n0.484\\n0.340\\n61.36\\nMerged∗\\n0.899\\n0.597\\n0.880\\n0.045\\n0.066\\n0.547\\n0.510\\n0.359\\n61.85\\nOracle\\n1.000\\n1.000\\n1.000\\n1.000\\n1.000\\n1.000\\n1.000\\n1.000\\n70.81\\nTable 1: Evaluation metrics of superpixels. The subscript indicates the average size of the superpixel for SLIC [1] and\\nSEEDS [35], while it indicates the round for Merged. Merged∗indicates superpixel merged by a model trained with full\\nsupervision. To compute the mIoU, we train a model with 100k randomly selected superpixels.\\n0.9\\n0.92\\n0.94\\n0.96\\n51\\n53\\n55\\n57\\n59\\n61\\n63\\nCorr = 0.05\\nASA(S; G)\\nmIoU (%)\\n0.2\\n0.25\\n0.3\\n0.35\\n51\\n53\\n55\\n57\\n59\\n61\\n63\\nCorr = 0.95\\nAF(G; S)\\nmIoU (%)\\nFigure 5: Relationship between metrics and mIoU. The cor-\\nrelation between ASA(S; G) and mIoU is low, while the\\ncorrelation between AF(G; S) and mIoU is high. For the\\ncorrelation calculation, Oracle in Table 1 is excluded.\\nwhere s′(g; S) := arg maxs∈S |g ∩s| refers to the gener-\\nated superpixel with the highest overlap, which is linked to\\nthe maximum amount of labeling we receive. Table 1 eval-\\nuates various superpixels through eight metrics. Although\\nour merged superpixels have a relatively low ASA(S; G),\\nthey exhibit high ASA(G; S) and AF(G; S).\\nCorrelation of metrics and mIoU. To show the proposed\\nmetric can accurately evaluate the superpixel quality for\\nactive segmentation, we measure the correlation between\\nvarious evaluation metric and the performance of actively\\nlearned model in Table 1 and Figure 5. We observe that\\nthe proposed AF(G; S) shows the highest correlation to\\nthe performance of the actively learned model. We except\\nAF(G; S) can select suitable superpixel algorithm for active\\nlearning, where the details are provided in Appendix E.\\n5.2. Ablation studies on epsilon\\nEpsilon sensitivity. In Figures 6a and 6b, we evaluate the\\nsensitivity of our method to ϵ, which determines the amount\\nof the merging. Proposed method show robustness to the\\nchange of ϵ, where the change of mIoU is less than 2%\\nfor both Citycapes and PASCAL when ϵ is between 0.05\\nand 0.2. We observe that for every investigated ϵ values,\\nAMSP+S surpasses the performance of the previous art [4].\\nAdaptive epsilon. We fix ϵ to 0.10 in all quantitative exper-\\niments, but there may exist an optimal ϵ for each round. In\\nTable 2, we analyze ϵ that maximizes AF(G; S) metric for\\neach round by assuming the existence of 10 validation im-\\nages with ground truth. As the round increases, the optimal\\nϵ increases as well, which implies that the improvement of\\nthe model enables us to merge aggressively.\\nEffect of epsilon. Table 3 presents the quality of the merg-\\ning algorithm under various criteria, defining correctness\\nbased on the agreement of dominant labels in paired super-\\npixels. We merge a pair of superpixels when their ground-\\ntruth label is identical (Ground Truth), when their domi-\\nnant top-1 model prediction is identical (Pseudo Label), and\\nwhen the Euclidean Distance (ED) or Jensen-Shannon Di-\\nvergence (JSD) of their averaged predictive probability is\\nsmaller than ϵ. Using pseudo labels leads to lower-quality\\nmerging as it ignores other minor classes. Since we uti-\\nlize the predicted class probability as a feature space, JSD\\nproves to be more effective than ED. As ϵ increases, the\\ncorrect ratio decreases due to the aggressive merging of su-\\nperpixels. We emphasize that ϵ can determine the trade-off\\nbetween the quantity and quality of labels.\\n5.3. Implementation remarks for practitioners\\nFast merging. The completion of the merging process for\\nan image essentially requires a linear time complexity in the\\nnumber of the base superpixels. However, to reduce this,\\nthe complete merging can be replaced with a partial merg-\\ning that scans only a subset of base superpixels with high\\nuncertainties as roots. This is considerable since we will\\neventually query only a subset of the merged superpixels\\naccording to the acquisition function (7), which prioritizes\\nthose with high uncertainties. In Table 4, we compare the\\ncomplete and partial mergings in terms of the mIoU at 100k\\n0.05\\n0.1\\n0.15\\n0.2\\n63\\n64\\n65\\n66\\n67\\nϵ\\nmIoU (%)\\nAMSP+S\\nSP\\n(a) Cityscapes\\n0.05\\n0.1\\n0.15\\n0.2\\n61\\n62\\n63\\nϵ\\nmIoU (%)\\n(b) PASCAL\\nFigure 6: Epsilon sensitivity. We experiment on superpix-\\nels with varying ϵ and demonstrate the robustness of our\\nAMSP+S, while SP is independent of ϵ. Each experiment is\\nconducted on the second round.\\nEpsilon\\n0.04\\n0.05\\n0.06\\n0.07\\n0.08\\nMerged1\\n0.344\\n0.346\\n0.344\\n0.340\\n0.336\\nMerged2\\n0.347\\n0.346\\n0.348\\n0.345\\n0.344\\nMerged3\\n0.346\\n0.349\\n0.350\\n0.351\\n0.349\\nMerged4\\n0.347\\n0.347\\n0.347\\n0.348\\n0.346\\nTable 2: Adaptive epsilon. AF(G; S) for adaptive superpix-\\nels generated by varying ϵ is reported. The subscript indi-\\ncates the round.\\nclicks. As expected, the partial merging on base superpix-\\nels with top-10% uncertainty has only a small gap to the\\ncomplete merging. In addition, we find that by employing\\nthe partial merging, the time complexity is reduced signifi-\\ncantly by a factor of 25.98 1. The partial merging is a useful\\nsuggestion to save computation resource for practitioners.\\nA further investigation and discussion on the partial merg-\\ning are presented in Appendix B.\\nCompatibility with other base superpixels.\\nFor a fair\\ncomparison to the previous study [4], we have employed\\nthe same superpixel algorithm called SEEDS [35] to gener-\\nate base superpixels at the beginning. However, our merg-\\ning and sieving processes can be applied on top of any other\\nbase superpixels. Indeed, in Figure 3c (and Appendix A),\\nour method consistently shows gains over SP [4] when us-\\ning base superpixels of different sizes. Furthermore, we\\ncompare SP and AMSP+S (ours) when using SLIC in-\\nstead of SEEDS in the same setting of Figure 3a, where\\nthe mIoU’s at 100k clicks of SP and ours have 65.97% and\\n67.56%, respectively, i.e., the merging and sieving are also\\neffective with SLIC as they were with SEEDS. We believe\\nthat our proposed method can work with any base superpix-\\n1The per-image runtime of the merging process (CPU-intensive) for\\nCityscapes is reduced from 12.42s to 0.48s on a server with two AMD\\nEPYC 7513 32-core processors.\\nMethod\\nEpsilon\\nCorrect\\nIncorrect\\nGround Truth\\n-\\n1.000\\n0.000\\nPseudo Label\\n-\\n0.832\\n0.168\\nED\\n0.05\\n0.915\\n0.085\\n0.10\\n0.901\\n0.099\\n0.15\\n0.891\\n0.109\\nJSD\\n0.05\\n0.934\\n0.066\\n0.10\\n0.911\\n0.089\\n0.15\\n0.896\\n0.104\\nTable 3: Various merging criteria. Using JSD performs\\nmore accurate merging than using ED. As ϵ increases, the\\nrate of incorrect merging increases.\\nMethods\\nmIoU\\nSP [4]\\n63.77\\nAMSP+S (top 10%)\\n65.99\\nAMSP+S (complete 100%)\\n66.53\\nTable 4: Various levels of partial merging. Experiments are\\nconducted under the same setting of Figure 3a with 100k\\nclicks (Cityscapes, superpixel size of 256).\\nels even when they are from an unsupervised segmentation\\nmethod [18] or a foundation model [19].\\n6. Conclusion\\nIn this work, we propose an adaptive active learning\\nframework with adaptive superpixels.\\nOur merging and\\nsieving methods operate adaptively every round, and the\\nexperimental results demonstrate the performance improve-\\nment of adaptive merging in various realistic situations.\\nFurthermore, we suggest novel achievable metrics for eval-\\nuating superpixels in advance that are suitable for active\\nlearning.\\nAcknowledgement. This work was supported by the IITP\\ngrants and the NRF grant funded by Ministry of Sci-\\nence and ICT, Korea (IITP-2019-0-01906, Artificial Intelli-\\ngence Graduate School Program (POSTECH); IITP-2021-\\n0-02068, Artificial Intelligence Innovation Hub; NRF-\\n2021M3E5D2A01023887; NRF-2018R1A5A1060031).\\nReferences\\n[1] Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien\\nLucchi, Pascal Fua, and Sabine S¨usstrunk. Slic superpix-\\nels compared to state-of-the-art superpixel methods. IEEE\\ntransactions on pattern analysis and machine intelligence,\\n34(11):2274–2282, 2012. 2, 3, 8\\n[2] David Acuna, Amlan Kar, and Sanja Fidler.\\nDevil is in\\nthe edges: Learning semantic boundaries from noisy annota-\\ntions. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition, pages 11075–11083,\\n2019. 3\\n[3] G. Bradski. The OpenCV Library. Dr. Dobb’s Journal of\\nSoftware Tools, 2000. 4, 15\\n[4] Lile Cai, Xun Xu, Jun Hao Liew, and Chuan Sheng Foo.\\nRevisiting superpixels for active learning in semantic seg-\\nmentation with realistic annotation costs. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 10988–10997, 2021. 1, 2, 3, 4, 5, 6, 8, 9,\\n12, 13\\n[5] Arantxa Casanova, Pedro O Pinheiro, Negar Rostamzadeh,\\nand Christopher J Pal. Reinforced active learning for im-\\nage segmentation. In International Conference on Learning\\nRepresentations, 2019. 1, 2, 3\\n[6] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian\\nSchroff, and Hartwig Adam. Encoder-decoder with atrous\\nseparable convolution for semantic image segmentation. In\\nProceedings of the European conference on computer vision\\n(ECCV), pages 801–818, 2018. 5\\n[7] Pascal Colling, Lutz Roese-Koerner, Hanno Gottschalk, and\\nMatthias Rottmann.\\nMetabox+: A new region based ac-\\ntive learning method for semantic segmentation using pri-\\nority maps. arXiv preprint arXiv:2010.01884, 2020. 3\\n[8] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo\\nRehfeld,\\nMarkus Enzweiler,\\nRodrigo Benenson,\\nUwe\\nFranke, Stefan Roth, and Bernt Schiele.\\nThe cityscapes\\ndataset for semantic urban scene understanding.\\nIn Proc.\\nof the IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR), 2016. 1, 3, 5\\n[9] Chengliang Dai, Shuo Wang, Yuanhan Mo, Kaichen Zhou,\\nElsa Angelini, Yike Guo, and Wenjia Bai. Suggestive an-\\nnotation of brain tumour images with gradient-guided sam-\\npling. International Conference on Medical Image Comput-\\ning and Computer Assisted Intervention, 2020, 2020. 1\\n[10] Shasvat Desai and Debasmita Ghose.\\nActive learning for\\nimproved semi-supervised semantic segmentation in satel-\\nlite images. In Proceedings of the IEEE/CVF Winter Con-\\nference on Applications of Computer Vision (WACV), pages\\n553–563, 2022. 1\\n[11] Mark Everingham, SM Ali Eslami, Luc Van Gool, Christo-\\npher KI Williams, John Winn, and Andrew Zisserman. The\\npascal visual object classes challenge: A retrospective. In-\\nternational journal of computer vision, 111:98–136, 2015. 1,\\n6\\n[12] Mark Everingham, Luc Van Gool, Christopher KI Williams,\\nJohn Winn, and Andrew Zisserman. The pascal visual object\\nclasses (voc) challenge. International journal of computer\\nvision, 88:303–308, 2009. 7\\n[13] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,\\nand A. Zisserman.\\nThe PASCAL Visual Object Classes\\nChallenge 2012 (VOC2012) Results.\\nhttp://www.pascal-\\nnetwork.org/challenges/VOC/voc2012/workshop/index.html.\\n5\\n[14] Sean Gillies et al. Shapely: manipulation and analysis of\\ngeometric objects, 2007. 15\\n[15] R´emi Giraud, Vinh-Thong Ta, and Nicolas Papadakis. Ro-\\nbust shape regularity criteria for superpixel evaluation. In\\n2017 IEEE International Conference on Image Processing\\n(ICIP), pages 3455–3459. IEEE, 2017. 3, 4, 7\\n[16] S. Alireza Golestaneh and Kris M. Kitani. Importance of\\nself-consistency in active learning for semantic segmenta-\\ntion. British Machine Vision Conference (BMVC), 2020. 3\\n[17] Ajay J Joshi, Fatih Porikli, and Nikolaos Papanikolopoulos.\\nMulti-class active learning for image classification. In 2009\\nieee conference on computer vision and pattern recognition,\\npages 2372–2379. IEEE, 2009. 4\\n[18] Tsung-Wei Ke, Jyh-Jing Hwang, Yunhui Guo, Xudong\\nWang, and Stella X Yu. Unsupervised hierarchical semantic\\nsegmentation with multiview cosegmentation and clustering\\ntransformers. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition, pages 2571–\\n2581, 2022. 9\\n[19] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\\nthing. arXiv preprint arXiv:2304.02643, 2023. 9\\n[20] Alex Levinshtein, Adrian Stere, Kiriakos N. Kutulakos,\\nDavid J. Fleet, Sven J. Dickinson, and Kaleem Siddiqi. Tur-\\nbopixels: Fast superpixels using geometric flows.\\nIEEE\\nTransactions on Pattern Analysis and Machine Intelligence,\\npages 2290–2297, 2009. 3\\n[21] Shuailin Li, Zhitong Gao, and Xuming He.\\nSuperpixel-\\nguided iterative learning from noisy labels for medical image\\nsegmentation. In Medical Image Computing and Computer\\nAssisted Intervention–MICCAI 2021:\\n24th International\\nConference, Strasbourg, France, September 27–October 1,\\n2021, Proceedings, Part I 24, pages 525–535. Springer,\\n2021. 3\\n[22] Ming-Yu Liu, Oncel Tuzel, Srikumar Ramalingam, and\\nRama Chellappa. Entropy rate superpixel segmentation. In\\nProceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition (CVPR), pages 2097–2104, 2011.\\n2, 3, 7\\n[23] Sheng Liu, Kangning Liu, Weicheng Zhu, Yiqiu Shen, and\\nCarlos Fernandez-Granda. Adaptive early-learning correc-\\ntion for segmentation from noisy annotations. In Proceed-\\nings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition, pages 2606–2616, 2022. 3\\n[24] Va¨ıa Machairas, Etienne Decenci`ere, and Thomas Walter.\\nWaterpixels: Superpixels based on the watershed transfor-\\nmation. In 2014 IEEE International Conference on Image\\nProcessing (ICIP), pages 4343–4347. IEEE, 2014. 3, 4, 7\\n[25] Radek Mackowiak, Philip Lenz, Omair Ghori, Ferran Diego,\\nOliver Lange, and Carsten Rother.\\nCereals-cost-effective\\nregion-based active learning for semantic segmentation. In\\nBMVC, 2018. 3\\n[26] Youngmin\\nOh,\\nBeomjun\\nKim,\\nand\\nBumsub\\nHam.\\nBackground-aware\\npooling\\nand\\nnoise-aware\\nloss\\nfor\\nweakly-supervised semantic segmentation. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition, pages 6913–6922, 2021. 3\\n[27] Yu Qiao, Jincheng Zhu, Chengjiang Long, Zeyao Zhang,\\nYuxin Wang, Zhenjun Du, and Xin Yang. Cpral: Collabora-\\ntive panoptic-regional active learning for semantic segmen-\\ntation. In Proceedings of the AAAI Conference on Artificial\\nIntelligence, pages 2108–2116, 2022. 1, 2\\n[28] Xiaofeng Ren and Jitendra Malik. Learning a classification\\nmodel for segmentation. In Computer Vision, IEEE Interna-\\ntional Conference on, volume 2, pages 10–10. IEEE Com-\\nputer Society, 2003. 3\\n[29] Gerard Salton. Introduction to modern information retrieval.\\nMcGraw-Hill, 1983. 7\\n[30] Ville Satopaa, Jeannie Albrecht, David Irwin, and Barath\\nRaghavan.\\nFinding a” kneedle” in a haystack:\\nDetect-\\ning knee points in system behavior. In 2011 31st interna-\\ntional conference on distributed computing systems work-\\nshops, pages 166–171. IEEE, 2011. 3, 5, 13, 14\\n[31] Alexander Schick, Mika Fischer, and Rainer Stiefelhagen.\\nMeasuring and evaluating the compactness of superpixels. In\\nProceedings of the 21st international conference on pattern\\nrecognition (ICPR2012), pages 930–934. IEEE, 2012. 3, 4,\\n7\\n[32] Jianbo Shi and J. Malik. Normalized cuts and image segmen-\\ntation. IEEE Transactions on Pattern Analysis and Machine\\nIntelligence, pages 888–905, 2000. 2, 3\\n[33] Yawar Siddiqui, Julien Valentin, and Matthias Nießner.\\nViewal: Active learning with viewpoint entropy for semantic\\nsegmentation. In Proceedings of the IEEE/CVF conference\\non computer vision and pattern recognition, pages 9433–\\n9443, 2020. 1, 2, 3\\n[34] Samarth Sinha, Sayna Ebrahimi, and Trevor Darrell. Vari-\\national adversarial active learning.\\nIn Proceedings of the\\nIEEE/CVF International Conference on Computer Vision,\\npages 5972–5981, 2019. 1, 3\\n[35] Michael Van den Bergh, Xavier Boix, Gemma Roig, Ben-\\njamin de Capitani, and Luc Van Gool. Seeds: Superpixels\\nextracted via energy-driven sampling. In Computer Vision–\\nECCV 2012: 12th European Conference on Computer Vi-\\nsion, Florence, Italy, October 7-13, 2012, Proceedings, Part\\nVII 12, pages 13–26. Springer, 2012. 2, 3, 7, 8, 9, 17\\n[36] Olga Veksler, Yuri Boykov, and Paria Mehrani. Superpixels\\nand supervoxels in an energy optimization framework. In\\nProceedings of the 11th European Conference on Computer\\nVision: Part V, page 211–224, 2010. 3\\n[37] Binhui Xie, Longhui Yuan, Shuang Li, Chi Harold Liu, and\\nXinjing Cheng. Towards fewer annotations: Active learn-\\ning via region impurity and prediction uncertainty for do-\\nmain adaptive semantic segmentation.\\nIn Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, pages 8068–8078, 2022. 1, 3\\n[38] Shuai Xie, Zunlei Feng, Ying Chen, Songtao Sun, Chao Ma,\\nand Mingli Song. Deal: Difficulty-aware active learning for\\nsemantic segmentation. In Proceedings of the Asian Confer-\\nence on Computer Vision (ACCV), 2020. 1\\n[39] Longrong Yang, Fanman Meng, Hongliang Li, Qingbo Wu,\\nand Qishang Cheng. Learning with noisy class labels for in-\\nstance segmentation. In Computer Vision–ECCV 2020: 16th\\nEuropean Conference, Glasgow, UK, August 23–28, 2020,\\nProceedings, Part XIV 16, pages 38–53. Springer, 2020. 3\\n[40] Lin Yang, Yizhe Zhang, Jianxu Chen, Siyuan Zhang, and\\nDanny Z Chen. Suggestive annotation: A deep active learn-\\ning framework for biomedical image segmentation. In In-\\nternational conference on medical image computing and\\ncomputer-assisted intervention, pages 399–407. Springer,\\n2017. 1, 3\\n[41] Jiachen Yao,\\nYikai Zhang,\\nSongzhu Zheng,\\nMayank\\nGoswami, Prateek Prasanna, and Chao Chen. Learning to\\nsegment from noisy annotations: A spatial correction ap-\\nproach. In International Conference on Learning Represen-\\ntations, 2023. 3\\n[42] Yuhang Zhang, Richard Hartley, John Mashford, and Stewart\\nBurn. Superpixels via pseudo-boolean optimization. In 2011\\nInternational Conference on Computer Vision, pages 1387–\\n1394, 2011. 3\\nAppendix\\nA. More gain with other base superpixel sizes\\n100k\\n150k\\n200k\\n60\\n62\\n64\\n66\\n68\\n70\\nThe number of clicks\\nmIoU (%)\\nAMSP+S (Ours)\\nSP [4]\\n(a) Base superpixel size of 64\\n100k\\n150k\\n200k\\n60\\n62\\n64\\n66\\n68\\n70\\nThe number of clicks\\nmIoU (%)\\n(b) Base superpixel size of 256\\nFigure 7: Effect of base superpixel size on Cityscapes. The\\nperformance difference is greater when the superpixel size\\nis smaller.\\n4\\n16\\n64\\n256 1024 4096\\n56\\n58\\n60\\n62\\nSize of base superpixels\\nmIoU (%)\\nAMSP+S (Ours)\\nSP [4]\\n(a) PASCAL\\nFigure 8: Effect of base superpixel size on PASCAL. Our\\nmethod exhibits robustness to large superpixels, while the\\nbaseline is sensitive.\\nFor ease of exposition, Figure 3 presents the gain of our\\nmethod (compared to SP [4]) for a limited set of base super-\\npixel sizes. In this section, we report an additional investi-\\ngation suggesting further gain with different base superpix-\\nels.\\nFurther gain on Cityscapes. In Figure 7a, we addition-\\nally provide a comparison between the proposed method\\n(AMSP+S) and SP [4], where the experiment setup with\\nCityscapes is identical to that in Figure 3a except that the\\nbase superpixel size is 64 (Figure 7a) instead of 256 (Fig-\\nure 7b). Our adaptive merging method (AMSP+S) is es-\\npecially effective when the superpixel size is small in Fig-\\nure 7a, thanks to the adaptive merging mechanism. This ob-\\nservation suggests more significant gain of our method with\\nother choices of base superpixel size than that in Figure 3.\\n100k\\n150k\\n200k\\n250k\\n300k\\n350k\\n400k\\n64\\n66\\n68\\n70\\n72\\nThe number of clicks\\nmIoU (%)\\n95% Fully-supervised\\nAMSP+S (Ours)\\nSP [4]\\nFigure 9: Additional rounds experiments on Cityscapes. We\\nextend the experiments in Figure 3a up to a budget of 400k.\\nThe performance improvement remains consistent across\\nvarious additional budgets.\\nMethods\\nmIoU\\nSP [4]\\n63.77\\nAMSP+S (bottom 10%)\\n64.33\\nAMSP+S (top 10%)\\n65.99\\nAMSP+S (complete 100%)\\n66.53\\nTable 5: Various levels of partial merging. Experiments are\\nconducted under the same setting of Figure 3a with 100k\\nclicks (Cityscapes, superpixel size of 256).\\nFurther gain on PASCAL. We also demonstrate a larger\\ngap between the proposed method and existing one in PAS-\\nCAL. In Figure 8, our adaptive merging method (AMSP+S)\\noutperforms the baseline (SP) for various superpixel sizes\\nas we observed in Figure 3. We stress that the gain of the\\nproposed method is particularly larger than the one reported\\nin Figure 3 when the base superpixel size is 4096, which is\\nmuch larger than 256 used in Figure 3. This is because the\\nsieving procedure to suppresses the noise from dominant\\nlabeling becomes more crucial when querying large super-\\npixels. The experimental setup used in Figure 8 is identical\\nto that of Figure 3d.\\nFurther rounds on Cityscapes. To demonstrate the effi-\\ncacy of our method across various budgets, we experiment\\nby gradually increasing the budget as illustrated in Figure 9\\non Cityscapes.\\nThe experimental setting in Figure 9 re-\\nmains consistent with that of Figure 3a.\\nThe advantage\\nof our method over SP [4] is continued in further rounds.\\nWe remark that the proposed method nearly achieves the\\n95% mIoU of the fully supervised model (71.95%) at 300k\\nclicks, whereas SP does at 400k clicks.\\nB. Rationale for line 3 of Algorithm 2\\nWe explain the rationale behind traversing nodes in the\\ndescending order of uncertainty in line 3 of Algorithm 2.\\n(a) Merging superpixels with low 10% uncertainty (b) Merging superpixels with high 10% uncertainty\\n(c) Merging all superpixels\\nFigure 10: Qualitative results for partial merging. The cyan boxes encompass superpixels exhibiting the highest 10% un-\\ncertainty, while the red boxes encompass superpixels with the lowest 10% uncertainty. (b) By merging only a portion of\\nsuperpixels in the order of high uncertainty, we can reduce time complexity, as it creates similar merged superpixels com-\\npared with the cyan box in (c).\\nMethods\\nmIoU\\nSP [4]\\n63.77\\nAMSP+S (ϕ(s; θ) = 0.0)\\n65.35\\nAMSP+S (ϕ(s; θ) = 0.2)\\n61.80\\nAMSP+S (ϕ(s; θ) = 0.4)\\n57.77\\nAMSP+S (ϕ(s; θ) = 0.6)\\n45.84\\nAMSP+S (ϕ(s; θ) = 0.8)\\n38.99\\nAMSP+S (Kneedle [30])\\n66.53\\nTable 6: Various sieving methods. Experiments are con-\\nducted on Cityscapes dataset with an average superpixel\\nsize of 256, using 100k costs for two rounds.\\nOur merging process requires a linear time complexity pro-\\nportional to the size of the base superpixels graph. However,\\ndue to the advantage of merging in descending uncertainty\\norder, we are able to acquire merged superpixels with con-\\nsiderable uncertainty at the beginning of merging. To re-\\nduce merging time complexity, we only merge the top 10%\\nof base superpixels with the highest uncertainty as query\\ncandidates.\\nTable 5 shows that it is important to priori-\\ntize the merging highly uncertain superpixels, and merg-\\ning along the ascending order of uncertainty degenerates the\\nperformance.\\nIn Figure 10, we exemplify the merged superpixels from\\nthe partial merging in the ascending or descending order\\nof uncertainty, and the full merging, where the cyan boxes\\ncontain higher values of acquisition function than the red\\n0\\n2\\n4\\n6\\n8 10 12 14 16 18\\n0\\n0.5\\n1\\nx\\nfθ(road; x)\\ndata\\nknee/elbow\\n(a) Road\\n0\\n2\\n4\\n6\\n8 10 12 14 16 18\\n0\\n0.5\\n1\\nx\\nfθ(pole; x)\\n(b) Pole\\nFigure 11: Examples of knee points on Cityscapes. We ob-\\ntain (a) a high knee value for the common road class and (b)\\na low knee value for the rare pole class.\\nboxes. The partial merging with the ascending order of un-\\ncertainty regrettably merges the superpixels that would not\\nbe selected in AL, while that with the ascending order effi-\\nciently combines the base superpixels of which selection is\\nhighly like. This difference indeed results in a huge gap in\\nthe final performance as shown in Table 5.\\nC. Rationale for the adaptive threshold ϕ(s; θ)\\nin the sieving\\nWe provide the reason for introducing the threshold\\nfunction ϕ(s) personalized for each superpixel s, described\\nin Section 3.3. We obtain the dominant label D(s) for a\\nqueried superpixel s, however, we only propagate the label\\nto pixels x ∈s that are predicted to have a positive impact\\nRoad\\nBuilding\\nVegetation\\nCar\\nSidewalk\\nSky\\nPole\\nPerson\\nTerrain\\nFence\\nWall\\nSign\\nBicycle\\nTruck\\nBus\\nTrain\\nLight\\nRider\\nMotorcycle\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nIoU\\nAMSP+S (Kneedle [30])\\nAMSP+S (ϕ(s) = 0.6)\\nFigure 12: Class-wise IoU according to ϕ(s; θ). Applying the same ϕ(s) of 0.6 to all pixels results in excessive sieving for\\nrelatively rare classes, leading to decreased performance for these classes (e.g. Light, Rider, and Motorcycle). Based on the\\nground-truth, class labels are organized in order of the total pixel count for each class.\\n(a) Semantic segmentation\\n(b) Panoptic segmentation\\n(c) Oracle superpixels (ours)\\nFigure 13: Difference between conventional segmentations and oracle superpixels. (a) When sharing the same class label,\\nthey are depicted as identical superpixels (i.e. green color on separate trees). (b) Although a building is divided by a pole,\\nit is represented as a single superpixel (i.e. cyan color). (c) We consider a building as two distinct superpixels (i.e. cyan and\\nlight yellow colors).\\non the training of model θ as:\\n  h( s; \\\\t he t a  ) \\n:\\n= \\\\{ x\\n \\n\\\\ in s : f _\\\\theta \\\\big ( \\\\text {D}(s); x \\\\big ) \\\\geq \\\\phi (s; \\\\theta ) \\\\} \\\\;, \\n(16)\\nwhere fθ (D(s); x) implies the confidence of pixel x to\\ndominant label D(s) given θ and ϕ(s; θ) determines the de-\\ngree of sieving. In Table 6, we study the effect of various\\nϕ(s; θ). When the same ϕ(s; θ) is applied to all pixels, it\\ncauses class imbalance by leaving relatively easy classes as\\ndescribed in Figure 12. To avoid this issue, we utilize the\\nKneedle algorithm [30] to obtain different ϕ(s; θ) for each\\nsuperpixel s. Specifically, ϕ(s; θ) is a knee point of the cu-\\nmulative distribution function of values of fθ\\n\\x00D(s); x\\n\\x01\\nin\\nsuperpixel x ∈s. However, for the Kneedle algorithm to\\nwork accurately, the curve of cumulative distribution must\\nbe either convex or concave. In addition, the algorithm may\\nprovide inaccurate knee points on very smooth curves. To\\naddress this issue, we use a subset of uniformly sampled\\nvalues based on fθ(D(s); x), instead of using the distribu-\\ntion for all pixels. We sample 20 and 5 pixels for Cityscapes\\nand PASCAL datasets, respectively. In Figure 11, different\\nknee points are detected according to the dominant class of\\nsuperpixels.\\nEffect of sieving. Our sieving method exhibits a signifi-\\ncant effect on larger superpixels, as illustrated in Figure 3c\\nand Figure 8. Especially, in Figure 8 with a large base su-\\nperpixel size of 4096, the first sieving excises 45.87% of\\nthe mislabeled pixels that disagree with their dominant la-\\nbels. Furthermore, we observe that the sieving is progres-\\nsively refined round by round. For instance, in Figure 3a,\\nthe portion of the mislabeled labels removed by the siev-\\ning increases over four rounds as follows: 3.58%, 8.54%,\\n10.46%, and 12.43%. Our sieving technique enhances label\\nquality by retaining only high-confidence labels and contin-\\nuously improves through multiple rounds.\\nD. Further discussion on the oracle superpixels\\nIn Section 4.1, we introduce the oracle superpixels,\\nwhich we believe is an achievable optimal set of superpixels\\nfor active learning. For clarification, we provide the detailed\\n0.1\\n0.2\\n0.3\\n0.4\\n51\\n53\\n55\\n57\\n59\\n61\\n63\\nCorr = 0.57\\nAF(G; S)\\nmIoU (%)\\n(a) Semantic segmentation\\n0.25\\n0.3\\n0.35\\n51\\n53\\n55\\n57\\n59\\n61\\n63\\nCorr = 0.95\\nAF(G; S)\\nmIoU (%)\\n(b) Panoptic segmentation\\n0.2\\n0.25\\n0.3\\n0.35\\n51\\n53\\n55\\n57\\n59\\n61\\n63\\nCorr = 0.95\\nAF(G; S)\\nmIoU (%)\\n(c) Oracle superpixels\\nFigure 14: Relationship between AF(G; S) and mIoU varying G. AF(G; S) and mIoU exhibit a high correlation when\\nground-truth G is represented by the panoptic segmentation and oracle superpixels in Figure 13. For the correlation calcula-\\ntion, Oracle in Table 1 is excluded.\\n0.88\\n0.9\\n0.92\\n0.94\\n0.96\\n51\\n53\\n55\\n57\\n59\\n61\\n63\\nCorr = 0.05\\nASA(S; G)\\nmIoU (%)\\n0\\n0.2\\n0.4\\n0.6\\n51\\n53\\n55\\n57\\n59\\n61\\n63\\nCorr = 0.71\\nASA(G; S)\\nmIoU (%)\\n0.88\\n0.9\\n0.92 0.94 0.96\\n51\\n53\\n55\\n57\\n59\\n61\\n63\\nCorr = −0.22\\nAP(S; G)\\nmIoU (%)\\n0\\n2\\n4\\n·10−2\\n51\\n53\\n55\\n57\\n59\\n61\\n63\\nCorr = −0.02\\nAR(S; G)\\nmIoU (%)\\n0\\n2\\n4\\n6\\n8\\n·10−2\\n51\\n53\\n55\\n57\\n59\\n61\\n63\\nCorr = −0.01\\nAF(S; G)\\nmIoU (%)\\n0.4\\n0.5\\n0.6\\n0.7\\n51\\n53\\n55\\n57\\n59\\n61\\n63\\nCorr = −0.43\\nAP(G; S)\\nmIoU (%)\\n0.4\\n0.6\\n51\\n53\\n55\\n57\\n59\\n61\\n63\\nCorr = 0.57\\nAR(G; S)\\nmIoU (%)\\n0.2\\n0.25\\n0.3\\n0.35\\n51\\n53\\n55\\n57\\n59\\n61\\n63\\nCorr = 0.95\\nAF(G; S)\\nmIoU (%)\\nFigure 15: Relationship between metrics and mIoU. The correlation between AF(G; S) and mIoU is especially high. For the\\ncorrelation calculation, Oracle in Table 1 is excluded.\\nprocess of generating the proposed oracle superpixels. In\\naddition, we provide further insights into the achievable no-\\ntion of optimal superpixels.\\nThe Cityscapes dataset is equipped with the ground-\\ntruth annotations for semantic segmentation, represented by\\ndense pixel-wise labels: i.e., each pixel in an annotated im-\\nage is assigned an ID that represents a ground-truth seman-\\ntic category (Figure 13a). In such annotation, each group\\nof pixels that share the same ID aligns perfectly with the\\nboundary of semantic objects. However, each such group is\\nnot guaranteed to be a single-connected component of pix-\\nels. For example, different cars in Figure 13a are assigned\\nthe same blue color despite being physically separated, and\\na car divided into two parts due to an obstructing pole is still\\ncolored blue. This is opposed to what we hope to achieve\\nby merging two adjacent superpixels repeatedly. To address\\nthis issue, we subdivide each superpixel as necessary to en-\\nsure that every pixel within a superpixel is adjacent to each\\nother. We utilize OpenCV [3] and Shapely [14] to iden-\\ntify the maximal connected component of pixels sharing the\\nsame semantic. We apply the same procedure to annotated\\nimages in the PASCAL dataset Figure 13 illustrates the dis-\\ntinction between conventional semantic and panoptic seg-\\nmentation and our oracle superpixels.\\n(a) Adaptive merged (t = 1)\\n(b) Adaptive merged (t = 2)\\n(c) Adaptive merged (t = 3)\\n(d) Adaptive merged (t = 4)\\nFigure 16: Qualitative results with varying round. (a-d) Superpixels generated with proposed adaptive merging at rounds 1\\nto 4. Thanks to the improved model, we observe that the merging becomes more accurate as the round increases. We use the\\nmodel reported in Figure 3a.\\n(a) Adaptive merged (ϵ = 0.05)\\n(b) Adaptive merged (ϵ = 0.1)\\n(c) Adaptive merged (ϵ = 0.15)\\n(d) Adaptive merged (ϵ = 0.2)\\nFigure 17: Qualitative results with varying ϵ. (a-d) Superpixels are generated with proposed adaptive merging with ϵ: 0.05,\\n0.1, 0.15, 0.2. We observe that an increase in ϵ gives more aggressive merging. Merging is conducted on Cityscapes with a\\nbase superpixel size of 256.\\nThe Cityscapes and PASCAL datasets are divided into\\n327k and 16k oracle superpixels, respectively. It is worth\\nnoting that the PASCAL has a lower number of oracle su-\\nperpixels due to the smaller number of classes per image.\\nIn other words, only a few objects are of interest in each\\nimage, and the rest are simply treated as the background.\\nE. Further analysis on the achievable metrics\\nIn Table 1, we evaluate various superpixels using eight\\nmetrics with oracle superpixels as ground-truth G. Figure\\n15 shows the correlation between each metric and mIoU.\\nWe observe that our AF(G; S) can be utilized to look-ahead\\na model’s performance in active learning without training.\\nIn addition, we examine how different ground-truth G im-\\npacts AF(G; S). In the field of semantic segmentation, two\\nconventional segmentations, semantic and panoptic seg-\\nmentations in Figure 13, are widely used as ground-truth.\\nFigure 14 indicates that using panoptic segmentation and or-\\nacle superpixels for G results in higher correlation between\\nAF(G; S) and mIoU than semantic segmentation. However,\\n(a) Base superpixels [35]\\n(b) Merged superpixels (Ours)\\n(c) Oracle superpixels\\nFigure 18: Qualitative results of adaptive superpixels. (a) Base superpixel generated by SEEDS [35] with size 256. (b)\\nSuperpixels generated with proposed adaptive merging at round 4. (c) Oracle superpixels generated from the ground truth.\\nRoad\\nBuilding\\nVegetation\\nCar\\nSidewalk\\nSky\\nPole\\nPerson\\nTerrain\\nFence\\nWall\\nSign\\nBicycle\\nTruck\\nBus\\nTrain\\nLight\\nRider\\nMotorcycle\\n0\\n5\\n10\\nRatio (%)\\nwithout class-balance\\nwith class-balance\\nFigure 19: Effect of class-balanced acquisition function. According to the ground-truth, class labels are arranged based on\\nthe total pixel count for each class, i.e. classes become rarer in images as you move from left to right along the x-axis. We\\nobserve that classes on the left are selected less with the class-balanced term, while classes on the right are selected more.\\nobtaining panoptic segmentation requires more costs than\\nsemantic segmentation since it utilizes additional instance\\ninformation. It is worth noting that our oracle superpixels\\n(Figure 13c) can be easily generated even in cost-limited\\npractical situations as they are produced from semantic seg-\\nmentation (Figure 13a).\\nF. Additional qualitative adaptive superpixels\\nTo facilitate comprehension of the merged superpixels,\\nwe display superpixels generated across diverse settings.\\nThe appearance of merged superpixels is mainly determined\\nby the model’s performance and ϵ. Figure 16 highlights that\\nas the round progresses, the model’s performance improves,\\nleading to more accurate merging. With the model fixed at\\nround 4, Figure 17 shows the impact of adjusting ϵ. As ϵ\\nNotations\\nDescription\\nI\\nthe set of unlabeled images\\nC\\nthe set of class labels\\nt\\na round\\nx\\na pixel\\ns\\na superpixel\\nSt(i)\\nthe set of superpixels in an image i in round t\\nSt\\nthe set of superpixels in all images in round t, St := S\\ni∈I St(i)\\nB\\nthe query budget per round\\nBt\\nthe set of B selected superpixels in round t, Bt ⊂St, |Bt| = B\\nθt\\nthe model at the end of round t\\nyθ(x)\\nthe estimated dominant label of pixel x given θ\\nD(s)\\nthe true dominant label of superpixel s\\nDθ(s)\\nthe estimated dominant label of superpixel s given θ\\nG(S) := (S, E(S))\\nthe graph consisting of the superpixels in S as nodes and the edge set E(S)\\nsuch that (s, n) ∈E(S) for each pair of adjacent superpixels s, n ∈S.\\nϵ\\nthe hyperparameter for merging in (2)\\nTable 7: Notations. The notations used in the paper are defined.\\ngrows, the merging process intensifies, ultimately decreas-\\ning the overall number of superpixels. In addition, Figure\\n18 shows further examples of our merged superpixels.\\nG. Class-balanced sampling\\nTo observe the impact of the class-balanced acquisition\\nfunction in (7), we analyze the class distribution of selected\\nsuperpixels both with and without the class-balanced term.\\nIn Figure 19, where class labels are sorted such that the left\\n(road) and right (motorcycle) ends represent the most and\\nleast popular classes, it is evident that the class-balanced\\nterm results in a higher selection of rarer classes, as in-\\ntended.\\n\"), ResearchPaper(title='Fully and Weakly Supervised Referring Expression Segmentation with End-to-End Learning', authors=[arxiv.Result.Author('Hui Li'), arxiv.Result.Author('Mingjie Sun'), arxiv.Result.Author('Jimin Xiao'), arxiv.Result.Author('Eng Gee Lim'), arxiv.Result.Author('Yao Zhao')], abstract='Referring Expression Segmentation (RES), which is aimed at localizing and\\nsegmenting the target according to the given language expression, has drawn\\nincreasing attention. Existing methods jointly consider the localization and\\nsegmentation steps, which rely on the fused visual and linguistic features for\\nboth steps. We argue that the conflict between the purpose of identifying an\\nobject and generating a mask limits the RES performance. To solve this problem,\\nwe propose a parallel position-kernel-segmentation pipeline to better isolate\\nand then interact the localization and segmentation steps. In our pipeline,\\nlinguistic information will not directly contaminate the visual feature for\\nsegmentation. Specifically, the localization step localizes the target object\\nin the image based on the referring expression, and then the visual kernel\\nobtained from the localization step guides the segmentation step. This pipeline\\nalso enables us to train RES in a weakly-supervised way, where the pixel-level\\nsegmentation labels are replaced by click annotations on center and corner\\npoints. The position head is fully-supervised and trained with the click\\nannotations as supervision, and the segmentation head is trained with\\nweakly-supervised segmentation losses. To validate our framework on a\\nweakly-supervised setting, we annotated three RES benchmark datasets (RefCOCO,\\nRefCOCO+ and RefCOCOg) with click annotations.Our method is simple but\\nsurprisingly effective, outperforming all previous state-of-the-art RES methods\\non fully- and weakly-supervised settings by a large margin. The benchmark code\\nand datasets will be released.', url='http://arxiv.org/abs/2212.10278v1', pdf_path='./papers/2212.10278v1.Fully_and_Weakly_Supervised_Referring_Expression_Segmentation_with_End_to_End_Learning.pdf', content='1\\nFully and Weakly Supervised Referring Expression\\nSegmentation with End-to-End Learning\\nHui Li, Mingjie Sun, Jimin Xiao, Eng Gee Lim, and Yao Zhao,\\nAbstract—Referring Expression Segmentation (RES), which is\\naimed at localizing and segmenting the target according to the\\ngiven language expression, has drawn increasing attention. Ex-\\nisting methods jointly consider the localization and segmentation\\nsteps, which rely on the fused visual and linguistic features for\\nboth steps. We argue that the conﬂict between the purpose of\\nidentifying an object and generating a mask limits the RES per-\\nformance. To solve this problem, we propose a parallel position-\\nkernel-segmentation pipeline to better isolate and then interact\\nthe localization and segmentation steps. In our pipeline, linguistic\\ninformation will not directly contaminate the visual feature for\\nsegmentation. Speciﬁcally, the localization step localizes the target\\nobject in the image based on the referring expression, and then\\nthe visual kernel obtained from the localization step guides the\\nsegmentation step. This pipeline also enables us to train RES\\nin a weakly-supervised way, where the pixel-level segmentation\\nlabels are replaced by click annotations on center and corner\\npoints. The position head is fully-supervised and trained with the\\nclick annotations as supervision, and the segmentation head is\\ntrained with weakly-supervised segmentation losses. To validate\\nour framework on a weakly-supervised setting, we annotated\\nthree RES benchmark datasets (RefCOCO, RefCOCO+ and\\nRefCOCOg) with click annotations. Our method is simple but\\nsurprisingly effective, outperforming all previous state-of-the-art\\nRES methods on fully- and weakly-supervised settings by a large\\nmargin. The benchmark code and datasets will be released.\\nIndex Terms—Referring Expression Segmentation, Weakly-\\nSupervised, End-to-End, Position-Kernel-Segmentation.\\nI. INTRODUCTION\\nR\\nEFERRING Expression Segmentation (RES) [1]–[6] is\\naimed at generating the segmentation mask on the most\\nrelevant object referring to the given language expression. RES\\nhas wide applications in human-robot interactions, such as\\nrobotic navigation with language instructions [7]–[9]. How-\\never, RES is still a challenging task due to the complex\\ninteractions between the two modalities.\\nThe RES segmentation quality is affected by both tar-\\nget object localization and segmentation [6]. Target object\\nlocalization is aimed at identifying the preliminary target\\nlocation by jointly analysing language expression and visual\\ncontent. Target object segmentation compares the localized\\ntarget region with surrounding regions to draw the object\\ncontour as the segmentation mask.\\nSome early RES works [2]–[4] do not separate the local-\\nization and segmentation modules. The segmentation masks\\nare directly generated without intermediate localization results.\\nRecently, end-to-end pipelines, which locate the target object\\nand then segment it, have been introduced [6], [10]. These\\npipelines decouple the referring image segmentation task into\\ntwo sequential tasks, and explicitly locate the referred object\\n“green woman”\\n（a)\\n（b)\\n  \\n✱\\n  \\n“green woman”\\n...\\n（1）\\n（2）\\n（3）\\nFig. 1.\\nComparison of our method with the vanilla method for referring\\nexpression segmentation, using the RefCOCO dataset as an example. (a) The\\nvanilla method predicts the mask of the target object via the fused feature of\\nthe vision and language. (b) Our method predicts the mask via a position-\\nkernel-segmentation process, where linguistic information will not directly\\ncontaminate the visual feature for segmentation. Note that the blue lines are\\nthe main difference from previous works.\\nguided by language expression. The two-step structure uses lo-\\ncalization as relevance ﬁltering, leading to more accurate seg-\\nmentation masks [10]. Other works use semantic segmentation\\ntools to improve RES quality. For instance, object boundaries\\nare utilized to strengthen visual features [5], or Conditional\\nRandom Field (CRF) is used for postprocessing [1], [11]–[14].\\nHowever, the difference between localization and segmenta-\\ntion has not been thoroughly considered in previous methods.\\nSpeciﬁcally, for localization, its primary purpose is to distin-\\nguish the target object from other objects, where the language\\nexpression is essential for excluding nontarget objects. As\\nshown in Fig.1(a), for the query “green woman”, the target\\nin the image is the lady in green clothes but blue trousers.\\nTo predict the preliminary target location, “green” contributes\\nto distinguishing the target person from other persons in\\ndifferent clothes. However, the incomplete information in\\nlanguage expressions may distract the segmentation process.\\nThe mismatch between the “green” clothes and the target’s\\nblue trousers may confuse the segmentation module with the\\ntrousers part unmasked.\\nTo solve this issue, we design an end-to-end parallel\\nposition-kernel-segmentation pipeline, where the position pre-\\narXiv:2212.10278v1  [cs.CV]  17 Dec 2022\\n2\\ndiction can be isolated from the segmentation module, as\\nshown in Fig.1(b). Speciﬁcally, the position head (1) ﬁnds\\nthe target object’s preliminary region according to the given\\nimage and query sentence. The kernel head (2) selects the\\nkernels corresponding to the visual feature points within the\\ntarget region, predicted from the position module. The segmen-\\ntation head (3) generates the ﬁnal ﬁne-grained mask, where\\nthe selected kernels are used to conduct the convolutional\\noperation. For example in Fig.1, our position head predicts the\\ncenter position of the “green woman” via the probability map\\nof the entire image. Then, our kernel head selects the most\\nrelevant regions. Ultimately, the segmentation head predicts\\nthe ﬁnal mask of that woman (dismiss “left girl”) based on\\nthe selected regions. Due to the independent structure, the\\nlocalization branch is isolated from the segmentation branch.\\nThe segmentation branch uses the visual features and kernels\\nas input, solely focusing on generating the segmentation mask.\\nInspired by weakly supervised semantic segmentation [15]–\\n[17] and weakly supervised salient detection [18], [19], this\\npaper also presents the ﬁrst attempt on Weakly supervised\\nReferring Expression Segmentation (WRES). We provide an-\\nnotations on three main RES datasets (RefCOCO, RefCOCOg\\nand RefCOCO+), where pixel-level segmentation labels are\\nreplaced by click annotations on the target object center\\nand corner points. Compared with REG, WRES dramatically\\nreduces the annotation cost of pixel-level labeling.\\nSpeciﬁcally,\\nour\\nparallel\\nposition-kernel-segmentation\\npipeline ﬁts the WRES task well, which could be treated as a\\nbenchmark method. (1) The position head is fully-supervised\\nand trained with the click annotations as supervision. Such a\\ndesign is not applicable to other RES methods, as they do not\\nrely on object localization information as supervision. (2) The\\nsegmentation head is trained with partial cross-entropy loss\\nand augmented with CRF loss, as full pixel-level segmentation\\nlabels are not available.\\nIn summary, the contributions of this paper are listed as\\nfollows:\\n• We propose a position-kernel-segmentation framework\\nfor RES, where localization and segmentation are sep-\\narated. In the localization branch, the linguistic feature is\\nfused into the visual feature for better localization. The\\nsegmentation branch uses the visual feature and kernels as\\ninput, solely focusing on generating better segmentation\\nmasks.\\n• This position-kernel-segmentation framework is naturally\\nsuited for weakly-supervised training, with object center\\nand corner clicks as labels. To validate our framework\\non the weakly-supervised setting, we annotated three\\nbenchmark datasets for the weakly-supervised RES using\\nclick annotations, which will be released to the public.\\n• For the fully-supervised setting, we achieve state-of-the-\\nart (SOTA) performances on three primary datasets on the\\nRES task, including RefCOCO [20], RefCOCO+ [20] and\\nRefCOCOg [21], [22], where the average mIoU scores\\n(val/testA/testB) are signiﬁcantly improved from the pre-\\nvious SOTA method by 3.07%, 4.66% and 2.39%, respec-\\ntively. For the weakly-supervised setting, our method also\\nobtains satisfactory performances, with average mIoU\\nscores of 49.27%, 37.79% and 40.98%, respectively.\\nII. RELATED WORK\\nA. Vision and Language\\nMultimedia systems have drawn increasing attention to\\nbig data artiﬁcial intelligence. Yang et al.\\n[23] ﬁrst in-\\ntroduced multiple knowledge representation (MKR), where\\nthe knowledge representation learns from different abstraction\\nlevels, different sources, and different perspectives for artiﬁcial\\nintelligence.\\nThe vision-language task has a variety of real-world appli-\\ncations, including Visual Question Answering (VQA) [24]–\\n[30], Image-Text Matching (ITM) [31]–[39], Referring Ex-\\npression Grounding (REG) [2], [40]–[44], Referring Video\\nObject Segmentation (RVOS) [45], [46], etc. An important\\nline of research in existing vision-language works is to align\\nthe image visual feature with the text linguistic feature [33],\\n[47]–[49]. For the image-text matching task, Wang et al. [47]\\nproposed consensus-aware embedding to enhance visual and\\nlanguage features, using the statistical correlations of words\\nvia a GCN [48]. The fused representation takes into account\\nthe cross-modality interactions via different fusion methods.\\nChen et al. [33] propose an iterative matching strategy with\\nrecurrent attention memory to associate semantic concepts\\nbetween visual features and linguistic features from low-level\\nobjects to higher-level relationships. For the action recognition\\ntask, Zhu et al. [49] proposed cross-layer attention, which\\nlearns the importance weight of different feature layers.\\nAs an important task for human-robot interactions, REG\\nis aimed at localizing a target object in an image described\\nby a referring expression. Yu et al. [41] view the proposal-\\nbased method as a region-retrieval problem with proposals of\\nall candidate objects provided in advance. Yang et al.\\n[42]\\ncombine the linguistic feature of the query sentence and the\\nvisual feature from the image to generate a multimodality\\nfeature map. Then, the binary classiﬁcation scores (foreground\\nor background) for all points and their corresponding bounding\\nbox coordinates are predicted.\\nB. Referring Expression Segmentation\\nRES is aimed at segmenting the objects based on the\\nexpression instead of at generating their bounding boxes,\\nwhich request ﬁne-grained masks with more details, making\\nit more challenging.\\nEarly RES works focused on directly improving the seg-\\nmentation quality, and never considered the separation between\\nthe localization modules and the segmentation modules. The\\ntransformer-based architecture is adopted in [2] to better fuse\\nthe vision and language features. Multiple sets of textual\\nattention maps are proposed in [4], and in each attention\\nmap, an attention weight is predicted for every single word\\nin the query sentence. Thus, different attention maps can\\nrepresent diversiﬁed textual comprehensions from different\\naspects to better understand the query sentence. Different\\nfrom the aforementioned works that combine localization and\\n3\\n✱\\n“dish in \\ntop right \\ncorner” \\nBi-GRU\\nKernel Head\\n...\\nSegmentation Head\\nPosition Head\\nEncoding\\nPosition-Kernel-Segmentation\\nFully-Supervised Losses\\nS\\nX\\nig\\uf0a2\\nid\\nFocal Loss \\npos\\niy\\nip\\nseg\\nY\\nDice Loss\\nid\\nip\\nM\\nI\\nX\\nig\\nI\\nix\\nMSF\\nWeakly-Supervised Losses\\nFocal Loss\\npCE Loss\\nGCRF Loss\\nI\\npos\\niy\\nid\\nip\\nseg\\nYˆ\\nip\\nFig. 2. Overview of the proposed position-kernel-segmentation RES pipeline. The main parts of the proposed method consist of three heads: (a) The position\\nhead predicts a heatmap to localize the object described by the referring expression via a Multi-Step Fusion (MSF) process. (b) The kernel head selects\\nthe kernel vectors from the image feature map using the position information. (c) The segmentation head predicts the ﬁnal segmentation mask of the target\\nobject. The MSF module fuses the linguistic and visual features. Best viewed in color.\\nsegmentation modules, Jing et al. [6] ﬁrst proposed an end-to-\\nend locate-then-segment pipeline, where the localization result\\nis concatenated with the fused visual-linguistic feature for\\nsegmentation prediction. CMS-Net [10] proposes an end-to-\\nend cross-modality synergy network, which uses the language\\nas guidance to get the language-aware visual feature, and\\nthe language-aware visual feature is fused with the original\\nlanguage feature for ﬁnal segmentation predictions.\\nOther works use semantic segmentation tools to improve\\nRES quality. Feng et al. [5] use the boundary information\\nas the additional supervision in the training process, which\\nstrengthens the visual features to generate smoother masks. Hu\\net al. [14] ﬁrst introduced the widely-employed segmentation\\ntool, CRF, to reﬁne the output semantic segmentation map,\\nwhich is also adopted by later works [1], [11]–[14].\\nUnlike existing methods, this is the ﬁrst work to de-\\nsign an end-to-end position-kernel-segmentation pipeline that\\nseparates the localization and segmentation modules, where\\nlinguistic information will not directly contaminate the visual\\nfeature for segmentation. Different from previous methods,\\nour position head inﬂuences the segmentation head via the\\nkernel head, where the noise of the attention does not directly\\ninﬂuence the segmentation process. However, the previous\\nmethod does not isolate the segmentation branch, thus the\\nunstable attention in localization affects the mask generation\\nprocess.\\nC. Semantic Segmentation\\nSupervised semantic segmentation takes pixel-level labels\\nas supervision to train a model to predict the precise object\\nmasks. The recent deep-learning-based methods use anchor-\\nbased (Mask R-CNN [50]), kernel-based (SOLOv2 [51]),\\nand transformer-based (Swin Transformer [52]) solutions for\\nprecise mask generation. However, supervised semantic seg-\\nmentation has one major limitation: pixel-level annotation is\\ntime-consuming and costly.\\nTo reduce the human annotation cost, many works in-\\nvestigate weakly-supervised semantic segmentation including\\nimage-level [53], scribble-level [16], and point-level [17]\\nannotations. For image-level labels, the basic idea is to use\\nthe classiﬁcation information to generate Class Activation\\nMap (CAM), where the activated areas can be treated as the\\nseeds for pseudo labels. For scribble- and point-based methods\\nwith partial pixel-level annotations, ScribbleSup [15] uses\\nthe superpixel method of Simple Linear Iterative Clustering\\n(SLIC) [54] to expand the original labels. Additionally, other\\nstudies focus on reducing the noise of the expanded pseudo\\nlabels. For instance, Tang et al. [55] use the normalized cut\\nloss to cut noisy labels lower than the trust threshold.\\nWeakly-supervised RES is more challenging than weakly-\\nsupervised semantic segmentation, since weakly-supervised\\nsemantic segmentation has classiﬁcation information to gener-\\nate a CAM as pseudo labels, which is not available in weakly-\\nsupervised RES. In our setting, the only labels for weakly-\\nsupervised RES are the object center and corner points.\\nIII. METHODOLOGY\\nAs shown in Fig.2, our framework mainly consists of 5\\ncomponents: the feature encoding module, position head, ker-\\nnel head, segmentation head and training losses. The position\\nhead, kernel head and segmentation head follow a position-\\nkernel-segmentation workﬂow to generate the segmentation\\nmask of the target object.\\n• The feature encoding module encodes input images\\nand referring sentences into high dimensional features\\n(Sect.III-A). All levels of the hierarchical visual features\\nwill be applied in the following steps.\\n• The position head fuses linguistic and visual features via\\nour designed MSF mechanism; then, the position heatmap\\n4\\nis predicted, describing the target object’s spatial position\\nin the image (Sect.III-B).\\n• The kernel head takes the visual features and position\\nheatmaps to select the kernel features, where the ultimate\\nselections correspond to the high probability regions from\\nthe position heatmaps (Sect.III-C).\\n• The segmentation head uses the visual features and\\nselected kernels to predict the ﬁnal segmentation mask\\nof the target object (Sect.III-D).\\n• The fully-supervised setting uses two parallel losses for\\ntarget object localization and segmentation (Sect.III-E);\\nthe weakly-supervised setting uses three losses, with an\\naddition loss considering color consistency (Sect.IV-B).\\nA. Feature Encoding Module\\nFor an input image I ∈RH×W ×3 with height H and\\nwidth W, the initial visual pyramid features are deﬁned as\\nXI={xI\\ni }L\\ni=1 with L levels, obtained from the pretrained\\nfeature extractor Feature Pyramid Network (FPN) [56], where\\nxI\\ni corresponds to the i-th level of the pyramid features.\\nFor a sentence with N words S={ut}N\\nt=1, its linguistic\\nfeature is denoted as XS={xS\\nt }N\\nt=1, where each word’s feature\\nxS\\nt at position t is generated by the linguistic feature extractor\\nGlove [57], followed by the sequential bidirectional feature\\nextractor Bi-GRU [58].\\nB. Position Head\\nOur position head predicts heatmaps to localize the tar-\\nget object described by the referring expression via a MSF\\nprocess. Using the visual feature XI and linguistic feature\\nXS from Sect.III-A as inputs, the position head predicts the\\nheatmaps D={di}L\\ni=1, which have the same levels L as XI.\\ndi is the i-th position heatmap corresponding to feature xI\\ni .\\nEach pixel value in di ∈Rhi×wi represents the probability of\\nwhether this pixel belongs to the target object.\\nMulti-Step Fusion: The visual features XI are fused with\\nthe linguistic feature XS to obtain the position heatmaps D.\\nSpeciﬁcally, di is obtained after fusing xI\\ni with XS.\\nA MSF process is conducted to fuse the visual and language\\nfeatures, following one fully convolutional layer to calculate\\nthe probability values in the position heatmap.\\nThe MSF takes J steps. In each step, the linguistic feature\\nXS is blended into feature F j\\ni (where F 0\\ni = xI\\ni ) to obtain the\\nnext step fused feature F j+1\\ni\\n:\\nF j+1\\ni\\n= Fusion(XS, F j\\ni ), 0 ≤j ≤J −1,\\n(1)\\nwhere F j\\ni ∈Rhi×wi×cj\\ni is the fused feature on the i-th level\\nand at the j-th step, and J is the total step number.\\nIn different fusion steps, different attention values are al-\\nlocated to different phrases to focus on different phrases. For\\ninstance, “dish” takes the highest attention value in the ﬁrst\\nfusion step while the second step ﬁnds “top” and “right” to\\nbe the most relevant, as shown in Fig.3 (left). More examples\\nare provided be found in Fig.4.\\n...\\nFusion\\nFusion\\nFusion\\n...\\nMulti-Step Fusion\\nS\\nX\\nj\\niF\\n1\\nj\\niF \\uf02b\\nI\\nix\\nJ\\niF\\n“dish in top right corner ” “dish in top right corner ” \\n⊙\\nT\\nj\\niF\\nS\\nX\\n1\\nj\\niF\\n\\uf02b\\n)\\n(\\nAvgT \\uf0d7\\n)\\nF\\n(\\nj\\ni\\nF\\n\\uf046\\n)\\nF\\n(\\nj\\ni\\nF\\n\\uf06a\\n)\\nXˆ\\n(\\nS\\nS\\n\\uf06a\\nj\\niA\\nS\\nXˆ\\nj\\ni\\ni\\ni\\nc\\nw\\nh\\n\\uf0b4\\n\\uf0b4\\nS\\nc\\nN \\uf0b4\\nj\\ni\\ni\\ni\\nc\\nw\\nh\\n\\uf0b4\\n\\uf0b4\\nj\\nic\\nN \\uf0b4\\n1\\nc j\\ni \\uf0b4\\n1\\nc j\\ni \\uf0b4\\n1\\nc\\n1\\nj\\ni\\n\\uf0b4\\n\\uf02b\\n1\\nj\\ni\\ni\\ni\\nc\\nw\\nh\\n\\uf02b\\n\\uf0b4\\n\\uf0b4\\n1\\nj\\ni\\ni\\ni\\nc\\nw\\nh\\n\\uf02b\\n\\uf0b4\\n\\uf0b4\\nN\\nc j\\ni \\uf0b4\\n1\\nN \\uf0b4\\n)\\nX\\n(\\nS\\nS\\n\\uf046\\nFusion\\nFig. 3. The MSF (left) and the details of fusion with attention on level i and\\nstep j (right). The fusion process generates the next step fused feature F j+1\\ni\\nprovided with XS and F j\\ni . The feature size of each element is marked above\\neach feature. ⊙represents the element-wise multiplication on the channel\\ndimension, where ϕS( ˆ\\nXS) is broadcast to the same size of ϕF (F j\\ni ). T is\\nthe transpose operation.\\nstep 1\\nstep 2\\nstep 3\\nstep 4\\nwoman\\nin\\ntop\\npicture\\non\\nthe\\nstep 1\\nstep 2\\nstep 3\\nstep 4\\ncat\\non\\nleft\\nstep 1\\nstep 2\\nstep 3\\nstep 4\\nguy\\nstanding\\nplaying\\nwiifit\\nstep 1\\nstep 2\\nstep 3\\nstep 4\\nman\\non\\nright\\nin\\nshorts\\nelephant\\nFig. 4. Examples of the different attention values that are allocated to different\\nphrases in different fusion steps.\\nUsing the ﬁnal fusion feature F J\\ni\\nas input, one Fully\\nConvolutional Layer (FCL) is utilized to predict the ﬁnal\\nposition heatmap di:\\ndi = FCL(F J\\ni ).\\n(2)\\nFusion with Attention: This subsection describes the im-\\nplementation of Eq.(1) using the attention mechanism. Specif-\\nically, the detailed process, which consists of an attention\\noperation between the language feature and the visual feature,\\nis illustrated in Fig.3 (right).\\nThe language attention calculates the importance of each\\nword in the sentences based on the image visual feature. The\\nlanguage attention matrix for the i-th level and j-th step, i.e.,\\nAj\\ni ∈RN×1, is calculated by:\\nAj\\ni = φS(XS)(Avg(φF (F j\\ni )))T ,\\n(3)\\nwhere φS(·) and φF (·) are convolutional operations to unify\\nthe linguistic feature and fused features into the same channel\\nsize. Their ﬁnal sizes are N ×cj\\ni and hi×wi×cj\\ni. To represent\\nthe entire image, we use Avg(φF (F j\\ni )) ∈R1×cj\\ni , which is\\n5\\nthe average feature of all feature pixels in the hi × wi map.\\n(·)T is the transpose operation. Then, matrix multiplication is\\nperformed to calculate the ﬁnal attention values Aj\\ni.\\nWith\\nthe\\nlanguage\\nattention\\nmatrix\\nAj\\ni\\nin\\nEq.(3),\\nwe\\nobtain\\nthe\\nweighted\\naverage\\nlinguistic\\nfeature\\nˆXS=(φS(XS))T Sigmoid(Aj\\ni), where the sigmoid function\\nis used to scale the attention values to range (0, 1).\\nThe fused feature F j+1\\ni\\nis calculated as:\\nF j+1\\ni\\n= ϕS( ˆXS) ⊙ϕF (F j\\ni ),\\n(4)\\nwhere ϕS(·) and ϕF (·) are convolutional operations to convert\\nthe weighted linguistic feature and fused feature into the\\nnext step’s channel size by one convolutional layer. The ﬁnal\\nfused feature is calculated by element-wise multiplication.\\nSpeciﬁcally, ϕS( ˆXS) is broadcast to the same size of ϕF (F j\\ni ).\\nNote that Sect.III-B is the only module where the language\\ninformation is implemented.\\nC. Kernel Head\\nThe kernel head uses the visual features XI and the position\\nheatmaps D to select the kernels G′, which are used in the\\nsegmentation head to produce the target object mask.\\nSpeciﬁcally, the ﬁrst step in our kernel head is to generate\\nthe kernel map pools G={gi}L\\ni=1, using the image features XI.\\ngi corresponds to the i-th feature xI\\ni , which is calculated as\\ngi = CoordConv(xI\\ni ).\\n(5)\\nCoordConv(·) [59] represents a set of convolutional oper-\\nations on the coordinate-enhanced visual feature. Each ci-\\nchannel feature point in xI\\ni (resolution is hi × wi) is concate-\\nnated with its two-dimensional coordinates, where the channel\\nnumber becomes ci + 2.\\nThe ﬁnal gi\\n∈Rhi×wi×c′ shares the same height hi\\nand width wi with the position heatmap di. Based on the\\nconﬁdence values in di, ki kernel features g′\\ni ∈Rki×c′ are\\nselected from gi for level i:\\ng′\\ni =\\n\\x08\\ngi[m, n, :]\\n\\x0c\\x0cdi[m, n] > λf\\n\\thi,wi\\nm=1,n=1,\\n(6)\\nwhere [m, n] is the coordinate within hi × wi. λf is the\\nthreshold, where the probability value di[m, n] is higher than\\nλf for the selected kernel position [m, n]. The element number\\nof the selected kernel set G′={g′\\ni}L\\ni=1 is PL\\ni=1 ki. Note that,\\nduring the training process, the kernels g′\\ni are selected by\\nthe ground-truth label from the position head. The details of\\ngenerating the ground-truth label for the position head are\\nintroduced later in Sect.III-E.\\nD. Segmentation Head\\nThe segmentation head uses the visual features XI and the\\nselected kernels G′ to predict the ﬁnal segmentation mask P\\nof the target object.\\nFirst, a segmentation decoder [60] is employed to generate\\nthe mask feature map M by taking the L −1 levels of the\\nvisual feature pyramid XI as input:\\nM = Decoder({xI\\ni }L\\ni=2),\\n(7)\\n（a) Image\\n（b) Fully-Supervised Mask\\n（c) Click Annotations\\n（d) Superpixel Mask\\nFig. 5.\\nAnnotated example for weakly-supervised RES from dataset Ref-\\nCOCO [20]. (a) The original image. (b) The fully-supervised mask, where\\nthe blue area is the ground-truth mask. (c) Click annotations, where the blue\\npoint is the object center point, while the white points are the object corners.\\n(d) Superpixel mask, where the blue area is the superpixel mask, while the\\nareas outside the corner points are the background.\\nTABLE I\\nCOMPARISON OF THE COST BETWEEN FULLY-SUPERVISED ANNOTATIONS\\nAND WEAKLY-SUPERVISED ANNOTATIONS FOR ONE TARGET.THE TIME\\nCOST IS OBTAINED FROM [61], [62].\\nParameters\\nFully-Supervised\\nWeakly-Supervised\\nRequest\\npolygon\\npoints\\nNumber of points\\n≈15\\n3\\nTime\\n≈79s\\n≈7.2s\\nwhere M ∈R\\nH\\n4 × W\\n4 ×c′ has a quarter size of the original input\\nimage I and the same channel number c′ as g′\\ni in Eq.(6).\\nSecond, the ﬁnal predicted mask P={pi}L\\ni=1 (each pi ∈\\nRki× H\\n4 × W\\n4 ) is generated using the convolution operation be-\\ntween the mask feature M and the kernel g′\\ni ∈G′,\\npi = Sigmoid(conv(M, g′\\ni)),\\n(8)\\nwhere the sigmoid operation is used to scale the predicted\\nvalues to range (0, 1).\\nE. Fully-Supervised Losses\\nIn the localization branch, the training loss Lpos measures\\nthe difference between the predicted position heatmaps D and\\nthe ground-truth label:\\nLpos = 1\\nL\\nL\\nX\\ni=1\\nFocal(di, ypos\\ni\\n),\\n(9)\\nwhich uses the focal loss [63] to optimize the predicted di on\\nall L levels. ypos\\ni\\n∈{0, 1}hi×wi is the one-hot ground-truth\\nmap for the i-th position map, where the object’s center 3 × 3\\npoints are set to 1 to represent the position of the target object.\\nIn the segmentation branch, taking the predicted mask P\\nfrom Eq.(8), the segmentation loss is:\\nLseg = Dice(P, Y seg),\\n(10)\\n6\\n“middle \\nkid”\\n“person \\nbottom \\nleft”\\n“person \\non left of \\nbench”\\n“main guy \\non the tv”\\n“guy on \\nright”\\n“player \\nnumber 8”\\n“baby \\nholding \\ntoy”\\n“right \\nbatter”\\n“glasses”\\n“guy in \\nbrown \\nshirt”\\n“vase of \\nflower \\ndrooping”\\n“urinal \\nclosest to \\nthe white \\npipe”\\n“fully \\nvisible \\nbus”\\n“smaller \\nblack \\nbird”\\n“part with \\nonions on \\nit”\\n“lazy cow”\\n“cow \\nsticking \\ntheir \\nbooty at \\nyou”\\n“black \\nshirt”\\n（a）RefCOCO\\n（b）RefCOCO+\\nFig. 6. Examples of the fully-supervised and weakly-supervised annotations on (a) RefCOCO [20] and (b) RefCOCO+ [20]. For each example, we provide:\\nthe expression, original image, superpixel pieces, fully-supervised mask, center point annotation, and weakly-supervised superpixel mask.\\nwhere the Dice loss [64] is adopted to measure the mask\\ndifference. Y seg ∈{0, 1}\\nH\\n4 × W\\n4\\nis the one-hot ground-truth\\nmask of the target object with an 1/4 bilinear down-sampling\\nprocess.\\nThe ﬁnal training loss is the sum of Eq.(9) and Eq.(10):\\nL = Lpos + Lseg.\\n(11)\\nIn the inference process, the Matrix NMS [51] is employed\\nto merge all the predicted masks P from Eq.(8) into one ﬁnal\\nsegmentation mask.\\nIV. EXTENSION FOR WEAKLY-SUPERVISED SETTING\\nA. Annotation of Weakly-Supervised RES\\nFully-supervised RES requests the annotator to draw pre-\\ncious polygon vertices of the target object (Fig.5(b)). In\\ncontrast, weakly-supervised RES only needs clicks in the\\nlabelling process. Fig.5(c) shows one example of the weak\\nannotations for the target object “baby elephant”.\\nWe annotate three datasets including RefCOCO [20], Ref-\\nCOCO+ [20] and RefCOCOg [21], [22]. For each object, we\\nneed three key points for the target object: (1) center of the\\nobject; (2) left-top corner of the object, and (3) the right-down\\ncorner of the object. In our case, we use the object’s center-of-\\nmass as the center point, the left-top and right-down bounding\\nbox corners as two corner clicks for convenience.\\nInspired by the semantic segmentation task, we use super-\\npixel [54] to expand the center point annotation to generate\\ninitial labels for the target object. The areas outside the two\\ncorners are treated as the background for the target object. The\\nremaining regions are deﬁned as unknown areas (Fig.5(d)).\\nTable I compares the annotation cost of fully-supervised\\nand weakly-supervised RES. Each target object needs approx-\\nimately 15 points for a precious boundary, costing 79 s [61].\\nOur weakly-supervised annotation only needs 3 points for one\\nobject, costing 7.2 s, which is approximately ten times faster\\nthan the fully-supervised counterpart. A lower annotation cost\\nalso means lower ﬁnancial budget. Some annotation examples\\nare shown in Fig.6.\\nB. Training Losses of Weakly-Supervised RES\\nIn the localization branch, the training loss ˆLpos measures\\nthe difference between the predicted position heatmaps D and\\n7\\nTABLE II\\nCOMPARISONS BETWEEN THE PROPOSED METHOD AND OTHER SOTA METHODS ACCORDING TO THE MIOU SCORE, ON REFCOCO [20],\\nREFCOCO+ [20] AND REFCOCOG [21], [22] DATASETS. U: UMD SPLIT. G: GOOGLE SPLIT.\\nBackbone\\nRefCOCO\\nRefCOCO+\\nRefCOCOg\\nval\\ntest A\\ntest B\\nval\\ntest A\\ntest B\\nval (U)\\ntest (U)\\nval(G)\\nRMI [1] ICCV’17\\nResNet101\\n45.18\\n45.69\\n45.57\\n29.86\\n30.48\\n29.50\\n-\\n-\\n34.52\\nDMN [65] ECCV’18\\nResNet101\\n49.78\\n54.83\\n45.13\\n38.88\\n44.22\\n32.29\\n-\\n-\\n36.76\\nRRN [11]+DCRF CVPR’18\\nResNet101\\n55.33\\n57.26\\n53.93\\n39.75\\n42.15\\n36.11\\n-\\n-\\n36.45\\nMAttNet [41] CVPR’18\\nResNet101\\n56.51\\n62.37\\n51.70\\n46.67\\n52.39\\n40.08\\n47.64\\n48.61\\n-\\nCMSA [12]+DCRF CVPR’19\\nResNet101\\n58.32\\n60.61\\n55.09\\n43.76\\n47.60\\n37.89\\n-\\n-\\n39.98\\nBRINet [14] CVPR’20\\nDeepLab-101\\n60.98\\n62.99\\n59.21\\n48.17\\n52.32\\n42.11\\n-\\n-\\n48.04\\nCMPC [13] CVPR’20\\nDeepLab-101\\n61.36\\n64.53\\n59.64\\n49.56\\n53.44\\n43.23\\n-\\n-\\n39.98\\nLSCM [66] ECCV’20\\nDeepLab-101\\n61.47\\n64.99\\n59.55\\n49.34\\n53.12\\n43.50\\n-\\n-\\n48.05\\nMCN [67] CVPR’20\\nDarkNet53\\n62.44\\n64.20\\n59.71\\n50.62\\n54.99\\n44.69\\n49.22\\n49.40\\n-\\nCGAN [68] ACM MM’20\\nDarkNet53\\n64.86\\n68.04\\n62.07\\n51.03\\n55.51\\n44.06\\n51.01\\n51.69\\n46.54\\nACM [5] CVPR’21\\nResNet101\\n62.76\\n65.69\\n59.67\\n51.50\\n55.24\\n43.01\\n51.93\\n-\\n-\\nLTS [6] CVPR’21\\nDarkNet53\\n65.43\\n67.76\\n63.08\\n54.21\\n58.32\\n48.02\\n54.40\\n54.25\\n-\\nVLT [4] ICCV’21\\nTransformer\\n65.65\\n68.29\\n62.73\\n55.50\\n59.20\\n49.36\\n52.99\\n56.65\\n49.76\\nReSTR [69] CVPR’22\\nTransformer\\n67.22\\n69.30\\n64.45\\n55.78\\n60.44\\n48.27\\n-\\n-\\n54.58\\nPKS\\nResNet101\\n70.87\\n74.23\\n65.07\\n60.90\\n66.21\\n51.35\\n60.38\\n60.98\\n56.97\\nTABLE III\\nABLATION STUDY OF THE INFLUENCE OF THE KERNEL HEAD ON THE\\nREFCOCO [20] VALIDATION SET.\\nModules\\nprec@X\\nmIoU\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\nw/o kernel\\n77.45\\n74.98\\n70.66\\n61.08\\n34.01\\n68.27\\nw/ kernel\\n80.13\\n77.67\\n73.15\\n64.60\\n35.87\\n70.87\\n+2.68 +2.69 +2.49 +3.52 +1.86 +2.60\\nthe ground-truth label, which involves the same calculation\\nas Eq.(9).\\nIn the segmentation branch, using the predicted mask P\\nfrom Eq.(8), the weakly-supervised segmentation loss is:\\nˆLseg = pCE(P, ˆY seg),\\n(12)\\nwhere partial Cross-Entropy Loss (pCE) is adopted to measure\\nthe mask difference. ˆY seg ∈{0, 1, −1}\\nH\\n4 × W\\n4 is the ground-\\ntruth mask of the target object for weakly-supervised RES,\\nwhich is deﬁned in Sect.IV-A. We only consider the fore-\\nground and background areas of the target object (as 1 and 0)\\nand disregard the unknown areas (as -1) during the training\\nprocess.\\nTo better consider the color information from the image, we\\nadopt the widely-applied Gated-CRF (GCRF) loss [70] in the\\ntraining process:\\nˆLw = GCRF(P, ˆI),\\n(13)\\nwhere ˆI is the resized raw RGB image and P is the predicted\\nmask.\\nThe ﬁnal training loss for weakly-supervised RES is:\\nˆL = ˆLpos + ˆLseg + λw ˆLw,\\n(14)\\nwhere λw is the hyperparameters to balance the inﬂuence of\\nthe GCRF loss.\\nV. EXPERIMENTS\\nA. Experimental Settings\\nMetrics: The RES has two evaluation metrics: the mean\\nIntersection over Union (mIoU) and the precision score over\\na threshold X (prec@X). mIoU calculates intersection regions\\nover union regions of the predicted segmentation mask and the\\nground-truth. prec@X measures the percentage of test images\\nwith an IoU score higher than the threshold X , where X∈\\n{0.5, 0.6, 0.7, 0.8, 0.9}.\\nDataset: The proposed method is evaluated on three main\\nRES datasets, including RefCOCO [20], RefCOCO+ [20] and\\nRefCOCOg [21], [22].\\nRefCOCO is a standard RES dataset, where the numbers\\nof images, objects, and expressions are 19,994, 50,000, and\\n142,210, respectively. Expressions are split into 120,624,\\n10,834, 5,657, and 5,095, as the train, val, testA, and testB\\nsets, respectively. A single image contains multiple targets,\\neach of which is annotated by several sentences.\\nRefCOCO+ is another widely-employed RES dataset, where\\nfewer spatial descriptions are applied in referring expressions.\\nThe numbers of images, objects, and expressions are 19,992,\\n49,856, and 141,564, respectively. Expressions are split into\\n120,191, 10,758, 5,726, and 4,889, as the train, val, testA, and\\ntestB sets, respectively.\\nRefCOCOg has two different branches: Google [21] and\\nUMD [22]. In terms of Google, the numbers of images,\\nobjects, and expressions are 26,711, 54,822, and 85,474,\\nrespectively. Objects are split into 44,822, 5,000, and 5,000\\nas train, val, and test sets, respectively, where the test set\\nis not released. For UMD, the numbers of images, objects,\\nand expressions are 25,799, 49,822, and 95,010, respectively.\\nObjects are split into 42,226, 2,573, and 5,023 as the train,\\nval, test and sets, respectively.\\nB. Implementation Details\\nThe adopted visual feature extractor is ResNet-101 [71],\\nwhere the feature maps of all levels in FPN [56] are used as the\\nmultiscale inputs of the later modules. The linguistic feature\\nextractor is Bi-GRU [58], where a sequential bidirectional\\nmodule is utilized to embed the sentence and each word is\\nembedded into a 1024-D vector. In terms of the MSF module\\ndescribed in Sect.III-B, we set the step number J=4 in Eq.(1).\\n8\\nTABLE IV\\nABLATION STUDY OF DIFFERENT FUSION STRATEGIES, CONDUCTED ON THE VALIDATION SET OF REFCOCO.\\nmodules\\nprec@X\\nmIoU\\nstrategy\\nmulti-level\\nmulti-step\\nprec@0.5\\nprec@0.6\\nprec@0.7\\nprec@0.8\\nprec@0.9\\nconcat\\n\\x13\\n\\x13\\n33.29\\n32.63\\n31.41\\n28.50\\n17.10\\n29.97\\nmultipy\\n\\x13\\n\\x13\\n64.91\\n60.78\\n55.73\\n46.17\\n22.10\\n56.76\\nmean\\n\\x13\\n\\x13\\n72.67\\n69.58\\n64.58\\n55.21\\n28.61\\n63.89\\nMSF\\n\\x17\\n\\x17\\n69.37\\n67.16\\n63.18\\n55.99\\n31.23\\n61.61\\nMSF\\n\\x13\\n\\x17\\n76.28\\n73.83\\n69.89\\n61.75\\n34.88\\n67.52\\nMSF\\n\\x17\\n\\x13\\n76.33\\n73.36\\n68.82\\n60.50\\n33.97\\n67.43\\nMSF\\n\\x13\\n\\x13\\n80.13\\n77.67\\n73.15\\n64.60\\n35.87\\n70.87\\nTABLE V\\nANALYSIS ON HOW THE STEP NUMBER J AFFECTS THE PERFORMANCE,\\nCONDUCTED ON THE VALIDATION SET OF REFCOCO [20]. THE RESULTS\\nARE EVALUATED BY PREC@X AND MIOU, ON DIFFERENT SETTINGS OF J\\nWITH/WITHOUT MULTI-LEVEL (ML) FEATURES.\\nML\\nJ\\nprec@X\\nmIoU\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n\\x17\\n1\\n69.37\\n67.16\\n63.18\\n55.99\\n31.23\\n61.61\\n2\\n73.76\\n71.05\\n67.20\\n59.35\\n32.19\\n65.47\\n3\\n76.14\\n73.07\\n69.01\\n60.74\\n34.01\\n67.39\\n4\\n76.33\\n73.36\\n68.82\\n60.50\\n33.97\\n67.43\\n\\x13\\n1\\n76.28\\n73.83\\n69.89\\n61.75\\n34.88\\n67.52\\n2\\n79.21\\n75.43\\n72.10\\n63.09\\n35.58\\n69.72\\n3\\n79.87\\n77.19\\n72.88\\n63.93\\n35.85\\n70.58\\n4\\n80.13\\n77.67\\n73.15\\n64.60\\n35.87\\n70.87\\nTABLE VI\\nANALYSIS ON POSITION PREDICTIONS ON DIFFERENT RANGE OF THE\\nFEATURE LEVELS CONDUCTED ON THE VALIDATION SET OF\\nREFCOCO [20]. THE RESULTS ARE EVALUATED ACCORDING TO PREC@X\\nAND MIOU. A LARGER LEVEL NUMBER REPRESENTS A DEEPER FEATURE,\\nWITH A SMALLER RESOLUTION.\\nlevel range\\nprec@X\\nmIoU\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\nlevel 5\\n76.33\\n73.36\\n68.82\\n60.50\\n33.97\\n67.43\\nlevel 4-5\\n78.39\\n75.89\\n71.61\\n62.68\\n34.77\\n69.46\\nlevel 3-5\\n78.98\\n76.09\\n71.61\\n62.95\\n35.30\\n69.87\\nlevel 2-5\\n79.31\\n77.05\\n72.22\\n63.85\\n35.34\\n70.18\\nlevel 1-5\\n80.13\\n77.67\\n73.15\\n64.60\\n35.87\\n70.87\\nThe hyperparameter λf for kernel selection in Eq.(6) is set to\\n0.1, as in SOLOv2 [51].\\nThe training process is conducted on four 32G TESLA\\nV100 GPUs, with a batch size of 8 for each GPU. The process\\ntakes approximately 50 hours for 30 epochs. The input image\\nsizes H and W for all images are rescaled to 1,333 and\\n800, respectively. SGD is adopted as the optimizer, with the\\nlearning rate set as 0.01, the momentum set as 0.9, and the\\nweight decay is set as 1 × 10−4. All the obtained results have\\nsome ﬂuctuations, from -0.2 to 0.2, when different seeds are\\nemployed.\\nFor weakly-supervised RES in Sect.IV, SLIC [54] is\\nadopted as the superpixel generation method for weakly-\\nsupervised annotation, where the number of segmentation\\nareas for each image is set to 100, and the other parameters\\nfollow the same settings as in the original paper. The hyperpa-\\nrameter λw in Eq.(14) is set to 0.01. The parameters in GCRF\\nloss are the same as in [70]. Other settings remain the same\\nas those in the fully-supervised case.\\nTABLE VII\\nUPPER-BOUND ANALYSIS ON THE REFCOCO [20] VALIDATION SET. gt\\npos. INDICATES THAT THE GROUND-TRUTH POSITION OF THE TARGET\\nOBJECT IS UTILIZED ON THE KERNEL SELECTING PROCESS IN SECT.III-C\\ngt pos.\\nprec@X\\nmIoU\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n\\x17\\n80.13\\n77.67\\n73.15\\n64.60\\n35.87\\n70.87\\n\\x13\\n93.28\\n90.28\\n84.65\\n74.50\\n41.56\\n81.91\\n+13.15 +12.61 +11.50 +9.90 +5.69 +11.04\\nC. Experimental Results for Fully-Supervised RES\\n1) Comparison with Fully-Supervised SOTA Methods:\\nTable II reports the comparisons between the proposed method\\nand other SOTA methods according to the mIoU score. The\\nproposed method achieves a new SOTA accuracy with a\\nsigniﬁcant gain on three RES datasets, including RefCOCO,\\nRefCOCO+ and RefCOCOg. Compared with the previous\\nSOTA method ReSTR [69], the proposed method improves the\\nmIoU scores (averaged over val, test A and test B) by 3.07%,\\n4.66% and 2.39% on RefCOCO, RefCOCO+ and RefCOCOg,\\nrespectively. This result demonstrates the effectiveness of the\\nproposed parallel pipeline in the RES task. Note that the\\nproposed model is a one-stage framework trained end-to-end\\nwithout generating proposals or masks in advance. In our\\nmethod, we use the original ResNet101 as the feature extractor\\nwhich is the same as [1], [5], [11], [12], [41], [65]. Some other\\nmethods use more powerful backbones (DarkNet53, DeepLab-\\n101 and Transformer). Nevertheless, our method still achieves\\nbetter performances than them, proving the effectiveness of\\nour method.\\n2) Ablation Studies on Fully-Supervised Components: The\\nﬁrst ablation study is to show the effectiveness of isolating\\nlocalization with segmentation, as reported in Table III. Item\\n“w/o kernel” means that the attention-fused language-visual\\nfeatures are directly provided to the segmentation predictions\\n(similar to [6], [10]). Item “w/ kernel” means that the kernel\\nhead is introduced as the bridge to isolate the position head and\\nsegmentation head, which is our method. The ﬁnal mIoU score\\non the validation set of RefCOCO increases from 68.27% to\\n70.87%, with a gain of 2.60%, proving the effectiveness of\\nisolating the two modules via the kernel head.\\nThe second ablation study is on MSF. This study is con-\\nducted on the validation set of RefCOCO, and the results are\\nreported in Table IV. We implement three different language\\nfusion strategies in addition to our MSF module. “concat”\\nmeans that the language and visual features are simultaneously\\n9\\na) Image\\nb) GT\\ne) VLT\\nd) MCN\\nf) Ours\\nc) CMPC\\n“green \\nwoman”\\n“lady \\nsitting”\\n“lady \\non right”\\n“zebra \\non left”\\n“suitcase \\nsecond from \\nright”\\n“man”\\n“girl in \\npurple”\\n“blue shirt”\\n“left \\nsandwich”\\n“woman \\non left”\\n“quilt”\\n“tennis player \\nin white shirt”\\ncases:\\n7\\n6\\n5\\n3\\n4\\n2\\n1\\n12\\n11\\n10\\n8\\n9\\nFig. 7. Qualitative comparisons between (f) our method and other SOTA methods: (c) CMPC [13], (d) MCN [67] and (e) VLT [4], on the validation set\\nof RefCOCO [20]. The segmentation results are obtained based on the expression at the bottom of each column. Note that (a) Image indicates the original\\nimage and (b) GT indicates the ground-truth segmentation mask.\\nconcatenated on the channel dimension, followed by one FCL\\nto keep the same dimension size in each layer. “multipy”\\nmeans that the language and visual features are element-\\nwise and simultaneously multiplied on the channel dimension,\\nfollowed by one FCL. “mean” represents that the mean feature\\nvalue of all words is used to present the entire sentence in our\\nMSF. The results prove that our MSF strategy is the most\\neffective feature fusion solution for our method.\\nWe also explore the inﬂuence of each component in the\\nproposed method (i.e., multi-level and multi-step). When the\\n“multi-step” part is disabled, the mIoU score descends from\\n70.87% to 67.52%. The result demonstrates that “multi-step”,\\nwhich allocates different phrases with different attention values\\nin different steps, renders language-vision fusion more effec-\\ntive than the traditional one-step fusion process. On the other\\nhand, when the “multi-level” part is forbidden, the mIoU score\\ndeclines by 3.44%, indicating that comprehensively using the\\nvisual features extracted under different resolutions can better\\nlocalize the target objects, especially for the minute features.\\nThe third ablation study is examines how the number of\\nsteps J in Eq.(1) affects the segmentation performance of the\\nproposed method. As shown in Table V, the proposed method\\nachieves the mIoU score of 61.61% when J=1 without multi-\\nlevel features. The performance is improved when the number\\nof steps increases, demonstrating the necessity of the MSF\\nmechanism. When J increases from 3 to 4, the improvement\\nis limited, thus J=4 is selected to balance performance and\\ncomplexity. A similar accuracy change is observed when\\nmulti-level features are adopted, indicating the robustness of\\nthe proposed method.\\nIn the last ablation study, we evaluate the performance when\\n“dish in top right corner”  \\n“lady sitting”  \\n“lady on right”  \\n“black cat on left”  \\na) Image\\nb) GT\\nc) Level-1\\nd) Level-2\\ne) Level-3\\nf) Level-4\\ng) Level-5\\nFig. 8.\\nVisualization of the position heatmaps on different feature levels,\\nwhere red areas represent the target regions while blue areas indicate the\\nnon-target regions. A larger level number represents a deeper feature, with a\\nsmaller resolution.\\nvisual features with a certain range of levels are adopted.\\nDuring the training stage, only the features of the selected\\nlevels are used, while in the inference period, Matrix NMS [51]\\nis conducted on the predictions of the selected levels to localize\\nthe target object. As shown in Table VI, the accuracy improves\\nwhen more levels of the features are adopted in the position\\nhead. The highest mIoU score of 70.87% is achieved when\\nall levels (“level 1-5”) of features are adopted, showing that\\nvisual features of all levels jointly contribute to localizing the\\ntarget objects.\\n10\\nTABLE VIII\\nCOMPARISONS BETWEEN THE PROPOSED METHOD AND OTHER METHODS ACCORDING TO THE MIOU SCORE, ON REFCOCO [20], REFCOCO+ [20]\\nAND REFCOCOG [21], [22] DATASETS. F REPRESENTS FULLY-SUPERVISED RESULTS, AND W REPRESENTS WEAKLY-SUPERVISED RESULTS.\\nRefCOCO\\nRefCOCO+\\nRefCOCOg\\nval\\ntest A\\ntest B\\nval\\ntest A\\ntest B\\nval (U)\\ntest (U)\\nval(G)\\nVLT [4]\\nF\\n65.65\\n68.29\\n62.73\\n55.50\\n59.20\\n49.36\\n52.99\\n56.65\\n49.76\\nPKS\\nF\\n70.87\\n74.23\\n65.07\\n60.90\\n66.21\\n51.35\\n60.38\\n60.98\\n56.97\\nVLT [4]\\nW\\n45.23\\n46.32\\n44.57\\n31.74\\n34.83\\n27.43\\n33.20\\n32.63\\n30.67\\nPKS\\nW\\n49.27\\n52.23\\n45.64\\n37.79\\n42.09\\n32.87\\n40.98\\n40.80\\n36.43\\nTABLE IX\\nABLATION STUDIES ON THE INFLUENCE OF LABELS AND GCRF LOSS, FOR THE WEAKLY-SUPERVISED SETTING, CONDUCTED ON THE VALIDATION SET\\nOF REFCOCO.\\nmodules\\nprec@X\\nmIoU\\np@0.1\\np@0.2\\np@0.3\\np@0.4\\np@0.5\\np@0.6\\np@0.7\\np@0.8\\np@0.9\\nFully-Supervised\\n85.40\\n84.54\\n83.57\\n82.12\\n80.13\\n77.67\\n73.15\\n64.60\\n35.87\\n70.87\\nbaseline + point FG\\n28.24\\n14.01\\n5.96\\n2.04\\n0.69\\n0.16\\n0.05\\n0.00\\n0.00\\n7.30\\nbaseline + point FG + BG\\n73.35\\n57.18\\n29.40\\n9.42\\n2.02\\n0.35\\n0.03\\n0.00\\n0.00\\n21.11\\nbaseline + superpixel FG + BG\\n76.98\\n74.07\\n70.50\\n64.80\\n56.21\\n42.47\\n25.61\\n8.60\\n0.75\\n46.25\\nbaseline + superpixel FG + BG + GCRF loss\\n80.18\\n77.90\\n74.77\\n69.69\\n61.53\\n48.35\\n28.56\\n9.56\\n0.78\\n49.27\\n3) Analysis on Upper-bound: We also analyse the upper-\\nbound of our method conducted on the validation set of\\nRefCOCO in Table VII. The upper-bound denotes that the\\nground-truth position of the target object is utilized in the\\nkernel selection process in Sect.III-C. The performance dif-\\nference between the upper-bound and the standard setting in\\nour method on position is 13.15% at prec@0.5 (prec@0.5 is\\nthe normal overlap threshold of correct ﬁndings). Even with\\nthe ground-truth position, the proposed method achieves an\\nmIoU of 81.91%, where the predicted masks still do not fully\\noverlap the ground-truth masks.\\n4) Qualitative Analysis for Fully-Supervised RES:\\nThe\\nqualitative comparison between our method and other SOTA\\nmethods, including CMPC [13], VLT [4], and MCN [67], is\\nshown in Fig.7. Different from the broken masks predicted\\nby CMPC [13], the proposed method generates more intact\\nresults, such as “suitcase” in Case (4) and “green woman” in\\nCase (7). Compared with MCN [67], our proposed method\\npredicts better contours, such as “lady sitting” in Case (6)\\nand “quilt” in Case (8). Compared with VLT [4], whose\\nprediction covers other distracting objects similar to the target,\\nthe proposed method correctly distinguishes the target from\\nother distracting objects, such as “girl in purple” in Case (1)\\nand “green woman” in Case (7).\\nThe visualization of the position heatmaps corresponding\\nto different levels is provided in Fig.8. As observed, the\\ntarget center is adequately localized in all heatmaps. The\\nlow-resolution deep-level features (such as level-5), which\\ncontains more semantic information, take more responsibility\\nfor identifying the main region of the object. However, the\\nlow-level feature, with a larger resolution, can predict a more\\nprecise position heatmap, especially for irregular, long-shaped\\nobjects. In this way, all these features are complementary,\\nwhich jointly contributes to the localization of the target.\\nIn addition, four failed cases are illustrated in Fig.9. In\\nthe left two cases, the target object is not properly localized,\\nand the distracting object is predicted. Speciﬁcally, the ﬁrst\\ncase shows that it is arduous for the proposed method to\\n“left kid”  \\n“pink”  \\na) Image\\nc) GT\\nd) Ours\\n“left half of \\nright sandwich”  \\nb) Pos\\n“third from left”  \\nFig. 9. Visualization of failed cases on the validation set of RefCOCO [20].\\nNote that GT indicates the ground truth segmentation mask and Pos indicates\\nthe position prediction. Best viewed in colors.\\nhandle ordinal numbers (e.g., “third”), as these words are not\\ncommon in the training set. In the second case, the proposed\\nmethod fails as too many spatial descriptions occur in a single\\nreferring sentence (e.g., “left half” and “right”). The right two\\ncases show that the segmentation quality needs to be further\\nimproved, especially when the target object contour is hard to\\nidentify, or the target object consists of several separate parts.\\nD. Experimental Results for Weakly-Supervised Setting\\n1) Comparison with Other Methods: Table VIII compares\\nthe mIoU scores between the proposed method and VLT [4],\\non both fully-supervised (F) and weakly-supervised (W) RES.\\nThe fully-supervised VLT is trained with the originally pub-\\nlished code, and the weakly-supervised VLT is trained with\\nsuperpixel masks by replacing CE loss with pCE loss. As\\nobserved, our method achieves clear gains for both fully-\\nsupervised and weakly-supervised settings.\\n11\\n“left zebra”\\n“gitl in \\nbikini on \\nright”\\n“broc \\nbottom \\nright”\\n“woman \\non left \\nstanding”\\n“lady in \\npurpl \\nasian”\\n“catcher”\\n“batter”\\n“guy near \\nwall”\\n“broken \\nhot dog”\\n“top comic \\nnear front”\\n“last book \\non top \\nright”\\n“left green shirt \\nbending down”\\n（a) \\n（b) \\n（c) \\n（d) \\n（e) \\nFig. 10.\\nQualitative results of weakly-supervised RES on the validation set of RefCOCO. (a) Original image. (b) Fully-supervised masks, white regions\\nare the foreground and black regions are the background. (c) Weakly-supervised masks, the grey regions have unknown labels for the training process. (d)\\nWeakly-supervised predictions.\\n2) Ablation Studies on a Weakly-Supervised Setting: Table\\nIX reports the performances when some components are\\ndisabled, to show their effectiveness. “Point FG” represents\\nthe setting that the ground-truth labels only contain the center\\npoint as the foreground. “Point FG + BG” means that the\\nground-truth labels contain both the foreground points and\\nbackground points. “superpixel FG + BG” means that the\\nsuperpixel method is used to expand the foreground labels\\nto replace the center point foreground. “superpixel FG + BG\\n+ GCRF loss” represents that the training optimization adopts\\nthe GCRF loss. As observed, with only the center point as the\\nforeground, a mIoU score of 7.30% is achieved. After adding\\nthe background points, the mIoU score increases to 21.11%,\\nshowing that the foreground labels do not contain enough\\ninformation and that background annotation is necessary for\\nweakly-supervised RES training process. When replacing the\\ncenter point with the superpixel foreground, a major improve-\\nment from 21.11% to 46.25% is obtained. After adopting\\nthe GCRF loss, the mIoU score increases to 49.27%. We\\nobserve that the gap between the fully-supervised and weakly-\\nsupervised results is 21.60%, indicating that there is still room\\nfor further improvement for weakly-supervised RES.\\n3) Qualitative Analysis for Weakly-Supervised RES: Fig.10\\nprovides examples of the weakly-supervised RES results.\\nThe target objects are sufﬁciently localized based on the\\nexpression, which shows that our position head is performed\\nwell with the click annotation as supervision. However, the\\npredicted masks are not precise, and the details of the objects\\nare missing, due to a lack of detailed ground-truth masks.\\nWe suggest that the segmentation head could be further\\nimproved, especially when the target object shape is complex.\\nFor instance, object saliency information could be used to\\nimprove the object boundary, which will be addressed in future\\nresearch.\\nVI. CONCLUSION\\nIn this paper, we have individually analysed the two main\\ncomponents of referring expression segmentation: localization\\nand segmentation. We propose a position-kernel-segmentation\\nframework to train the localization and segmentation process\\nin parallel and then interact the localization results with\\nthe segmentation process via a visual kernel. Our paral-\\nlel framework could generate complete, smooth and precise\\nmasks, achieving state-of-the-art performances with more than\\n4.5% mIoU gain over previous SOTA methods on three\\nmain RES datasets. Our framework also enables training\\nRES in a weakly-supervised way, where the position head\\nis fully-supervised and trained with the click annotations as\\nsupervision, and the segmentation head is trained with weak\\nsegmentation losses. We annotate three benchmark datasets\\nfor weakly-supervised training, and achieve satisfactory per-\\nformances.\\nREFERENCES\\n[1] C. Liu, Z. Lin, X. Shen, J. Yang, X. Lu, and A. Yuille, “Recurrent\\nmultimodal interaction for referring image segmentation,” in Proc. Int.\\nConf. Comput. Vis., 2017.\\n[2] A. Kamath, M. Singh, Y. LeCun, G. Synnaeve, I. Misra, and N. Carion,\\n“Mdetr-modulated detection for end-to-end multi-modal understanding,”\\nin Proc. Int. Conf. Comput. Vis., 2021.\\n[3] S. Yang, M. Xia, G. Li, H. Zhou, and Y. Yu, “Bottom-up shift and\\nreasoning for referring image segmentation,” in Proc. Conf. Comput.\\nVis. Pattern Recog., 2021.\\n[4] H. Ding, C. Liu, S. Wang, and X. Jiang, “Vision-language transformer\\nand query generation for referring segmentation,” in Proc. Int. Conf.\\nComput. Vis., 2021.\\n[5] G. Feng, Z. Hu, L. Zhang, and H. Lu, “Encoder fusion network with co-\\nattention embedding for referring image segmentation,” in Proc. Conf.\\nComput. Vis. Pattern Recog., 2021.\\n[6] Y. Jing, T. Kong, W. Wang, L. Wang, L. Li, and T. Tan, “Locate then\\nsegment: A strong pipeline for referring image segmentation,” in Proc.\\nConf. Comput. Vis. Pattern Recog., 2021.\\n[7] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. S¨underhauf,\\nI. Reid, S. Gould, and A. Van Den Hengel, “Vision-and-language\\nnavigation: Interpreting visually-grounded navigation instructions in real\\nenvironments,” in Proc. Conf. Comput. Vis. Pattern Recog., 2018.\\n[8] K. Ning, L. Xie, F. Wu, and Q. Tian, “Polar relative positional encoding\\nfor video-language segmentation.” in Proc. Int. J. Conf. Artif. Intell.,\\n2020.\\n[9] W. Zhang, C. Ma, Q. Wu, and X. Yang, “Language-guided navigation via\\ncross-modal grounding and alternate adversarial learning,” IEEE Trans.\\nCircuit Syst. Video Technol., vol. 31, no. 9, pp. 3469–3481, 2021.\\n12\\n[10] Q. Li, Y. Zhang, S. Sun, J. Wu, X. Zhao, and M. Tan, “Cross-\\nmodality synergy network for referring expression comprehension and\\nsegmentation,” Neurocomputing, vol. 467, pp. 99–114, 2022.\\n[11] R. Li, K. Li, Y.-C. Kuo, M. Shu, X. Qi, X. Shen, and J. Jia, “Referring\\nimage segmentation via recurrent reﬁnement networks,” in Proc. Conf.\\nComput. Vis. Pattern Recog., 2018.\\n[12] L. Ye, M. Rochan, Z. Liu, and Y. Wang, “Cross-modal self-attention\\nnetwork for referring image segmentation,” in Proc. Conf. Comput. Vis.\\nPattern Recog., 2019.\\n[13] S. Huang, T. Hui, S. Liu, G. Li, Y. Wei, J. Han, L. Liu, and B. Li, “Refer-\\nring image segmentation via cross-modal progressive comprehension,”\\nin Proc. Conf. Comput. Vis. Pattern Recog., 2020.\\n[14] Z. Hu, G. Feng, J. Sun, L. Zhang, and H. Lu, “Bi-directional relationship\\ninferring network for referring image segmentation,” in Proc. Conf.\\nComput. Vis. Pattern Recog., 2020.\\n[15] D. Lin, J. Dai, J. Jia, K. He, and J. Sun, “Scribblesup: Scribble-\\nsupervised convolutional networks for semantic segmentation,” in Proc.\\nConf. Comput. Vis. Pattern Recog., 2016.\\n[16] B. Zhang, J. Xiao, Y. Wei, M. Sun, and K. Huang, “Reliability\\ndoes matter: An end-to-end weakly supervised semantic segmentation\\napproach,” in Proc. Assoc. Adv. Artif. Intell., 2020.\\n[17] B. Zhang, J. Xiao, J. Jiao, Y. Wei, and Y. Zhao, “Afﬁnity attention\\ngraph neural network for weakly supervised semantic segmentation,”\\nIEEE Trans. Pattern Anal. Mach. Intell., pp. 1–1, 2021.\\n[18] X. Tian, K. Xu, X. Yang, B. Yin, and R. W. Lau, “Weakly-supervised\\nsalient instance detection,” in Proc. Brit. Mach. Vis. Conf., 2020.\\n[19] S. Yu, B. Zhang, J. Xiao, and E. G. Lim, “Structure-consistent weakly\\nsupervised salient object detection with local saliency coherence,” in\\nProc. Assoc. Adv. Artif. Intell., 2021.\\n[20] L. Yu, P. Poirson, S. Yang, A. C. Berg, and T. L. Berg, “Modeling context\\nin referring expressions,” in Proc. Eur. Conf. Comput. Vis., 2016.\\n[21] J. Mao, J. Huang, A. Toshev, O. Camburu, A. L. Yuille, and K. Murphy,\\n“Generation and comprehension of unambiguous object descriptions,” in\\nProc. Conf. Comput. Vis. Pattern Recog., 2016.\\n[22] V. K. Nagaraja, V. I. Morariu, and L. S. Davis, “Modeling context\\nbetween objects for referring expression understanding,” in Proc. Eur.\\nConf. Comput. Vis., 2016.\\n[23] Y. Yang, Y. Zhuang, and Y. Pan, “Multiple knowledge representation\\nfor big data artiﬁcial intelligence: framework, applications, and case\\nstudies,” Frontiers of Information Technology & Electronic Engineering,\\nvol. 22, no. 12, pp. 1551–1558, 2021.\\n[24] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick,\\nand D. Parikh, “VQA: Visual question answering,” in Proc. Int. Conf.\\nComput. Vis., 2015.\\n[25] H. Jiang, I. Misra, M. Rohrbach, E. Learned-Miller, and X. Chen, “In\\ndefense of grid features for visual question answering,” in Proc. Conf.\\nComput. Vis. Pattern Recog., 2020.\\n[26] T. M. Le, V. Le, S. Venkatesh, and T. Tran, “Hierarchical conditional\\nrelation networks for video question answering,” in Proc. Conf. Comput.\\nVis. Pattern Recog., 2020.\\n[27] A. Bansal, Y. Zhang, and R. Chellappa, “Visual question answering on\\nimage sets,” in Proc. Eur. Conf. Comput. Vis., 2020.\\n[28] L. Chen, X. Yan, J. Xiao, H. Zhang, S. Pu, and Y. Zhuang, “Counter-\\nfactual samples synthesizing for robust visual question answering,” in\\nProc. Conf. Comput. Vis. Pattern Recog., 2020.\\n[29] J. Kim, M. Ma, T. Pham, K. Kim, and C. D. Yoo, “Modality shifting\\nattention network for multi-modal video question answering,” in Proc.\\nConf. Comput. Vis. Pattern Recog., 2020.\\n[30] X. Wang, Y. Liu, C. Shen, C. C. Ng, C. Luo, L. Jin, C. S. Chan,\\nA. v. d. Hengel, and L. Wang, “On the general value of evidence, and\\nbilingual scene-text visual question answering,” in Proc. Conf. Comput.\\nVis. Pattern Recog., 2020.\\n[31] K. Andrej and F.-F. Li, “Deep visual-semantic alignments for generating\\nimage descriptions,” in Proc. Conf. Comput. Vis. Pattern Recog., 2015.\\n[32] Z. Wang, X. Liu, H. Li, L. Sheng, J. Yan, X. Wang, and J. Shao, “CAMP:\\nCross-modal adaptive message passing for text-image retrieval,” in Proc.\\nInt. Conf. Comput. Vis., 2019.\\n[33] H. Chen, G. Ding, X. Liu, Z. Lin, J. Liu, and J. Han, “IMRAM: Iterative\\nmatching with recurrent attention memory for cross-modal image-text\\nretrieval,” in Proc. Conf. Comput. Vis. Pattern Recog., 2020.\\n[34] W. Su, X. Zhu, Y. Cao, B. Li, L. Lu, F. Wei, and J. Dai, “VL-BERT:\\nPre-training of generic visual-linguistic representations,” in Proc. Int.\\nConf. Learn. Represent., 2020.\\n[35] J. Lu, D. Batra, D. Parikh, and S. Lee, “ViLBERT: Pretraining task-\\nagnostic visiolinguistic representations for vision-and-language tasks,”\\nin Proc. Adv. Neural Inform. Process. Syst., 2019.\\n[36] H. Tan and M. Bansal, “LXMERT: Learning cross-modality encoder\\nrepresentations from transformers,” in Proc. Conf. Empirical Methods\\nNatural Lang. Process., 2019.\\n[37] F. Huang, X. Zhang, Z. Zhao, and Z. Li, “Bi-directional spatial-\\nsemantic attention networks for image-text matching,” IEEE Trans.\\nImage Process., vol. 28, no. 4, pp. 2008–2020, 2019.\\n[38] H. Li, J. Xiao, M. Sun, E. G. Lim, and Y. Zhao, “Transformer-\\nbased language-person search with multiple region slicing,” IEEE Trans.\\nCircuit Syst. Video Technol., vol. 32, no. 3, pp. 1624–1633, 2022.\\n[39] K. Wen, X. Gu, and Q. Cheng, “Learning dual semantic relations with\\ngraph attention for image-text matching,” IEEE Trans. Circuit Syst.\\nVideo Technol., pp. 1–1, 2020.\\n[40] A. Rohrbach, M. Rohrbach, R. Hu, T. Darrell, and B. Schiele, “Ground-\\ning of textual phrases in images by reconstruction,” in Proc. Eur. Conf.\\nComput. Vis., 2016.\\n[41] L. Yu, Z. Lin, X. Shen, J. Yang, X. Lu, M. Bansal, and T. L.\\nBerg, “MAttNet: Modular attention network for referring expression\\ncomprehension,” in Proc. Conf. Comput. Vis. Pattern Recog., 2018.\\n[42] Z. Yang, B. Gong, L. Wang, W. Huang, D. Yu, and J. Luo, “A fast and\\naccurate one-stage approach to visual grounding,” in Proc. Int. Conf.\\nComput. Vis., 2019.\\n[43] X. Liu, Z. Wang, J. Shao, X. Wang, and H. Li, “Improving referring\\nexpression grounding with cross-modal attention-guided erasing,” in\\nProc. Conf. Comput. Vis. Pattern Recog., 2019.\\n[44] M. Sun, J. Xiao, E. G. Lim, S. Liu, and J. Y. Goulermas, “Discrimina-\\ntive triad matching and reconstruction for weakly referring expression\\ngrounding,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 43, no. 11, pp.\\n4189–4195, 2021.\\n[45] A. Khoreva, A. Rohrbach, and B. Schiele, “Video object segmentation\\nwith language referring expressions,” in Proc. Asian Conf. Comput. Vis.,\\n2018.\\n[46] S. Seo, J.-Y. Lee, and B. Han, “URVOS: Uniﬁed referring video object\\nsegmentation network with a large-scale benchmark,” in Proc. Eur. Conf.\\nComput. Vis., 2020.\\n[47] H. Wang, Y. Zhang, Z. Ji, Y. Pang, and L. Ma, “Consensus-aware\\nvisual-semantic embedding for image-text matching,” in Proc. Eur. Conf.\\nComput. Vis., 2020.\\n[48] T. N. Kipf and M. Welling, “Semi-supervised classiﬁcation with graph\\nconvolutional networks,” Proc. Int. Conf. Learn. Represent., 2017.\\n[49] L. Zhu, H. Fan, Y. Luo, M. Xu, and Y. Yang, “Temporal cross-layer\\ncorrelation mining for action recognition,” IEEE Trans. Multimedia,\\nvol. 24, pp. 668–676, 2022.\\n[50] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick, “Mask R-CNN,” in Proc.\\nInt. Conf. Comput. Vis., 2017.\\n[51] X. Wang, R. Zhang, T. Kong, L. Li, and C. Shen, “SOLOv2: Dynamic\\nand fast instance segmentation,” in Proc. Adv. Neural Inform. Process.\\nSyst., 2020.\\n[52] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and\\nB. Guo, “Swin transformer: Hierarchical vision transformer using shifted\\nwindows,” in Proc. Int. Conf. Comput. Vis., 2021.\\n[53] Y. Wei, J. Feng, X. Liang, M.-M. Cheng, Y. Zhao, and S. Yan,\\n“Object region mining with adversarial erasing: A simple classiﬁcation\\nto semantic segmentation approach,” in Proc. Conf. Comput. Vis. Pattern\\nRecog., 2017.\\n[54] Y.-J. Liu, M. Yu, B.-J. Li, and Y. He, “Intrinsic manifold slic: A simple\\nand efﬁcient method for computing content-sensitive superpixels,” IEEE\\nTrans. Pattern Anal. Mach. Intell., vol. 40, no. 3, pp. 653–666, 2018.\\n[55] M. Tang, A. Djelouah, F. Perazzi, Y. Boykov, and C. Schroers, “Normal-\\nized cut loss for weakly-supervised cnn segmentation,” in Proc. Conf.\\nComput. Vis. Pattern Recog., 2018.\\n[56] T.-Y. Lin, P. Doll´ar, R. Girshick, K. He, B. Hariharan, and S. Belongie,\\n“Feature pyramid networks for object detection,” in Proc. Conf. Comput.\\nVis. Pattern Recog., 2017.\\n[57] J. Pennington, R. Socher, and C. D. Manning, “GLOVE: Global vectors\\nfor word representation,” in Proc. Conf. Empirical Methods Natural\\nLang. Process., 2014.\\n[58] K. Cho, B. van Merri¨enboer, D. Bahdanau, and Y. Bengio, “On the\\nproperties of neural machine translation: Encoder–decoder approaches,”\\nin Proc. SSST-8, 2014.\\n[59] R. Liu, J. Lehman, P. Molino, F. Petroski Such, E. Frank, A. Sergeev,\\nand J. Yosinski, “An intriguing failing of convolutional neural networks\\nand the coordconv solution,” in Proc. Adv. Neural Inform. Process. Syst.,\\n2018.\\n[60] A. Kirillov, R. Girshick, K. He, and P. Doll´ar, “Panoptic feature pyramid\\nnetworks,” in Proc. Conf. Comput. Vis. Pattern Recog., 2019.\\n13\\n[61] A. Bearman, O. Russakovsky, V. Ferrari, and L. Fei-Fei, “What’s the\\npoint: Semantic segmentation with point supervision,” in Proc. Eur.\\nConf. Comput. Vis., 2016.\\n[62] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisser-\\nman, “The pascal visual object classes (voc) challenge,” Int. J. Comput.\\nVis., vol. 88, no. 2, pp. 303–338, 2010.\\n[63] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll´ar, “Focal loss for\\ndense object detection,” in Proc. Int. Conf. Comput. Vis., 2017.\\n[64] F. Milletari, N. Navab, and S.-A. Ahmadi, “V-Net: Fully convolutional\\nneural networks for volumetric medical image segmentation,” in Proc.\\n3DV, 2016.\\n[65] E. Margffoy-Tuay, J. C. P´erez, E. Botero, and P. Arbel´aez, “Dynamic\\nmultimodal instance segmentation guided by natural language queries,”\\nin Proc. Eur. Conf. Comput. Vis., 2018.\\n[66] T. Hui, S. Liu, S. Huang, G. Li, S. Yu, F. Zhang, and J. Han, “Linguistic\\nstructure guided context modeling for referring image segmentation,” in\\nProc. Eur. Conf. Comput. Vis., 2020.\\n[67] G. Luo, Y. Zhou, X. Sun, L. Cao, C. Wu, C. Deng, and R. Ji, “Multi-\\ntask collaborative network for joint referring expression comprehension\\nand segmentation,” in Proc. Conf. Comput. Vis. Pattern Recog., 2020.\\n[68] G. Luo, Y. Zhou, R. Ji, X. Sun, J. Su, C.-W. Lin, and Q. Tian, “Cascade\\ngrouped attention network for referring expression segmentation,” in\\nProc. ACM Int. Conf. Multimedia, 2020.\\n[69] N. Kim, D. Kim, C. Lan, W. Zeng, and S. Kwak, “Restr: Convolution-\\nfree referring image segmentation using transformers,” in Proc. Conf.\\nComput. Vis. Pattern Recog., 2022.\\n[70] A. Obukhov, S. Georgoulis, D. Dai, and L. Van Gool, “Gated CRF loss\\nfor weakly supervised semantic image segmentation,” CoRR, 2019.\\n[71] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\\nrecognition,” in Proc. Conf. Comput. Vis. Pattern Recog., 2016.\\n')])\n",
      "ResearchTopics(topic='Real time image segmentation using lightweight convolutional networks and boundary refinement', priority=2, query='lightweight realtime segmentation', timestamp='2024-11-23 18:21:07', research_papers=[ResearchPaper(title='LRNNet: A Light-Weighted Network with Efficient Reduced Non-Local Operation for Real-Time Semantic Segmentation', authors=[arxiv.Result.Author('Weihao Jiang'), arxiv.Result.Author('Zhaozhi Xie'), arxiv.Result.Author('Yaoyi Li'), arxiv.Result.Author('Chang Liu'), arxiv.Result.Author('Hongtao Lu')], abstract='The recent development of light-weighted neural networks has promoted the\\napplications of deep learning under resource constraints and mobile\\napplications. Many of these applications need to perform a real-time and\\nefficient prediction for semantic segmentation with a light-weighted network.\\nThis paper introduces a light-weighted network with an efficient reduced\\nnon-local module (LRNNet) for efficient and realtime semantic segmentation. We\\nproposed a factorized convolutional block in ResNet-Style encoder to achieve\\nmore lightweighted, efficient and powerful feature extraction. Meanwhile, our\\nproposed reduced non-local module utilizes spatial regional dominant singular\\nvectors to achieve reduced and more representative non-local feature\\nintegration with much lower computation and memory cost. Experiments\\ndemonstrate our superior trade-off among light-weight, speed, computation and\\naccuracy. Without additional processing and pretraining, LRNNet achieves 72.2%\\nmIoU on Cityscapes test dataset only using the fine annotation data for\\ntraining with only 0.68M parameters and with 71 FPS on a GTX 1080Ti card.', url='http://arxiv.org/abs/2006.02706v1', pdf_path='./papers/2006.02706v1.LRNNet__A_Light_Weighted_Network_with_Efficient_Reduced_Non_Local_Operation_for_Real_Time_Semantic_Segmentation.pdf', content='LRNNET: A LIGHT-WEIGHTED NETWORK WITH EFFICIENT REDUCED NON-LOCAL\\nOPERATION FOR REAL-TIME SEMANTIC SEGMENTATION\\nWeihao Jiang\\nZhaozhi Xie\\nYaoyi Li\\nChang Liu\\nHongtao Lu†\\nDepartment of Computer Science and Engineering, MoE\\nKey Lab of Articial Intelligence, AI Institute, Shanghai Jiao Tong University, China\\n{jiangweihao, xiezhzh, dsamuel, isonomialiu, htlu }@sjtu.edu.cn\\nABSTRACT\\nThe recent development of light-weighted neural networks\\nhas promoted the applications of deep learning under resource\\nconstraints and mobile applications. Many of these applica-\\ntions need to perform a real-time and efﬁcient prediction for\\nsemantic segmentation with a light-weighted network. This\\npaper introduces a light-weighted network with an efﬁcient\\nreduced non-local module (LRNNet) for efﬁcient and real-\\ntime semantic segmentation. We proposed a factorized convo-\\nlutional block in ResNet-Style encoder to achieve more light-\\nweighted, efﬁcient and powerful feature extraction. Mean-\\nwhile, our proposed reduced non-local module utilizes spatial\\nregional dominant singular vectors to achieve reduced and\\nmore representative non-local feature integration with much\\nlower computation and memory cost. Experiments demon-\\nstrate our superior trade-off among light-weight, speed, com-\\nputation and accuracy. Without additional processing and pre-\\ntraining, LRNNet achieves 72.2% mIoU on Cityscapes test\\ndataset only using the ﬁne annotation data for training with\\nonly 0.68M parameters and with 71 FPS on a GTX 1080Ti\\ncard.\\nIndex Terms— Light-weighted semantic segmentation,\\nreduced non-local, factorized convolution\\n1. INTRODUCTION\\nSemantic segmentation can be viewed as a task of pixel-wise\\nclassiﬁcation, which assigns a speciﬁc pre-deﬁned category to\\neach pixel in an image. The task has many potential applica-\\ntions in autonomous driving or image editing and so on. Al-\\nthough many works [1, 2, 3] have made great progress in the\\naccuracy of image semantic segmentation tasks, their network\\nsize, inference speed, computation and memory cost limit\\ntheir practical applications. Therefore, it’s essential to de-\\nvelop the light-weighted, efﬁcient and real-time methods for\\nsemantic segmentation. Among these properties, light-weight\\ncould be the most essential one, because using a smaller scale\\nnetwork can lead to faster speed and more efﬁcient in com-\\nputation or memory cost easier.\\nA less learnable network\\n†Corresponding author\\nFig. 1. Inference speed, accuracy and learnable network pa-\\nrameters on Cityscapes [4] test set. The smaller bubble means\\nfewer parameters. We compare LRNNet with methods im-\\nplemented in open-source deep-learning frameworks, such\\nas Pytorch [5] and Caffe, including ICNet [6], CGNet [7],\\nERFNet [8], SegNet [9], ENet [10] and LEDNet [11].\\n.\\nparameter means that the network has less redundancy and\\nnetwork structure makes its parameters more effective. Prac-\\ntically, a smaller model size is more favorable for cellphone\\napps. Designing blocks with proper factorized convolution to\\nconstruct an effective light-weighted network could be a more\\nscalable way and could be easier to balance among accuracy,\\nnetwork size, speed, and efﬁciency. In this way, many works\\n[10, 12, 8, 11, 13] achieve promising results and show their\\npotential for many potential applications. But those works\\n[8, 11] do not balance factorized convolution and long-range\\nfeatures in a proper way.\\nThe recent study [14] shows the powerful potential of at-\\ntention mechanism in computer vision. Non-local methods\\nare employed to model long-range dependencies in seman-\\ntic segmentation [2]. However, modeling relationships be-\\ntween every position could be rather heavy in computation\\nand memory cost.\\nSome works try to develop factorized\\n[3] or reduced [15] non-local methods to make it more efﬁ-\\ncient. Since efﬁcient non-local or positional attention is not\\ndeveloped enough for light-weighted and efﬁcient semantic\\nsegmentation, our approach tries to develop a powerful re-\\nduced non-local method to model long-range dependencies\\nand global feature selection efﬁciently.\\n978-1-7281-1485-9/20/$31.00 2020 IEEE\\narXiv:2006.02706v1  [cs.CV]  4 Jun 2020\\nIn our work, we develop a light-weighted factorized con-\\nvolution block (FCB) (Fig. 4) to build a feature extraction net-\\nwork (encoder), which deals with long-range and short-range\\nfeatures with proper factorized convolution respectively, and\\nwe proposed a powerful reduced non-local module with re-\\ngional singular vectors to model long-range dependencies and\\nglobal feature selection for the features from encoder to en-\\nhance the segmentation results. Contributions are summa-\\nrized as follows:\\n• We proposed a factorized convolution block (FCB) to\\nbuild a very light-weighted, powerful and efﬁcient fea-\\nture extraction network by dealing with long-range and\\nshort-range features in a more proper way.\\n• The proposed efﬁcient reduced non-local module\\n(SVN) utilizes regional singular vectors to produced\\nmore reduced and representative features to model\\nlong-range dependencies and global feature selection.\\n• All experiments show the state-of-the-art trade-off in\\nterms of parameter size, speed, computation and accu-\\nracy of our LRNNet on Cityscapes [4] and Camvid [16]\\ndatasets.\\n2. RELATED WORK\\nLight-weighted and Real-time Segmentation.\\nReal-time\\nsemantic segmentation approaches aim to generate high-\\nquality prediction in limited time, which is usually performed\\nunder resource constraints or mobile applications.\\nLight-\\nweight models save storage space and potentially have lower\\ncomputation and faster speed. Therefore, developing light-\\nweight segmentation is a potential way to get a good trade-off\\nfor real-time semantic segmentation [7, 11, 8]. Our model fol-\\nlows the light-weight style to achieve real-time segmentation.\\nFactorized Convolution.\\nStandard convolution adopts\\na 2D convolution kernel to form a full connection between\\ninput and output channels, which learns local relation and\\nchannel interaction.\\nHowever, this may suffer from the\\nlarge parameter size and redundancy for real-time tasks un-\\nder resource constraints. Xception [17] and MobileNet [18]\\nadopt depthwise separable convolution, which consists of a\\ndepthwise convolution followed by a pointwise convolution.\\nDepthwise convolution learns local relation in every chan-\\nnel and pointwise convolution learns the interaction between\\nchannels to reduce parameters and computation. ShufﬂeNet\\n[19] adopts a split-shufﬂe strategy to reduce parameters and\\ncomputation. In this strategy, standard convolution is split\\ninto some groups of channels and a channel shufﬂe operation\\nhelps the information ﬂows between groups. Factorizing the\\n2D convolution kernel into a combination of two 1D convo-\\nlution kernels is another way to reduce parameter size and\\ncomputation cost. Many light-weight approaches [11, 8] take\\nthis way and get promising performances. In this paper, our\\nconvolution factorization block (FCB) utilizes these strategies\\nto build a light-weighted, efﬁcient and powerful structure.\\nAttention Models. Attention modules model long-range\\ndependencies and have been applied in many computer vision\\ntasks. Position attention and channel attention are two im-\\nportant mechanisms. Channel attention modules are widely\\napplied in semantic segmentation [13] including some light-\\nweighted approaches [7, 13]. Position attention or non-local\\nmethods have a higher computational complexity. Although\\nsome works [3, 20, 15] try to develop more efﬁcient non-local\\nmethods, position attention or non-local methods are rarely\\nexplored in light-weighted semantic segmentation.\\n3. METHODOLOGY\\nWe introduce the preliminary related to our SVN module in\\nsection 3.1, network architecture in section 3.2, the proposed\\nFCB unit in section 3.3 and SVN module in section 3.4.\\n3.1. Preliminary\\nBefore introducing the proposed method, we ﬁrst intro-\\nduce the singular value decomposition and non-local method,\\nwhich are related to our SVN module in section 3.4.\\nSingular Value Decomposition and Approximation.\\nGiven a real matrix A = (aij) ∈Rm×n(m ≥n), with real\\nnumbers σ1 ≥σ2 ≥... ≥σr > 0, there exist two orthogonal\\nmatrices U ∈Rm×m and V ∈Rn×n, satisfying Equation 1,\\nA = UDV T =\\nr\\nX\\ni=1\\nσiuivT\\ni\\n(1)\\nwhere D\\n=\\ndiag{σ1, σ2, ..., σr, 0, ..., 0}m×n,\\nU\\n=\\n{u1, u2, ..., um} and V\\n= {v1, v2, ..., vn}.\\nIf we choose\\nK ≤r, we can get\\nA ≈\\nK\\nX\\ni=1\\nσiuivT\\ni = ˆA\\n(2)\\nwhere ˆ\\nA approximates the original A, because the larger sin-\\ngular values and their singular vectors keep most of the in-\\nformation of A. The corresponding singular vectors of the\\nlarger singular value contain more information of the matrix,\\nespecially the dominant singular vectors. We can calculate\\nthe dominant singular vectors by power iteration Algorithm 1\\nefﬁciently. Based on Equation 1, rotating columns of A does\\nnot change U and ui and their singular values.\\nNon-local Module. Non-local module [14] models global\\nfeature relationships. We illustrate it in the form of Query-\\nKey-Value. It can be formulated as:\\nOi =\\n1\\nC(vj)\\nX\\nj=1\\nSim(qi, kj)vj\\n(3)\\n(a) Our Encoder Network\\n(b) Our Decoder (classiﬁer) with SVN Module\\nFig. 2. Overview of our LRNNet: (a) shows our light-weighted encoder which is constructed by our factorized convolution\\nblock (FCB) in a three-stages ResNet-style. (b) shows our decoder with SVN module. The upper branch performs our non-local\\noperation with regional dominant singular vectors in different scales. The red arrows represent 1 × 1 convolutions adjusting the\\nchannel size to forming an bottleneck structure. The classiﬁer consists a 3 × 3 convolution followed by a 1 × 1 convolution.\\nAlgorithm 1 Power Iteration(A, T)\\n1: Input: A ∈Rm×n, iterations: T and init u ∈Rm\\n2: for i in T do\\n3:\\nu :=\\nu\\n||u||2 , v := AT u\\n4:\\nv :=\\nv\\n||v||2 , u := AvT\\n5: end for\\n6: return u :=\\nu\\n||u||2\\nwhere qi ∈Q, kj ∈K, vj ∈V , qi ∈RC1 is a Query,\\nOi is the corresponding output of qi, kj ∈RC1, is a Key,\\nvi ∈RC2, is a Value, Sim(·, ·) is the measure of similar-\\nity between ki and qi, and C(x) is a normalization function,\\nQ ∈RC1×N1, K ∈RC1×N2 and V ∈RC2×N2 are the col-\\nlections of the Queries, the Keys and the Values, respectively.\\nAnd a smaller N2 means less computation.\\n3.2. Overview of the Network Architecture\\nIn this section, we introduce our network architecture. Our\\nLRNNet consists of a feature extraction network constructed\\nby our proposed factorized convolution block (Fig. 4(c)) and\\na pixel classiﬁer enhanced by our SVN module (Fig. 2).\\nWe form our encoder in a three-stages ResNet-style ( Fig.\\n2(a)). We adopt the same transition between stages as ENet\\n[10] using a downsampling unit. The core components are our\\nFCB units, which provide light-weighted and efﬁcient feature\\nextraction. For better comparison of other light-weight factor-\\nized convolution block, we adopt the same dilation series of\\nFCB in encoder as LEDNet [11] after the last downsampling\\nunit (details in supplemental material). Our decoder ( Fig.\\n2(b)) is a pixel-wise classiﬁer enhanced by our SVN module.\\n3.3. Factorized Convolution Block\\nDesigning a factorized convolution block is a popular way to\\nachieve light-weighted segmentation. Techniques like dilated\\nconvolution for enlarging receptive ﬁeld are also important\\nfor semantic segmentation models. Our factorized convolu-\\ntion block is inspired by the observation that 1D factorized\\nkernel could be more suitable for spatially less informative\\nfeatures than the spatially informative features. Consider the\\nsituation of a 3 × 3 convolution kernel is replaced by a 3 × 1\\nconvolution kernel followed by a 1 × 3 convolution kernel,\\nwhich could have the same receptive ﬁeld and fewer param-\\neters. However, neglecting the information lost of crossing\\nthe activate function between the two 1D convolution kernel,\\nit could be a rank-1 approximation for the 3 × 3 convolution\\nkernel. Assuming that different spatial semantic regions have\\ndifferent features, if the dilation of convolution kernel is one\\nor small, the kernel may not lay across multiple different spa-\\ntial semantic regions and the receptive features are less infor-\\nmative and simple so that the rank-1 approximation is more\\nlikely to be effective, and vice versa.\\nTherefore, the convolution kernel with large dilation will\\nreceive complex or spatially informative long-range features\\n(features separated with a large dilation) in space, and it needs\\nmore parameters in space. Meanwhile, a convolution ker-\\nnel with small dilation will receive simple or less informative\\nshort-range features in space, and fewer parameters in space\\nare enough. Our FCB (Fig. 4(c)) ﬁrst deals with short-range\\nand spatially less informative features with 1D factorized con-\\nvolution in two split groups, which is fully connected in chan-\\nnel, so the factorized convolution reduces the parameter and\\ncomputation a lot. To enlarge the receptive ﬁeld, our FCB\\nutilizes 2D kernel with larger dilation and use depthwise sep-\\narable convolution to reduce the parameter and computation.\\nA channel shufﬂe operation is set at last because there is a\\nresidual connection after the point-wise convolution. In total,\\nFCB uses a low-rank approximation (1D kernel) in space for\\nshort-range features and depth-wise spatial 2D dilated kernel\\nfor long-range features, which lead to more light-weight, ef-\\nﬁcient and powerful feature extraction.\\nCompared with other factorized convolution blocks (Fig.\\n4), our FCB has a more elaborate design, fewer parameters,\\nimage\\nground truth\\nSS-nbt [11]\\nModel A\\nModel B\\nModel C\\nFig. 3. Visual resluts on Cityscapes validation set. More detail results are in supplemental materials.\\nless computation, and faster speed, which will be shown in\\nthe experiment part further.\\n3.4. SVN Module\\nA light-weighted model can hardly achieve powerful feature\\nextraction as a big network. Therefore, to produce reduced,\\nrobust and representative features and combine them into non-\\nlocal modules is an essential way to explore the efﬁcient non-\\nlocal mechanism for light-weighted semantic segmentation.\\nWe revisit the non-local mechanism in the form of Query-\\nKey-Value and claim that using the reduced and representa-\\ntive features as the Keys and the Values could reduce compu-\\ntation and memory, as well as maintain effectiveness.\\nOur SVN module is presented in Fig. 2(b). We reduced\\nthe cost in two ways, which are forming a bottleneck by\\nConv1 and Conv2 to reduced channels for non-local opera-\\ntion and replacing the Keys and Values by their regional dom-\\ninant singular vectors. The proposed SVN consists of two\\nbranches. The lower branch is a residual connection from the\\ninput. The upper branch is the bottleneck of our reduced non-\\nlocal operation. In the bottleneck, we divide the feature maps\\ninto spatial sub-regions. We divide C′ ×H ×W feature maps\\ninto S =\\nH×W\\nH′×W ′ (S ≪N = WH) spatial sub-regions with a\\nscale of C′ ×H′ ×W ′ . For each sub-region, we ﬂatten it into\\na C′×(H′W ′) matrix, then use the Power Iteration Algorithm\\n1 to calculate their left dominant singular vectors (C′×1) efﬁ-\\nciently. As is mentioned in Sec 3.1, rotating columns does not\\naffect the left orthogonal matrix, so the left dominant singular\\nvector is agnostic to the way of ﬂattening and this property is\\nsimilar to pooling. Then the regional dominant singular vec-\\ntors are used as the Keys ∈RC′×S and V alues ∈RC′×S for\\nthe non-local operation, where a smaller S means less com-\\nputation, and the Queries ∈RC′×N are positional vectors\\n(C′ × 1) from the feature maps before dominant singular vec-\\ntor extraction. To enhance the reduced non-local module, we\\nalso perform multi-scale region extraction and gather domi-\\nnant singular vectors from different scales as the Keys and\\nValues (see Fig. 2(b) and Equation 4).\\nOi =\\nX\\nVj,Kj∈S1∪...Sn\\ndot(Qi, Kj)Vj\\n(4)\\n(a) Non-bt-1D\\n(b) SS-nbt\\n(c) Our FCB\\nFig. 4.\\nComparison on factorized convolution with other\\nmethods. The ’R’ is the dilation of kernel and ’D’ represents\\na depth-wise convolution, ’N’ is the umber of output channel.\\n(a) Non-bt-1D of ERFNet [8]. (b) SS-nbt of LEDNet [11]. (c)\\nOur FCB.\\nwhere Oi is the output of SVN, Sns are collections of re-\\ngional dominant singular vectors from their related scales, the\\nregional dominant vectors are used as both the Keys (Kj) and\\nValues (Vj), Qi is a Query from feature maps before domi-\\nnant singular vectors extraction, and our SVN uses dot prod-\\nuct.\\nAs is illustrated above, the SVN module forms a reduced\\nand effective non-local operation by bottleneck structure and\\nreduced and representative regional dominant singular vec-\\ntors. The regional dominant singular vector could be the most\\nrepresentative for a region of feature maps. Since some works\\n[15] utilize pooling as the Keys and Values, we will compare\\nthe pooling, single-scale and multi-scale region extraction in\\nour structure in the ablation experiments.\\n4. EXPERIMENTS\\nWe conduct experiments to demonstrate the performance of\\nour FCB block and SVN module and the state-of-the-art\\ntrade-off among light-weight, accuracy and efﬁciency of our\\nproposed segmentation architecture. For ablation, we denote\\nour LRNNet without SVN, with single-scale SVN and multi-\\nscale SVN as Model A, Model B and Model C, respectively.\\nTable 1. Evaluation on Cityscapes validation set, including\\naccuracy, inference time, parameter size and computation.\\nModel\\nmIoU\\nTimes(ms)\\nPara(M)\\nGFLOPS\\nSS-nbt [11]\\n69.6\\n14\\n0.95\\n11.7\\nModel A\\n70.6\\n13\\n0.67\\n8.48\\nMax Pooling\\n70.2\\n14\\n0.68\\n8.54\\nAvg Pooling\\n70.3\\n14\\n0.68\\n8.54\\nModel B\\n71.1\\n14\\n0.68\\n8.57\\nModel C\\n71.4\\n14\\n0.68\\n8.58\\nTable 2.\\nAblation for sub-region selection of SVN on\\nCityscapes val set comparing with standard non-local. SS\\nmeans single-scale and MS means Multi-scale.\\nSub-region\\nmIoU\\nTimes(ms)\\nGFLOPS\\nnon-local (64×128)\\n71.2\\n22\\n12.5\\nSS (16×16)\\n71.1\\n15\\n8.68\\nSS (8×8)\\n71.1\\n14\\n8.57\\nSS (4×4)\\n70.8\\n14\\n8.51\\nMS (8×8+4×4)\\n71.4\\n14\\n8.58\\nMS (8×8+4×4+2×2)\\n71.4\\n14\\n8.59\\n4.1. Datasets and Settings\\nThe Cityscapes dataset [4] consists of high-quality pixel-level\\nannotations of 5000 street scenes 2048 × 1024 images and\\nthere are 2975, 500 and 1525 images in the training set,\\nvalidation set and test set respectively. Following the light-\\nweighted approaches [11, 8], we adopt 512×1024 subsam-\\npled image for testing. The CamVid dataset[16] contains 367\\ntraining, 101 validating and 233 testing images with a reso-\\nlution of 960×720, but we follow the setting as [10, 9] using\\n480×360 images for training and testing.\\nWe implement all our methods using Pytorch [5] on a sin-\\ngle GTX 1080Ti. Following [21], we employ a poly learning\\nrate policy and the base learning rate is 0.01. The batch size is\\nset to 8 for all training. For CamVid testing and evaluation on\\nCityscapes validation set, we take 250k iterations for training\\nto study our network quickly. And we only train our model on\\nﬁne annotations for Cityscapes test set with 520K iterations.\\n4.2. Ablation Study for FCB\\nComparing with other factorized convolution blocks shown\\nin Figure 4, ERFNet [8] and LEDNet [11] simply use 1D fac-\\ntorized kernel to deal with short-range and long-range (with\\ndilation) features. As is analyzed in Section 3.3, our FCB\\ndeals with short-range features with 1D factorized kernel and\\nlong-range features with the 2D depth-wise kernel. We com-\\npare our FCB (Model A) with SS-nbt from LEDNet [11]\\nin the same architecture. As shown in Table 1 and 4, our\\nTable 3. Evaluation on Cityscapes test set.\\nModel\\nSubsample\\nPre-trained\\nmIoU\\nFPS\\nPara(M)\\nGFLOPS\\nSegNet [9]\\n3\\nN\\n57.0\\n16.7\\n29.5\\n286\\nENet [10]\\n3\\nN\\n58.3\\n135\\n0.37\\n3.8\\nFRRN [22]\\n2\\nN\\n71.8\\n0.25\\n24.8\\n235\\nICNet [6]\\n1\\nY\\n69.5\\n30.3\\n26.5\\n28.3\\nERFNet [8]\\n2\\nN\\n68.0\\n41.7\\n2.1\\n21.0\\nCGNet [7]\\n3\\nY\\n64.8\\n50.0\\n0.5\\n6.0\\nBiSeNet [12]\\n4/3\\nN\\n68.4\\n-\\n5.8\\n14.8\\nDFANet [13]\\n2\\nY\\n70.3\\n-\\n7.8\\n1.7\\nLEDNet [11]\\n2\\nN\\n69.2\\n71\\n0.94\\n11.5\\nModel A\\n2\\nN\\n70.6\\n76.5\\n0.67\\n8.48\\nModel B\\n2\\nN\\n71.6\\n71\\n0.68\\n8.57\\nModel C\\n2\\nN\\n72.2\\n71\\n0.68\\n8.58\\nFCB (Model A) achieves better accuracy with lower parame-\\nter size, computation and inference time comparing with SS-\\nnbt which using 1D factorized kernel for both short-range and\\nlong-range features. Visual examples are in Fig. 3.\\n4.3. Ablation Study for SVN\\nTable 2 shows the performances of different sub-region\\nchoices of our SVN module and the standard non-local. Bal-\\nancing accuracy, speed and computation cost, we choose\\n64 (8×8) sub-region as single-scale SVN (Model B) and\\n8×8+4×4 sub-regions for multi-scale SVN (Model C).\\nWe analyze the efﬁciency of our SVN. Since Algorithm\\n1 converges efﬁciently, we set the T as 2, whose computa-\\ntion complexity is O(C(WH)T). The features in our bottle-\\nneck on Cityscapes is 32 × 64 × 128, and the computation is\\n4.0 GFLOPS in standard non-local operation neglecting the\\nconvolution and the complexity is O(C(WH)2). For our re-\\nduced non-local operation, the complexity is O(C(WH)S),\\nwhere S (S ≪WH) is the number of the Keys and Val-\\nues. For single-scale SVN (Model B) and pooling, we divide\\nfeature maps into 64 sub-regions and the computation is 32\\nMFLOPS. For multi-scale SVN, feature maps into 64 and 16\\nsub-regions, and the computation is 40 MFLOPS. The cost\\nof Power Iteration in single-scale and multi-scale SVN are 1\\nMFLOPS and 2 MFLOPS, respectively.\\nWe compare using regional dominant singular vectors\\n(Model B) with using pooling features in single scale (8 × 8)\\n(Table 1) to show the effectiveness of dominant singular vec-\\ntors. Results on Cityscapes validation are shown in Table 1.\\nComparison of Model single-scale SVN (Model B) and pool-\\ning (max or average) shows that regional singular vectors are\\neffective for our network with a light-weighted encoder with\\n0.5 mIoU improvement and additional 0.09 GFLOPS, while\\nusing pooling can not provide representative features for a\\nlight-weighted network. And the multi-scale SVN (Model C)\\nfurther improves the result to 71.4% mIoU with a little cost\\non inference time and computation.\\n4.4. Comparison with Other Methods\\nWe compare our LRNNet with other light-weighted methods\\non Cityscapes and Camvid test sets in terms of parameter size,\\naccuracy, speed and computation. We only report the speed of\\nmethods on the open-source deep-learning framework, such\\nas Pytorch, TensorFlow and Caffe, because they have com-\\nparable implemented performance, but have a large gap com-\\nparing with the non open-source deep-learning framework of\\nthose works [13, 12] and details are in supplemental material.\\nResults are shown in Table 3 and 4. - indicates that the speed\\nis not achieved by open-source deep learning frameworks or\\nnot provided. Our network constructed by FCB (Model A)\\nachieves 70.6% mIoU and 76.5 FPS on Cityscapes test set\\nwith only 0.67M parameters and 8.48 GFLOPS, which is\\nmore light-weighted and efﬁcient than ERFNet [8] and LED-\\nNet [11] with better accuracy. With single-scale (Model B)\\nand multi-scale SVN (Model C), LRNNet achieves 71.6%\\nand 72.2% mIoU on Cityscapes test set with a little cost on\\nspeed and efﬁciency, respectively. Our LRNNet with multi-\\nscale SVN achieves 69.2% mIoU with only 0.68M parameters\\non CamVid test set. All results show the state-of-the-art trade-\\noff among parameter size, speed, computation and accuracy\\nof our LRNNet. Visual comparison can be viewed in Fig. 1.\\nTable 4. Evaluation on Camvid test set.\\nModel\\nInput Size\\nPre-trained\\nmIoU\\nFPS\\nPara(M)\\nSegNet [9]\\n360 × 480\\nN\\n46.4\\n46\\n29.5\\nENet [10]\\n360 × 480\\nN\\n51.3\\n-\\n0.37\\nICNet [6]\\n720 × 960\\nY\\n67.1\\n27.8\\n26.5\\nCGNet [7]\\n360 × 480\\nY\\n65.6\\n-\\n0.5\\nBiSeNet [12]\\n720 × 960\\nN\\n65.6\\n-\\n5.8\\nDFANet [13]\\n720 × 960\\nY\\n64.7\\n-\\n7.8\\nSS-nbt [11]\\n360 × 480\\nN\\n66.6\\n77\\n0.95\\nModel A\\n360 × 480\\nN\\n67.6\\n83\\n0.67\\nModel B\\n360 × 480\\nN\\n68.9\\n77\\n0.68\\nModel C\\n360 × 480\\nN\\n69.2\\n76.5\\n0.68\\n5. CONCLUSION\\nWe have proposed LRNNet for real-time semantic segmenta-\\ntion. The proposed FCB unit explores a proper form of fac-\\ntorized convolution block to deal with short-range and long-\\nrange features, which provides light-weighted, efﬁcient and\\npowerful feature extraction for the encoder of our LRNNet.\\nOur SVN module utilizes regional dominant singular vectors\\nto construct the efﬁcient reduced non-local operation, which\\nenhances the decoder with a very low cost. Extensive exper-\\nimental results have validated our state-of-the-art trade-off in\\nterms of parameter size, speed, computation and accuracy.\\n6. ACKNOWLEDGEMENT\\nThis paper is supported by NSFC (No.61772330, 61533012,\\n61876109),\\nthe pre-research project (No.61403120201),\\nShanghai\\nKey\\nLaboratory\\nof\\nCrime\\nScene\\nEvidence\\n(2017XCWZK01) and the Interdisciplinary Program of\\nShanghai Jiao Tong University (YG2019QNA09).\\n7. REFERENCES\\n[1] Jonathan Long, Evan Shelhamer, and Trevor Darrell, “Fully convolutional net-\\nworks for semantic segmentation,” in CVPR, 2015.\\n[2] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, and Hanqing\\nLu, “Dual attention network for scene segmentation,” in CVPR, 2019.\\n[3] Zilong Huang, Xinggang Wang, Lichao Huang, et al., “Ccnet: Criss-cross atten-\\ntion for semantic segmentation,” in ICCV, 2019.\\n[4] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, et al., “The\\ncityscapes dataset for semantic urban scene understanding,” in CVPR, 2016.\\n[5] Adam Paszke, Sam Gross, Soumith Chintala, et al., “Automatic differentiation in\\npytorch,” 2017.\\n[6] Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, et al., “Icnet for real-time se-\\nmantic segmentation on high-resolution images,” in ECCV, 2018.\\n[7] Tianyi Wu, Sheng Tang, Rui Zhang, and Yongdong Zhang, “Cgnet: A light-weight\\ncontext guided network for semantic segmentation,” CoRR, vol. abs/1811.08201,\\n2018.\\n[8] Eduardo Romera, Jos´e M. Alvarez, Luis M. Bergasa, and Roberto Arroyo,\\n“Erfnet: Efﬁcient residual factorized convnet for real-time semantic segmenta-\\ntion,” IEEE TITS, vol. PP, no. 99, pp. 1–10, 2017.\\n[9] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla, “Segnet: A deep con-\\nvolutional encoder-decoder architecture for image segmentation,” IEEE TPAMI,\\nvol. 39, no. 12, pp. 2481–2495, 2017.\\n[10] Adam Paszke, Abhishek Chaurasia, Sangpil Kim, and Eugenio Culurciello, “Enet:\\nA deep neural network architecture for real-time semantic segmentation,” CoRR,\\nvol. abs/1606.02147, 2016.\\n[11] Yu Wang, Quan Zhou, Jia Liu, et al., “Lednet: A lightweight encoder-decoder\\nnetwork for real-time semantic segmentation,” arXiv preprint arXiv:1905.02423,\\n2019.\\n[12] Changqian Yu, Jingbo Wang, Chao Peng, et al., “Bisenet: Bilateral segmentation\\nnetwork for real-time semantic segmentation,” in ECCV, 2018.\\n[13] Hanchao Li, Pengfei Xiong, Haoqiang Fan, and Jian Sun, “Dfanet: Deep feature\\naggregation for real-time semantic segmentation,” in CVPR, 2019.\\n[14] Xiaolong Wang, Ross Girshick, Abhinav Gupta, et al., “Non-local neural net-\\nworks,” in CVPR, 2018.\\n[15] Zhen Zhu, Mengde Xu, Song Bai, Tengteng Huang, and Xiang Bai, “Asymmetric\\nnon-local neural networks for semantic segmentation,” in ICCV, 2019.\\n[16] Gabriel J. Brostow, Julien Fauqueur, and Roberto Cipolla,\\n“Semantic object\\nclasses in video: A high-deﬁnition ground truth database,” Pattern Recognition\\nLetters, vol. 30, no. 2, pp. 88–97, 2009.\\n[17] Franc¸ois Chollet, “Xception: Deep learning with depthwise separable convolu-\\ntions,” in CVPR, 2017.\\n[18] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun\\nWang, Tobias Weyand, Marco Andreetto, and Hartwig Adam, “Mobilenets: Ef-\\nﬁcient convolutional neural networks for mobile vision applications,” CoRR, vol.\\nabs/1704.04861, 2017.\\n[19] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun, “Shufﬂenet: An ex-\\ntremely efﬁcient convolutional neural network for mobile devices,”\\nin CVPR,\\n2018.\\n[20] Lang Huang, Yuhui Yuan, Jianyuan Guo, et al., “Interlaced sparse self-attention\\nfor semantic segmentation,” CoRR, vol. abs/1907.12273, 2019.\\n[21] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam,\\n“Rethinking atrous convolution for semantic image segmentation,” CoRR, vol.\\nabs/1706.05587, 2017.\\n[22] Tobias Pohlen, Alexander Hermans, Markus Mathias, et al.,\\n“Full-resolution\\nresidual networks for semantic segmentation in street scenes,” in CVPR, 2017.\\n'), ResearchPaper(title='Realtime Global Attention Network for Semantic Segmentation', authors=[arxiv.Result.Author('Xi Mo'), arxiv.Result.Author('Xiangyu Chen')], abstract='In this paper, we proposed an end-to-end realtime global attention neural\\nnetwork (RGANet) for the challenging task of semantic segmentation. Different\\nfrom the encoding strategy deployed by self-attention paradigms, the proposed\\nglobal attention module encodes global attention via depth-wise convolution and\\naffine transformations. The integration of these global attention modules into\\na hierarchy architecture maintains high inferential performance. In addition,\\nan improved evaluation metric, namely MGRID, is proposed to alleviate the\\nnegative effect of non-convex, widely scattered ground-truth areas. Results\\nfrom extensive experiments on state-of-the-art architectures for semantic\\nsegmentation manifest the leading performance of proposed approaches for\\nrobotic monocular visual perception.', url='http://arxiv.org/abs/2112.12939v1', pdf_path='./papers/2112.12939v1.Realtime_Global_Attention_Network_for_Semantic_Segmentation.pdf', content='Realtime Global Attention Network for Semantic Segmentation\\nXi Mo1,∗, Xiangyu Chen1\\nAbstract— In this paper, we proposed an end-to-end realtime\\nglobal attention neural network (RGANet) for the challenging\\ntask of semantic segmentation. Different from the encoding\\nstrategy deployed by self-attention paradigms, the proposed\\nglobal attention module encodes global attention via depth-wise\\nconvolution and afﬁne transformations. The integration of these\\nglobal attention modules into a hierarchy architecture main-\\ntains high inferential performance. In addition, an improved\\nevaluation metric, namely MGRID, is proposed to alleviate the\\nnegative effect of non-convex, widely scattered ground-truth\\nareas. Results from extensive experiments on state-of-the-art\\narchitectures for semantic segmentation manifest the leading\\nperformance of proposed approaches for robotic monocular\\nvisual perception.\\nIndex Terms— semantic segmentation, global attention, visual\\nperception, hierarchy inference, neural network.\\nI. INTRODUCTION\\nCorrect bin picking by suction from cluttered environment\\nis nontrivial for a robotic hand [1]. Since robotic hands don’t\\nhave much prior knowledge of spatial shape by category,\\ntexture of material or normal vectors of surfaces, it remains\\nto be an open topic whether a robotic hand is capable of rec-\\nognizing and analyzing objectiveness as human being does.\\nThis topic becomes more intricate given the constraint that,\\nthe very limited information offered is the RGB images of\\ncluttered scene, and plausible areas for suction annotated by\\nvarious human experts. Individuals may make mistakes while\\nannotating images in their unique styles, which diversiﬁes\\nannotations greatly, making it more difﬁcult for traditional\\nmachine vision techniques to forge ahead with.\\nIn terms of neural network based semantic segmentation,\\nrealtime visual perception with light computational liability\\nis preferred, especially for robotic bin picking by suction.\\nZeng et al [1] names semantic segmentations of adsorbability\\nas affordance maps to indicate the possibilities of objects\\nbeing picked up. It is widely known that, the predicted\\naffordance maps are not yet applicable enough for actual bin\\npicking - post-reﬁnement of estimating normal vectors of 3D\\nsurfaces, registering affordance maps to multiple coordinates\\nsystems, calibrations on robotic hand and cameras, and\\ndata stream synchronization etc. are necessary though time-\\nconsuming. In this case, predict more reliable affordance\\nmaps in real-time with low-cost inferential processors will\\ngreatly beneﬁt the real world implementations.\\nWe propose to efﬁciently predict affordance maps by\\nrealtime global attention network (RGANet), which can be\\neasily adapted to other semantic segmentation tasks. As\\ndepicted in Fig. 1, RGANet is a light-weighted hierarchy\\n*Corresponding Author, email: x618m566@ku.edu\\n1School of Engineering, University of Kansas, Lawrence KS 66049, USA\\nInput image\\nIB-1\\nIB-5\\nIB-3\\nIB-4\\nIB-2\\nEncoder\\nDecoder\\n2× upsample\\nDecision Unit\\n2× downsample\\noutput\\nHead\\nGAM\\nConcat & Vote\\nfeature volume\\nfeature volume\\nBackbone Networks\\nFig. 1.\\nHierarchy inference architecture of 5-scales RGANet5. Backbone\\nnetwork consists of encoder and decoder networks. One IB processes feature\\nvolume at a scale, decoder network interacts with encoder at all scales,\\ngenerates high-order feature volumes. The decision unit has a Head to group\\nboth high-order features and raw image channels, and a GAM to acquire\\nthe ﬁnal segmentations (affordance maps).\\narchitecture composed of inference blocks (IBs) that are\\nbased on attention mechanism [2], [3]. For the purpose\\nof preserving full-size activation of feature maps, pooling\\nand dropout layers are deprecated. To explore the potentials\\nof proposed global attention modules (GAMs), we adopt a\\nGAM-enhanced encoder-decoder backbone network.\\nOne speciﬁc forward pass of the proposed framework is\\nillustrated in Fig. 2. RGANet5 has 5 levels of inference\\nblocks (IBs), each one applies to a 2× down-sampling of\\nits previous feature map to capture features at each scale.\\nThe decoder network is composed of vote & upsample (VU)\\nblocks and excite-squeeze-stack (ESS) bottleneck blocks for\\nbetter decoding capabilities. Due to the enhancement of\\nGAM at each IB, we only utilize 3×3 standard convolution\\nw/o pooling layers to downsample features, and 1 × 1\\nconvolution for reformulating feature depth. We also tested\\nlarge kernels for down-sampling, which doesn’t outperform\\n3×3 convolutions.\\nVanilla convolution convolves and aggregates feature maps\\nfaster than fully-connected layers due to shared weights.\\nDepth-wise convolution [4] further reduces parameters by\\nconvolving groups of feature maps separately then concate-\\nnates. We propose to reformulate self-attention mechanism\\nby implementing long convolutional kernels (please refer\\nto section III-B) and depth-wise convolutions instead of\\ncomputing cosine similarities of feature vectors [3] or pure\\nmatrix multiplication [2] upon feature volumes. Our percep-\\ntion is that single convolutional kernel only needs to encode\\nneighboring features other than entire feature volume, which\\ncan be mutually correlated by afﬁne-transforming into global\\narXiv:2112.12939v1  [cs.CV]  24 Dec 2021\\nC\\nESS\\nConv, 2× down\\nInput\\nGlobal Att\\nA\\nConv, 1×1\\nIB-1\\nIB-2IB-3 IB-4 IB-5\\n2×Up\\nC\\nfc\\nVU1\\nC\\nC\\nOutput\\nS\\nS\\nC - Feature Concatenation\\n- Softmax\\nA - Element-wise Addition\\nESS-3\\n2×Up\\nVU2\\nESS-3\\nVU3\\nVU4\\nIB - inference block\\nESS - excite-squeeze-stack block\\nVU - vote & upsample block\\nDecision Unit\\nHead Global\\nAtt\\nC\\nFig. 2.\\nForward pass of RGANet5. It should be noted that detailed framework of ESS, GAM and decision unit are presented in Section III, thereby\\ncorresponding blocks (in deeper blue color) do not represent feature maps. The network takes monocular input image, outputs pixel-wise, one-hot encoded\\npredictions of background (blue), negative sample (green), suction areas (red) respectively. The dashed line indicates the highway between encoder and\\ndecoder can be disconnected.\\nencoding. This is beneﬁcial for reducing the computational\\ncomplexity, and speeding up inference to meet realtime\\nrequirements.\\nFurthermore, during evaluation phase, due to unconnected\\nground-truth areas that are distributed across the entire\\nimage per the trade-off circumstance illustrated in Fig. 4(b),\\ngenerally applied segmentation metrics fail to correctly eval-\\nuate predictions. Mean-Grid Fbeta-score (MGRID), a novel\\nmetric for segmentation evaluation, alleviates the ﬂaw by\\ntwo-stage operations: partition and synthetics.\\nTo summarize our contributions, this paper highlights the\\nfollowing novelties:\\ni. Propose a one-stage hierarchy inference architecture for\\nsemantic segmentation without any auxiliary losses.\\nii. Propose the GAM for realtime inference - 54fps on a GTX\\n1070 laptop, and 134fps with a V100 GPU.\\niii. Propose the metric MGRID for evaluating widely-\\ndistributed predictions.\\nII. RELATED WORKS\\nA. Predict Affordance Maps\\nZeng et al [1] implemented two FCNs [5] with ResNet-\\n101 [6] backbones to fuse color and depth streams of\\ncluttered scene. This approach yields 83.4% precision at\\nTop-1% percentile affordance proposals. Azpiri et at [7]\\nfurther reﬁned the Top-1% percentile precision to ∼94%\\nby a deep Graph-Convolutional Network backbone, which\\noutperforms FCN (ResNet-101 backbone) by ∼2%. Neither\\nof two approaches takes colored images as the only input,\\nconsuming much inference time in processing depth infor-\\nmation. Shao et at [8] proposed to predict affordance map\\nby sharing a ResNet-50 backbone between colored image\\nstream and depth image stream, and a U-Net [9] to fuse\\nfeatures at different scales. Their approach is annotation-\\nfree, but requires the robotic hand attempting to ﬁnd the\\nmost possible points for suction, which is more expensive\\nthan deriving direct predictions from monocular images.\\nB. Self-Attention Modules in Semantic Segmentation\\nSelf-Attention mechanism was ﬁrst proposed in the ﬁeld\\nof nature language processing [3] for temporal domain.\\nWang et al [10] adapted the idea to spatial domain, that\\na feature vector is spatially related to all other features in\\nthe feature volume, the features that are highly related will\\ngenerate strong response, which facilitates network to model\\nthe non-local relations across the entire volume.\\nLi et al [11] designed a fully-convolutional feature pyra-\\nmid attention module to replace the spatial-pyramid-pooling\\nmodule [12], and a global-attention-upsample module with\\nwhich high-level features perform global average pool-\\ning [13] as the guidance for low-level features. Woo et al [14]\\nbelieved that simple channels-wise attention and spatial-wise\\nattention sub-modules can boost representation power of\\nCNNs. Recently, OCNet [15] efﬁciently aligns a global rela-\\ntion module and a local relation module, dividing and merg-\\ning feature volumes in different styles. Bello et al [16] pro-\\nposed to augment standard convolution by attention mech-\\nanism. Cao et at [17] proposed a global-context block that\\nbeneﬁts from simpliﬁed non-local block [10] and squeeze-\\nexcitation module [18]. The proposed GAM in this paper\\nis inspired by the criss-cross attention (CCA) mechanism of\\nCCNet [2]. CCNet approximates non-local attention by two\\ncascade CCA modules, each of which correlates all feature\\nvectors aligned horizontally and vertically.\\nIII. METHODOLOGY\\nA. Hierarchy Inference Architecture\\nRGANet5 is designed as a single ‘distillation tower’ (see\\nFig. 2) with 5 temperature levels, each level is composed of\\none IB. The architecture is expandable such that RGANetn\\nhas n cascade IBs. Each IB halves input feature size, then\\nmodulates channels to ratio k = 15 by a standard 3 × 3\\nconvolution. ESS block (see Fig. 3(a)) is regarded as the\\nbackbone for IB, we adopt ESS-3, ESS-6, ESS-12, ESS-24\\nfor IB-1 and IB-2, IB-3, IB-4, IB-5 respectively. Output of\\nGAM (refer to Fig. 3(b)) is directly added to the output\\nC - Concatenation\\nBottleNeck (BNK)-1\\ncin\\ns×k\\nk\\n1\\n1\\n3\\n3\\nC\\nC\\nBNK-2\\nBNK-3\\nC\\n3×k+cin\\n(a) Illustration of ESS-3 framework (3 BottleNecks).\\nc\\nh\\nw\\nR\\nw\\nh\\nc\\nh\\nw\\nc\\nR\\nc×1\\n1×h\\nM\\nM\\nw\\nw×1\\nh\\nh\\nw\\nw\\nc\\nh\\n1\\n1\\n1\\n1\\nc\\nh\\nw\\nR\\n×\\nC\\nA\\nM\\nR\\nC\\nA×\\n - Matrix Multiplication\\n - Rotation\\n - Concatenation\\n - Aggregation\\n - Element-wise\\n   Multiplication\\n1×c\\nc\\nR\\n(b) The structure of GAM.\\nFig. 3.\\nESS-3 and realtime GAM. (a) ESS-n block ﬁrst expands input channel Cin channels to s × k channels by 1×1 convolution, scalers s and k\\ncontrol the number of expanded channels; then it squeezes intermediate features to k channels by 3×3 convolution, densely stacking all k-channel features\\ngradually towards ﬁnal n × k + Cin channels. Specially, we set Cin = k to enforce the entire network scalable w.r.t. ratio k. (b) Query and key of GAm\\nare derived from afﬁne transforms, either query or key is channel-wise (1 × c and c × 1 depth-wise convolutions) and spatial-wise (w × 1 and 1 × h\\ndepth-wise convolutions), globally related by matrix multiplication and aggregation layers.\\nfeature volume x of ESS block as the residual λ · x, which\\nformulates the incremental up-sampling layers accordingly.\\nThe ﬁnal output takes the form (1 + λ) · x, where λ is the\\nlearnable weights volume that has the same size of x, and\\noperator ‘·’ signiﬁes element-wise product. Therefore, non-\\nnegative activation of GAM artifact is preferred, and Batch-\\nNorm (BN) layers are necessary to restrict its upper bound.\\nAs the synthesis of 5-scales distillation artifacts, highway\\nconnections not only populate previous inferences to up-\\nsampling layers accordingly, they also facilitate gradient\\nback-propagation during training phase, especially for a\\nvery deep network. The ‘vote & upsample’ (VU) block\\n(see Fig. 2) weights concatenated input feature maps by\\n1 × 1 convolution without bias, these weighted features are\\nthen 2× upsampled via nearest interpolation. If inferring\\nwithout last two ESS-3 blocks, RGANet will not yield ﬁne-\\ngrained predications. Also, we noticed that IB-1 is directly\\nlinked to the last UV4 block, which performs poorly without\\nDecision Unit (DU) shown in Fig. 2. DU consists of feature\\nmodulating head to deduct channels from k+3 to the number\\nof classes by 1×1 convolution, and one last GAM that yields\\npredictions.\\nB. Realtime Global Attention\\nFig. 3(b) illustrates the pipeline of the GAM. With a\\nbatch-size of 1, input feature volume x ﬁltered by previous\\nESS block has a shape of h × w × c, which is rotated to\\nc × h × w query Q(c, h, w) (upper branch, blue color) and\\nw×c×h key K(w, c, h) (lower branch, purple color). Query\\nconducts channel-wise attention via depth-wise sliding w\\nkernels W(c, 1) shaped c×1 across c−h view, which results\\nin a c × 1 × w feature volume Qc. Similarly, h horizontal\\npositions are encoded by 1 × h depth-wise convolutional\\nkernel W(1, h), mapping into the c × 1 × w horizontally\\npositional encoding Qh. Key is transformed accordingly\\nexcept the sizes of kernels, we denote its artifacts as channel-\\nwise encoding Kc and vertically positional encoding Kv.\\nThe ﬁnal output Fout are then formulated as:\\nFout = λ · x = f{rot[QhQc]\\n[\\nrot[KvKc]} · x,\\n(1)\\nwhere rot[·] operator signiﬁes rotation; f{·} operator per-\\nforms 1 × 1 convolution to resize 2c-channels to c-channels\\nwith Swish activation function [19], then maps to [0,\\n1] weights volume via Sigmoid or Softmax functions;\\n‘lhs S rhs’ operator concatenate lhs and rhs features; Let\\n‘∗’ denotes the depth-wise convolution, we know:\\nQh = Q(c, h, w) ∗W(1, h),Qc = Q(c, h, w) ∗W(c, 1),\\nKv = Q(w, c, h) ∗W(w, 1),Kc = Q(w, c, h) ∗W(1, c).\\n(2)\\nConsider Eqn. (1) and Eqn. (2), RGA’s total number of\\ntrainable parameters is 2hw + cw + hc (exclude BN layer,\\nbias and 1 × 1 point-wise convolution), while for vanilla\\nconvolution with the same kernel height and width, this\\nnumber becomes hw2 + wh2 + cw2 + ch2. In terms of\\nglobal attention, although matrix multiplication operation\\nonly correlates features horizontally and vertically aligned\\nfor any feature vector, Qh, Qc, Kv and Kc themselves are\\nthe artifacts of global depth-wise convolutions, which, as a\\nresult, each element in these 4 Queries and Keys bounds\\nother elements with shared weights. We can also treat RGA\\nas the type of ‘learnable global attention’. Furthermore, all\\nafﬁne operations of RGA module are differentiable, we didn’t\\nobserve vanishing, or exploding gradients issues during train-\\ning.\\nC. Rethink Densely-Stacked Bottlenecks\\nA Dense block [20] has multiple densely-connected Bot-\\ntlenecks (BNK) modules, each BNK acts in a stack-squeeze\\nmanner, and the last one outputs a k-channels feature volume.\\nThe ESS-n block, as illustrated in Fig. 3(a), outputs a\\nstacked (n + 1)k-channels feature volume by n densely-\\nconnected BNKs, each contributes k-channels of features,\\nand we let Cin = k. Each channel of stacked feature maps\\nshows one degree of ﬁltered raw input features, such that\\nRGA modules are capable of correlating high-order features\\nto low-level features. RGANet5 adopts ESS-3 for IB-1, ESS-\\n3 for IB-2, ESS-6 for IB-3, ESS-12 for IB-4, and ESS-24 for\\nIB-5. One of our future works is to let RGA module ignore\\n100(%)\\n100(%)\\n(Fm, Cm)\\n5\\n0\\nCurved\\nFbeta-score\\nS  = 3.8\\nFm = 0.5\\nCm = 0.525\\n(a)\\n TP \\n FN \\n TN \\n same TP and FN\\nlocate single object\\nfind both objects\\nw/ MGRID\\n TP↓ FN↑\\n TP↓ FN↑\\n same TP and FN\\n TP↑ FN↓\\n 4 cells’ average↑\\nCell-1\\nCell-2\\nCell-3\\nCell-4\\n(b)\\nFig. 4.\\nMGRID metric. (a) Synthetic function. (b) A special case for predicting suction areas when the same precision and recall indicate better predictions\\nfor real-world implementation, and how MGRID mitigates the circumstance by partition and synthetics. True Negative (TN) and False Positive (FP) samples\\nstay unchanged during the processes.\\nnoisy low-level features, and locate more reliable high-order\\nfeatures.\\nD. MGRID metric for Evaluation\\nExisting evaluation metrics poorly treat the special case\\nof a prediction map as illustrated in Fig. 4(b) that, when\\noriginal predictions cover object-1 (red), but fail to locate\\nobject-2 (green) at the left-bottom corner of image, off-\\nthe-shelf metrics yields the same results if part of object-1\\nrelocates to the object-2, because this relocation does not\\naffect paranormal statistics of TP, FN, TN and FP. In reality,\\nwe want predications to be able to cover more objects,\\nsuch that a robotic hand would seek-and-pick all objects\\neven though Precision or Recall is yet not favorable enough\\n(e.g., ∼15% Recall on object-2 alone). It would be more\\nreasonable to assign higher score for the prediction map that\\ncovers 2 objects.\\na) Partition.: 2-Stages MGRID metric aims to remedy\\nthe issue mentioned above. As shown in the third image of\\nFig. 4(b), during partition stage, predication map is manually\\ndivided into four cells, each cell is treated fairly against\\nany other cell. An ideal partition will separates objects by\\ndifferent cells. Next, only calculate Fbeta-scores (or any other\\nexisting metrics) for all cells that contain predictions and\\ncategorical ground-truth (non-zero TP, FP or FN samples),\\nusing the deﬁnition\\nFβ =\\n(1 + β2)TP\\nβ2(TP + FN) + TP + FP + ϵ,\\n(3)\\nwhere ϵ = 1 × 10−31, β > 1 weights Recall more than\\nPrecision, and 0 < β < 1 weights the opposite.\\nb) Synthetics.: The second step is to synthesize all n\\nFbeta-scores F = {Fβ(i)|i = 1, 2, ..., n} collected from all\\npartitions, any Fbeta-score F ∈F is curved by a regulator\\nas shown in Fig. 4(a), which takes the form\\nΓ(F) =\\n\\x1a S(F −Fm)3 + Cm,\\n0 < F ≤100%\\n0,\\nF = 0\\n,\\n(4)\\nwhere coefﬁcients 0 < Fm, Cm < 1 requires to be manually\\nset. Fig. 4(a) shows the curve by setting (Fm, Cm) =\\n(0.5, 0.525), then S is calculated as S = (1−Cm)/(1−Fm)3.\\nLet T = [Fm/(1 −Fm)]3, then intercept B at F = 0 can be\\ndenoted as Cm(1 + T) −T. To make the regulator effective,\\nB should fall within the interval (0, Fm), which leads to\\nvalid ranges of Fm and Cm:\\nT\\n1 + T < Cm < Fm + T\\n1 + T , 0 < Fm < 1.\\n(5)\\nThe ﬁnal conﬁdence score is derived by\\nMGRID = 1\\nn\\nX\\nF ∈F\\nΓ(F).\\n(6)\\nIV. EXPERIMENTS\\nA. Conﬁguration\\nPublic-available suction dataset [21] consists of camera\\nintrinsics and pose records, RGB-D images of clutter scenes\\nand their backgrounds, and labels. We only adopted color\\nimages and labels w/ a train-split of 1470 images and a\\ntest-split of 367 images, each image has a resolution of\\n480×640. All colored images were normalized to tensors\\nvalued between 0 and 1, which were not resized or padded\\nduring training and testing.\\nB. Train and Test\\nWe implemented AdamW optimizer [22] with AMS-\\nGrad [23], compared two weighted loss function during\\ntraining - focal loss (FLoss) [24] and cross-entropy loss\\n(CELoss). Assume y denotes the prediction, y′ the one-hot\\nencoded ground-truth, n the total number of classes, γ and\\nα are constants, then FLoss and CEloss are denoted as:\\nFLoss(y, y′) = −\\nn\\nX\\nc=1\\nαc\\nX\\nc\\n(1 −y)γ log y,\\nCELoss(y, y′) = −\\nn\\nX\\nc=1\\nαc\\nX\\nc\\nlog y\\n(7)\\nwhere Pn\\nc=1 αc = 1 and αc ≥0, γ > 0, and all y ∈[0, 1].\\nTraining-set is augmented during training by random hue,\\nﬂip, rotation, blur, shift etc.\\nOnline testing comes simultaneously during training,\\nwhich shows calculations of Jaccard Index, precision, and\\nrecall of running batches. The Ofﬂine testing loads check-\\npoint, merely evaluates the class corresponds to predicted\\nsuction areas.\\n百分位数 \\n90 \\n85.4960 \\n95 \\n90.1340 \\n99 \\n98.5852 \\n95.38\\n30.09\\n43.87\\n48.53\\n42.16\\n41.83\\n28.58\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\n90\\n100\\nAccuracy\\nJaccard\\nPrecision\\nRecall\\nDice\\nMCC\\nMGRID\\nRGANet5-NB\\nRGANet5-B1\\nRGANet5-B2\\nRGANet5-B3\\nRGANet5-B4\\n \\nFig. 5.\\nAverages and its estimated standard deviations over test-set w/o DU. All values are presented in ‘%’. RGANet5-Bn signiﬁes last n highways are\\nblocked. RGANet5-B4 (purple) shows the best performance among all tested architectures.\\nAll experiments were conducted using a GTX1070 laptop,\\nand one Tesla V100 GPU. Network scaler k was set to 15\\nconstantly for a better trade-off between module size and\\nperformance. We set constant learning rate to 1.5 × 10−4,\\nweights decay rate to 0; γ = 1.3, α1 = 0.25 and α2 = 0.25\\nupon background and negative samples, α3 = 0.5 upon\\nsuction areas because of unbalanced proportions; default\\nMGRID parameters β = 0.5, grid intervals (δH, δW ) =\\n(12px, 12px), Cm and Fm took the same values as illustrated\\nin Fig. 4(a). The training of RGANet5 lasted for ∼2 days.\\nWe implement multiple metrics to evaluate inferential\\nartifacts. Note that for the suction dataset, users only care\\nabout feasible regions to adhere. Therefore, only the category\\nthat represents predicted suction areas is evaluated, the ﬁnal\\nscores are computed via averages over entire test-set.\\nC. Ablation Study\\na) Block highways between IBs and remove DU: To\\nevaluate the performance of RGANet5 w/o DU, as well\\nas the effects of disconnecting different highways between\\nIBs, we tested 5 circumstances when different highways\\nto IBs are blocked. The results were acquired via CELoss\\nbecause of its better performance than implementing FLoss.\\nWe ﬁnd that, FLoss enhances weights for negative samples\\nto weaken salient positive responses, in the meantime, it\\nalso ‘confuses’ inferential and voting blocks of RGANet by\\nunreasonable probabilistic distributions of classes, especially\\nwhen negative samples dominate. Additionally, we applied\\nnearest interpolation instead of deconvolutional layers to VU.\\nFig. 5 illustrates the testing performance of RGANet5-NB,\\nRGANet5-B1, RGANet5-B2, RGANet5-B3, and RGANet5-\\nB4 by comparing mean and observing standard deviation of\\naccuracy, Jaccard Index, precision, recall, Dice (F1-score),\\nMCC (Phi Index) and MGRID. It is obvious that IB-1\\nprovides the most crucial lower level features, IB-2, IB-3,\\nIB-4 and IB-5 reﬁne those features distinctively - IB-4 and\\nIB-5 provide more useful semantic information than IB-2\\nand IB-3. When all highways are disconnected, RGANet5-\\nB4 outputs less precise proposals than RGANet5-B3 - the\\nbest performer during the test.\\nIn conclusion, blocking highways has insigniﬁcant inﬂu-\\nence on the performance over test-set.\\nMethod\\nPrec.\\nRecall\\nJacc.\\nDice\\nMGRID\\nRGANet-B3(w/o)\\n43.87\\n48.53\\n30.09\\n42.16\\n28.58\\nRGANet-B3(w/)\\n50.52\\n58.92\\n37.84\\n50.44\\n34.20\\nRGANet-NB(w/o)\\n43.45\\n46.45\\n28.88\\n40.65\\n27.65\\nRGANet-NB(w/)\\n47.16\\n63.36\\n37.92\\n50.60\\n34.58\\nTABLE I\\nAblation study on DU. All averaged evaluation metrics are presented in ‘%’.\\nGAM in DU tends to make better trade-off between precision and recall.\\nb) Without DU vs With DU: According to Tab. I, GAM\\nboosts the overall performance of the backbone networks by\\nlarge margins, indicating the powerful encoding capabilities\\nof global attention mechanism. Furthermore, GAM shows\\na favor of correlating more feature maps, which leads to\\nthe superior performance of RGANet-NB. As for RGANet-\\nB3(w/ DU), we also substituted the nearest interpolation\\nlayer in VU by a 2 × 2 deconvolutional layer to evaluate\\nits up-sampling performance.\\nD. Compare to State-of-the-Arts\\nWe conducted experiments to compare RGANet5 with\\nseveral novel semantic segmentation approaches. These ap-\\nproaches can be divided into two groups - one group that\\nhas much more parameters/FLOPs that substantially can\\noutperform RGANet, and the other one that has comparable\\nmodel sizes. As shown in Tab. II, Deeplabv3 [25] and\\nFCN [5] do not rely on attention mechanism, while CC-\\nNet [2] has merely two cascade CCA modules that bring in\\ntremendous amount of trainable parameters and operations.\\nLight-weighted RGANet with 6 GAMs, on the other hand,\\nachieves competitive performance (4-7% less) against the\\nbest performer on the test-set. Also, RGANet5 outperforms\\nBiSeNetv1 [26], another attention-based realtime approach,\\nby ∼6× less parameters and FLOPs.\\nAs illustrated in Tab. III, proposed approach achieves the\\nbest Jaccard Index, Dice coefﬁcient and MGRID score when\\ncompared with top-tier realtime approaches selected from\\nCityscape Leader board [33]. Although behaves better in\\nmetrics evaluation, RGANet runs relatively slow due to the\\nfact that PyTorch is well-optimized for convolutional neural\\nInput\\nCCNet\\nDDRNet\\nShelfNet\\nRGANet-NB\\nGround Truth\\nFig. 6.\\nQualitative results on test-set. Red regions indicate the highest affordance for adhesion, which corresponds to white regions in ground-truth.\\nGroup 1 - Large Models\\nBackbone\\nParameters\\nInference Time\\nFLOPs\\nJaccard\\nDice\\nMGRID\\nRGANet5-B3 (Ours)\\n-\\n3.67M\\n7.45ms / 134fps\\n1.83B\\n37.84\\n50.44\\n34.20\\nRGANet5-NB (Ours)\\n-\\n3.41M\\n7.51ms / 133fps\\n1.57B\\n37.92\\n50.60\\n34.58\\nCCNet (ResNet101) [2]\\nResNet101\\n71.27M\\n51.03ms / 19fps\\n73.03B\\n43.83\\n57.00\\n38.90\\nFCN (ResNet50) [5]\\nResNet50\\n32.96M\\n22.85ms / 43fps\\n32.48B\\n42.15\\n55.32\\n37.91\\nFCN (ResNet101) [5]\\nResNet101\\n51.95M\\n39.41ms / 25fps\\n50.72B\\n42.28\\n55.51\\n37.69\\nDeepLabv3 (ResNet50) [25]\\nResNet50\\n39.64M\\n32.71ms / 30fps\\n38.40B\\n43.17\\n56.32\\n38.42\\nDeepLabv3 (ResNet101) [25]\\nResNet101\\n58.63M\\n49.17ms / 20fps\\n56.63B\\n41.98\\n55.05\\n37.55\\nBiSeNetv1 [26]\\nResNet18\\n23.08M\\n9.65ms / 103fp/s\\n9.53B\\n37.64\\n50.38\\n34.18\\nTABLE II\\nComparison with large semantic segmentation models using a Tesla V100 GPU. All averaged evaluation metrics are presented in ‘%’. Proposed approaches\\nachieve competitive performance with the least total parameters, indicating a better trade-off between model size and performance.\\nGroup 2 - Light-Weighted Models\\nBackbone\\nParameters\\nInference Time\\nFLOPs\\nJaccard\\nDice\\nMGRID\\nRGANet5-B3 (Ours)\\n-\\n3.67M\\n7.45ms / 134fps\\n1.83B\\n37.84\\n50.44\\n34.20\\nRGANet5-NB (Ours)\\n-\\n3.41M\\n7.51ms / 133fps\\n1.57B\\n37.92\\n50.60\\n34.58\\nDeepLabv3 [27]\\nMobileNetv2\\n4.12M\\n2.92ms / 342fps\\n1.16B\\n34.61\\n47.33\\n31.76\\nDDRNet-23-slim [28]\\n-\\n5.69M\\n1.23ms / 813fps\\n1.07B\\n32.30\\n43.95\\n32.30\\nHRNet-small-v1 [29]\\n-\\n1.54M\\n1.84ms / 543fps\\n0.97B\\n34.31\\n46.28\\n31.97\\nHarDNet [30]\\n-\\n4.12M\\n1.63ms / 613fps\\n1.03B\\n35.15\\n47.13\\n32.61\\nShelfNet [31]\\nResNet18\\n14.57M\\n2.06ms / 485fps\\n2.91B\\n36.17\\n48.61\\n33.32\\nSTDCv1 [32]\\n-\\n14.23M\\n2.45ms / 408fps\\n5.48B\\n36.10\\n48.34\\n33.19\\nTABLE III\\nComparison with light-weighted semantic segmentation models using a Tesla V100 GPU. All averaged evaluation metrics are presented in ‘%’.\\nnetworks. It is one of our future works to further optimize\\nRGANet for faster and more accurate inference. Readers may\\nrefer to Fig. 6 for our qualitative evaluation.\\nV. CONCLUSION AND FUTURE WORKS\\nFirstly, we introduced a novel light-weighted, hierar-\\nchy inference network embedded with realtime global\\nattention modules. Densely-connected excite-squeeze-stack\\nblocks generate feature volume as the input to realtime global\\nmodules, and the attention module correlates features via\\nlearnable weights and afﬁne transformations. Ablation study,\\nas well as the comparison with the state-of-the-art approaches\\nmanifests the competitive performance of the proposed\\nRGANet5. Secondly, we designed the MGRID metric, which\\neffectively leverages on the weights of predictive regions via\\npartition and synthesis stages. Our future works include but\\nnot limit to, enhance the encoding capability of inferential\\nblocks by efﬁcient backbone networks and optimizations.\\nACKNOWLEDGMENT\\nWe would like to thank Li, Rui for the sponsorship of\\nNVIDIA GPUs and INTEL CPUs. We appreciate authors\\nof references [2], [5], [25], [26], [28], [30], [31], [32] for\\nproviding open-source codes of their great works.\\nREFERENCES\\n[1] A. Zeng, S. Song, K.-T. Yu, et al., “Robotic pick-and-place of novel\\nobjects in clutter with multi-affordance grasping and cross-domain\\nimage matching,” in Proceedings of the International Conference on\\nRobotics and Automation (ICRA).\\nIEEE, 2018.\\n[2] Z. Huang, X. Wang, L. Huang, et al., “Ccnet: Criss-cross attention for\\nsemantic segmentation,” in Proceedings of the International Confer-\\nence on Computer Vision (ICCV).\\nIEEE/CVF, 2019, pp. 603–612.\\n[3] A. Vaswani, N. Shazeer, N. Parmar, et al., “Attention is all you need,”\\nin Proceedings of the Conference and Workshop on Neural Information\\nProcessing Systems (NIPS), 2017.\\n[4] F. Chollet, “Xception: Deep learning with depthwise separable con-\\nvolutions,” in Proceedings of the Conference on Computer Vision and\\nPattern Recognition (CVPR).\\nIEEE, 2017, pp. 1251–1258.\\n[5] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks\\nfor semantic segmentation,” in Proceedings of the Conference on\\nComputer Vision and Pattern Recognition (CVPR). IEEE/CVF, 2015,\\npp. 3431–3440.\\n[6] C. Szegedy, S. Ioffe, V. Vanhoucke, et al., “Inception-v4, inception-\\nresnet and the impact of residual connections on learning,” Proceed-\\nings of the AAAI Conference on Artiﬁcial Intelligence, vol. 31, no. 1,\\n2017.\\n[7] A. I. Azpiri, E. L. Ortega, and A. A. Cobo, “Affordance-based grasping\\npoint detection using graph convolutional networks for industrial bin-\\npicking applications,” Sensors, vol. 21, no. 3, p. 816, 2021.\\n[8] Q. Shao, J. Hu, W. Wang, et al., “Suction grasp region prediction\\nusing self-supervised learning for object picking in dense clutter,” in\\nProceedings of the International Conference on Mechatronics System\\nand Robots (ICMSR).\\nIEEE, 2019, pp. 7–12.\\n[9] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional\\nnetworks for biomedical image segmentation,” in International Con-\\nference on Medical Image Computing and Computer-Assisted Inter-\\nvention.\\nSpringer, 2015, pp. 234–241.\\n[10] X. Wang, R. Girshick, A. Gupta, et al., “Non-local neural networks,”\\nin Proceedings of the Conference on Computer Vision and Pattern\\nRecognition (CVPR).\\nIEEE/CVF, 2018, pp. 7794–7803.\\n[11] H. Li, P. Xiong, J. An, and L. Wang, “Pyramid attention network for\\nsemantic segmentation,” British Machine Vision Conference (BMVC),\\n2018.\\n[12] H. Zhao, J. Shi, X. Qi, et al., “Pyramid scene parsing network,”\\nin Proceedings of the Conference on Computer Vision and Pattern\\nRecognition (CVPR).\\nIEEE/CVF, 2017, pp. 2881–2890.\\n[13] B. Zhou, A. Khosla, A. Lapedriza, et al., “Learning deep features\\nfor discriminative localization,” in Proceedings of the Conference on\\nComputer Vision and Pattern Recognition (CVPR). IEEE/CVF, 2016,\\npp. 2921–2929.\\n[14] S. Woo, J. Park, J.-Y. Lee, and I. S. Kweon, “Cbam: Convolutional\\nblock attention module,” in Proceedings of the European Conference\\non Computer Vision (ECCV), 2018, pp. 3–19.\\n[15] Y. Yuan, L. Huang, J. Guo, et al., “Ocnet: Object context for semantic\\nsegmentation,” International Journal of Computer Vision, pp. 1–24,\\n2021.\\n[16] I. Bello, B. Zoph, A. Vaswani, et al., “Attention augmented convolu-\\ntional networks,” in Proceedings of the International Conference on\\nComputer Vision (ICCV), 2019, pp. 3286–3295.\\n[17] Y. Cao, J. Xu, S. Lin, et al., “Gcnet: Non-local networks meet squeeze-\\nexcitation networks and beyond,” in Proceedings of the International\\nConference on Computer Vision Workshops, 2019.\\n[18] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in\\nProceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR).\\nIEEE/CVF, 2018, pp. 7132–7141.\\n[19] P. Ramachandran, B. Zoph, and Q. V. Le, “Searching for activation\\nfunctions,” in International Conference on Learning Representations\\n(ICLR), 2018.\\n[20] G. Huang, Z. Liu, L. Van Der Maaten, et al., “Densely connected con-\\nvolutional networks,” in Proceedings of the Conference on Computer\\nVision and Pattern Recognition (CVPR). IEEE, 2017, pp. 4700–4708.\\n[21] A. Zeng, “Datasets,” https://vision.princeton.edu/projects/2017/\\narc/, Mar. 2018, accessed 5/4/2021.\\n[22] I. Loshchilov and F. Hutter, “Decoupled weight decay reg-\\nularization,” in International Conference on Learning Repre-\\nsentations (ICLR), 2018.\\n[23] S. Reddi, S. Kale, and S. Kumar, “On the convergence of\\nadam and beyond,” in International Conference on Learning\\nRepresentations (ICLR), 2018.\\n[24] T.-Y. Lin, P. Goyal, R. Girshick, et al., “Focal loss for\\ndense object detection,” in Proceedings of the International\\nConference on Computer Vision (ICCV).\\nIEEE, 2017, pp.\\n2980–2988.\\n[25] L.-C. C. G. P. Florian and S. H. Adam, “Rethinking atrous\\nconvolution for semantic image segmentation,” in Confer-\\nence on Computer Vision and Pattern Recognition (CVPR).\\nIEEE/CVF, 2017.\\n[26] C. Yu, J. Wang, C. Peng, et al., “Bisenet: Bilateral seg-\\nmentation network for real-time semantic segmentation,” in\\nProceedings of the European conference on computer vision\\n(ECCV), 2018, pp. 325–341.\\n[27] M. Sandler, A. Howard, M. Zhu, et al., “Mobilenetv2: Inverted\\nresiduals and linear bottlenecks,” in Proceedings of the Con-\\nference on Computer Vision and Pattern Recognition (CVPR).\\nIEEE/CVF, 2018, pp. 4510–4520.\\n[28] Y. Hong, H. Pan, W. Sun, et al., “Deep dual-resolution\\nnetworks for real-time and accurate semantic segmentation of\\nroad scenes,” arXiv preprint arXiv:2101.06085, 2021.\\n[29] J. Wang, K. Sun, T. Cheng, et al., “Deep high-resolution rep-\\nresentation learning for visual recognition,” IEEE Transactions\\non Pattern Analysis and Machine Intelligence, 2020.\\n[30] P. Chao, C.-Y. Kao, Y.-S. Ruan, et al., “Hardnet: A low\\nmemory trafﬁc network,” in Proceedings of the International\\nConference on Computer Vision (ICCV).\\nIEEE/CVF, 2019,\\npp. 3552–3561.\\n[31] J. Zhuang, J. Yang, L. Gu, and N. Dvornek, “Shelfnet for\\nfast semantic segmentation,” in Proceedings of the Interna-\\ntional Conference on Computer Vision Workshops (ICCVW).\\nIEEE/CVF, 2019, pp. 0–0.\\n[32] M. Fan, S. Lai, J. Huang, et al., “Rethinking bisenet for real-\\ntime semantic segmentation,” in Proceedings of the Confer-\\nence on Computer Vision and Pattern Recognition (CVPR).\\nIEEE/CVF, 2021, pp. 9716–9725.\\n[33] paperswithcode.com,\\n“real-time\\nsemantic\\nsegmenta-\\ntion\\nleader\\nboard,”\\nhttps://paperswithcode.com/sota/\\nreal-time-semantic-segmentation-on-cityscapes,\\n2021,\\naccessed 9/2/2021.\\n'), ResearchPaper(title='BiSeNet V2: Bilateral Network with Guided Aggregation for Real-time Semantic Segmentation', authors=[arxiv.Result.Author('Changqian Yu'), arxiv.Result.Author('Changxin Gao'), arxiv.Result.Author('Jingbo Wang'), arxiv.Result.Author('Gang Yu'), arxiv.Result.Author('Chunhua Shen'), arxiv.Result.Author('Nong Sang')], abstract='The low-level details and high-level semantics are both essential to the\\nsemantic segmentation task. However, to speed up the model inference, current\\napproaches almost always sacrifice the low-level details, which leads to a\\nconsiderable accuracy decrease. We propose to treat these spatial details and\\ncategorical semantics separately to achieve high accuracy and high efficiency\\nfor realtime semantic segmentation. To this end, we propose an efficient and\\neffective architecture with a good trade-off between speed and accuracy, termed\\nBilateral Segmentation Network (BiSeNet V2). This architecture involves: (i) a\\nDetail Branch, with wide channels and shallow layers to capture low-level\\ndetails and generate high-resolution feature representation; (ii) a Semantic\\nBranch, with narrow channels and deep layers to obtain high-level semantic\\ncontext. The Semantic Branch is lightweight due to reducing the channel\\ncapacity and a fast-downsampling strategy. Furthermore, we design a Guided\\nAggregation Layer to enhance mutual connections and fuse both types of feature\\nrepresentation. Besides, a booster training strategy is designed to improve the\\nsegmentation performance without any extra inference cost. Extensive\\nquantitative and qualitative evaluations demonstrate that the proposed\\narchitecture performs favourably against a few state-of-the-art real-time\\nsemantic segmentation approaches. Specifically, for a 2,048x1,024 input, we\\nachieve 72.6% Mean IoU on the Cityscapes test set with a speed of 156 FPS on\\none NVIDIA GeForce GTX 1080 Ti card, which is significantly faster than\\nexisting methods, yet we achieve better segmentation accuracy.', url='http://arxiv.org/abs/2004.02147v1', pdf_path='./papers/2004.02147v1.BiSeNet_V2__Bilateral_Network_with_Guided_Aggregation_for_Real_time_Semantic_Segmentation.pdf', content='Noname manuscript No.\\n(will be inserted by the editor)\\nBiSeNet V2: Bilateral Network with Guided Aggregation for\\nReal-time Semantic Segmentation\\nChangqian Yu1,2 · Changxin Gao1∗· Jingbo Wang3 · Gang Yu4 ·\\nChunhua Shen2 · Nong Sang1\\nReceived: date / Accepted: date\\nAbstract The low-level details and high-level seman-\\ntics are both essential to the semantic segmentation\\ntask. However, to speed up the model inference, cur-\\nrent approaches almost always sacriﬁce the low-level\\ndetails, which leads to a considerable accuracy decrease.\\nWe propose to treat these spatial details and categori-\\ncal semantics separately to achieve high accuracy and\\nhigh eﬃciency for real-time semantic segmentation. To\\nthis end, we propose an eﬃcient and eﬀective architec-\\nture with a good trade-oﬀbetween speed and accuracy,\\ntermed Bilateral Segmentation Network (BiSeNet V2).\\nThis architecture involves: (i) a Detail Branch, with\\nwide channels and shallow layers to capture low-level\\ndetails and generate high-resolution feature represen-\\ntation; (ii) a Semantic Branch, with narrow channels\\nand deep layers to obtain high-level semantic context.\\nThe Semantic Branch is lightweight due to reducing\\nthe channel capacity and a fast-downsampling strategy.\\nFurthermore, we design a Guided Aggregation Layer to\\nenhance mutual connections and fuse both types of fea-\\nture representation. Besides, a booster training strategy\\nis designed to improve the segmentation performance\\nwithout any extra inference cost. Extensive quantitative\\nand qualitative evaluations demonstrate that the pro-\\nChangqian Yu, Changxin Gao, Nong Sang\\nE-mail: {changqian yu, cgao, nsang}@hust.edu.cn\\nChunhua Shen\\nE-mail: chunhua.shen@adelaide.edu.au\\n1National Key Laboratory of Science and Technology on Multi-\\nspectral Information Processing, School of Artiﬁcial Intelligence\\nand Automation, Huazhong University of Science and Technol-\\nogy, Wuhan, China\\n2The University of Adelaide, Australia\\n3The Chinese University of Hong Kong\\n4Tencent\\n∗Corresponding author\\nCRF-RNN\\nDeeplab\\nFCN-8s\\nDilation10\\nFRRN\\nPSPNet\\nENet\\nSQ\\nESPNet\\nERFNet\\nFast-SCNN\\nICNet\\nDABNet\\nDFANet A`\\nDFANet A\\nDFANet B\\nGUN\\nSwiftNet\\nBiSeNetV1 A\\nBiSeNetV1 B\\nBiSeNetV2\\nBiSeNetV2-Large\\n58\\n63\\n68\\n73\\n78\\n0\\n30\\n60\\n90\\n120\\n150\\nMean IoU (%)\\nInference Speed (FPS)\\nFig. 1 Speed-accuracy trade-oﬀcomparison on the\\nCityscapes test set. Red dots indicate our methods, while\\ngrey dots means other methods. The red line represents the\\nreal-time speed.\\nposed architecture performs favourably against a few\\nstate-of-the-art real-time semantic segmentation appr-\\noaches. Speciﬁcally, for a 2,048×1,024 input, we achieve\\n72.6% Mean IoU on the Cityscapes test set with a speed\\nof 156 FPS on one NVIDIA GeForce GTX 1080 Ti\\ncard, which is signiﬁcantly faster than existing meth-\\nods, yet we achieve better segmentation accuracy. Code\\nand trained models will be made publicly available.\\nKeywords Semantic Segmentation · Real-time\\nProcessing · Deep Learning\\n1 Introduction\\nSemantic segmentation is the task of assigning seman-\\ntic labels to each pixel. It is a fundamental problem in\\ncomputer vision with extensive applications, including\\nscene understanding (Zhou et al., 2019), autonomous\\ndriving (Cordts et al., 2016; Geiger et al., 2012), human-\\narXiv:2004.02147v1  [cs.CV]  5 Apr 2020\\n2\\nChangqian Yu et al.\\nmachine interaction and video surveillance, just to name\\na few. In recent years, with the advance of convolu-\\ntional neural network (Krizhevsky et al., 2012), a series\\nof semantic segmentation methods (Zhao et al., 2017;\\nChen et al., 2017; Yu et al., 2018b; Chen et al., 2018;\\nZhang et al., 2018a) based on fully convolutional net-\\nwork (FCN) (Long et al., 2015) have constantly ad-\\nvanced the state-of-the-art performance.\\nThe high accuracy of these methods depends on\\ntheir backbone networks. There are two main archi-\\ntectures as the backbone networks: (i) Dilation Back-\\nbone, removing the downsampling operations and up-\\nsampling the corresponding ﬁlter kernels to maintain\\nhigh-resolution feature representation (Chen et al., 2015,\\n2018; Zhao et al., 2017, 2018b; Fu et al., 2019; Yu et al.,\\n2020), as shown in Figure 2 (a). (ii) Encoder-Decoder\\nBackbone, with top-down and skip connections to re-\\ncover the high-resolution feature representation in the\\ndecoder part (Lin et al., 2017; Peng et al., 2017; Yu\\net al., 2018b), as illustrated in Figure 2 (b). However,\\nboth architectures are designed for general semantic\\nsegmentation tasks with less care about the inference\\nspeed and computational cost. In the dilation back-\\nbone, the dilation convolution is time-consuming and\\nremoving down-sampling operation brings heavy com-\\nputation complexity and memory footprint. Numerous\\nconnections in the encoder-decoder architecture are less\\nfriendly to the memory access cost (Ma et al., 2018).\\nHowever, the real-time semantic segmentation applica-\\ntions demand for an eﬃcient inference speed.\\nFacing this demand, based on both backbone net-\\nworks, existing methods (Badrinarayanan et al., 2017;\\nPaszke et al., 2016; Zhao et al., 2018a; Romera et al.,\\n2018; Mazzini, 2018) mainly employ two appraches to\\naccelerate the model: (i) Input Restricting. Smaller in-\\nput resolution results in less computation cost with\\nthe same network architecture. To achieve real-time\\ninference speed, many algorithms (Zhao et al., 2018a;\\nRomera et al., 2018; Mazzini, 2018; Romera et al., 2018)\\nattempt to restrict the input size to reduce the whole\\ncomputation complexity; (ii)Channel Pruning. It is a\\nstraight-forward acceleration method, especially prun-\\ning channels in early stages to boost inference speed\\n(Badrinarayanan et al., 2017; Paszke et al., 2016; Chol-\\nlet, 2017). Although both manners can improve the in-\\nference speed to some extent, they sacriﬁce the low-level\\ndetails and spatial capacity leading to a dramatic accu-\\nracy decrease. Therefore, to achieve high eﬃciency and\\nhigh accuracy simultaneously, it is challenging and of\\ngreat importance to exploit a speciﬁc architecture for\\nthe real-time semantic segmentation task.\\nWe observe that both of the low-level details and\\nhigh-level semantics are crucial to the semantic seg-\\nmentation task. In the general semantic segmentation\\ntask, the deep and wide networks encode both informa-\\ntion simultaneously. However, in the real-time semantic\\nsegmentation task, we can treat spatial details and cat-\\negorical semantics separately to achieve the trade-oﬀ\\nbetween the accuracy and inference speed.\\nTo this end, we propose a two-pathway architecture,\\ntermed Bilateral Segmentation Network (BiSeNet V2),\\nfor real-time semantic segmentation. One pathway is\\ndesigned to capture the spatial details with wide chan-\\nnels and shallow layers, called Detail Branch. In con-\\ntrast, the other pathway is introduced to extract the\\ncategorical semantics with narrow channels and deep\\nlayers, called Semantic Branch. The Semantic Branch\\nsimply requires a large receptive ﬁeld to capture seman-\\ntic context, while the detail information can be supplied\\nby the Detail Branch. Therefore, the Semantic Branch\\ncan be made very lightweight with fewer channels and\\na fast-downsampling strategy. Both types of feature\\nrepresentation are merged to construct a stronger and\\nmore comprehensive feature representation. This con-\\nceptual design leads to an eﬃcient and eﬀective archi-\\ntecture for real-time semantic segmentation, as illus-\\ntrated in Figure 2 (c).\\nSpeciﬁcally, in this study, we design a Guided Aggre-\\ngation Layer to merge both types of features eﬀectively.\\nTo further improve the performance without increasing\\nthe inference complexity, we present a booster train-\\ning strategy with a series of auxiliary prediction heads,\\nwhich can be discarded in the inference phase. Exten-\\nsive quantitative and qualitative evaluations demon-\\nstrate that the proposed architecture performs favourably\\nagainst state-of-the-art real-time semantic segmenta-\\ntion approaches, as shown in Figure 1.\\nThe main contributions are summarized as follows:\\n– We propose an eﬃcient and eﬀective two-pathway\\narchitecture, termed Bilateral Segmentation Network,\\nfor real-time semantic segmentation, which treats\\nthe spatial details and categorical semantics sepa-\\nrately.\\n– For the Semantic Branch, we design a new light-\\nweight network based on depth-wise convolutions\\nto enhance the receptive ﬁeld and capture rich con-\\ntextual information.\\n– A booster training strategy is introduced to further\\nimprove the segmentation performance without in-\\ncreasing the inference cost.\\n– Our architecture achieves impressive results on dif-\\nferent benchmarks of Cityscapes (Cordts et al., 2016),\\nCamVid (Brostow et al., 2008a), and COCO-Stuﬀ\\n(Caesar et al., 2018). More speciﬁcally, we obtain\\nthe results of 72.6% mean IoU on the Cityscapes\\nBiSeNet V2: Bilateral Network with Guided Aggregation for Real-time Semantic Segmentation\\n3\\n1 2\\n⁄\\n1 4\\n⁄\\n1 8\\n⁄\\nImage\\n1 8\\n⁄\\n1 8\\n⁄\\n1 2\\n⁄\\n1 4\\n⁄\\n1 8\\n⁄\\nImage\\n1 16\\n⁄\\n1 32\\n⁄\\nDilation\\nConvolution\\n1 2\\n⁄\\n1 4\\n⁄\\n1 8\\n⁄\\nImage\\n4×\\n1 4\\n⁄\\n1 8\\n⁄\\nPrediction\\n1 16\\n⁄\\n1 16\\n⁄\\n1 32\\n⁄\\n1 32\\n⁄\\n1 2\\n⁄\\n1 4\\n⁄\\n1 8\\n⁄\\n1 2\\n⁄\\n1 4\\n⁄\\n1 8\\n⁄\\n1 16\\n⁄\\n1 32\\n⁄\\n4×\\n𝛼𝐶!\\n𝛼𝐶\"\\n𝛼𝐶#\\n𝐶!\\n𝐶\"\\n𝐶#\\nImage\\n(a) Dilation Backbone\\n(b) Encoder-Decoder Backbone\\n(c) Bilateral Segmentation Backbone\\nFig. 2 Illustration of diﬀerent backbone architectures. (a) is the dilation backbone network, which removes the downsam-\\npling operations and upsampling the corresponding convolution ﬁlters. It has heavy computation complexity and memory footprint.\\n(b) is the encoder-decoder backbone network, which adds extra top-down and lateral connections to recover the high-resolution\\nfeature map. These connections in the network are less friendly to the memory access cost. To achieve high accuracy and high\\neﬃciency simultaneously, we design the (c) Bilateral Segmentation backbone network. This architecture has two pathways, Detail\\nBranch for spatial details and Semantic Branch for categorical semantics. The detail branch has wide channels and shallow layers,\\nwhile the semantic branch has narrow channels and deep layers, which can be made very lightweight by the factor (λ, e.g., 1/4).\\ntest set with the speed of 156 FPS on one NVIDIA\\nGeForce GTX 1080Ti card.\\nA preliminary version of this work was published in\\n(Yu et al., 2018a). We have extended our conference\\nversion as follows. (i) We simplify the original struc-\\nture to present an eﬃcient and eﬀective architecture for\\nreal-time semantic segmentation. We remove the time-\\nconsuming cross-layer connections in the original ver-\\nsion to obtain a more clear and simpler architecture.\\n(ii) We re-design the overall architecture with more\\ncompact network structures and well-designed compo-\\nnents. Speciﬁcally, we deepen the Detail Path to encode\\nmore details. We design light-weight components based\\non the depth-wise convolutions for the Semantic Path.\\nMeanwhile, we propose an eﬃcient aggregation layer to\\nenhance the mutual connections between both paths.\\n(iii) We conduct comprehensive ablative experiments\\nto elaborate on the eﬀectiveness and eﬃciency of the\\nproposed method. (iv) We have signiﬁcantly improved\\nthe accuracy and speed of the method in our previ-\\nous work, i.e., for a 2048 × 1024 input, achieving 72.6%\\nMean IoU on the Cityscapes test set with a speed of\\n156 FPS on one NVIDIA GeForce GTX 1080Ti card.\\n2 Related Work\\nRecent years have witnessed signiﬁcant advances in im-\\nage semantic segmentation. In this section, our discus-\\nsion mainly focuses on three groups of methods most\\nrelevant to our work, i.e., generic semantic segmenta-\\ntion methods, real-time semantic segmentation meth-\\nods, and light-weight architectures.\\n2.1 Generic Semantic Segmentation\\nTraditional segmentation methods based on the thresh-\\nold selection (Otsu, 1979), the region growing (Vin-\\ncent and Soille, 1991), the super-pixel (Ren and Ma-\\nlik, 2003; Achanta et al., 2012; Van den Bergh et al.,\\n2012) and the graph (Boykov and Jolly, 2001; Rother\\net al., 2004) algorithms adopt the hand-crafted fea-\\ntures to solve this problem. Recently, a new genera-\\ntion of algorithms based on FCN (Long et al., 2015;\\nShelhamer et al., 2017) keep improving state-of-the-art\\nperformance on diﬀerent benchmarks. Various methods\\nare based on two types of backbone network: (i) di-\\nlation backbone network; (ii) encoder-decoder backbone\\nnetwork.\\nOn one hand, the dilation backbone removes the\\ndownsampling operations and upsamples the convolu-\\ntion ﬁlter to preserve high-resolution feature represen-\\ntations. Due to the simplicity of the dilation convolu-\\ntion, various methods (Chen et al., 2015, 2018; Zhao\\net al., 2017; Wang et al., 2018a; Zhang et al., 2018a; Yu\\net al., 2020) develop diﬀerent novel and eﬀective compo-\\nnents on it. The Deeplabv3 (Chen et al., 2017) devises\\nan atrous spatial pyramid pooling to capture multi-\\nscale context, while the PSPNet (Zhao et al., 2017)\\nadopts a pyramid pooling module on the dilation back-\\nbone. Meanwhile, some methods introduce the atten-\\ntion mechanisms, e.g., self-attention (Yuan and Wang,\\n2018; Fu et al., 2019; Yu et al., 2020), spatial atten-\\ntion (Zhao et al., 2018b) and channel attention (Zhang\\net al., 2018a), to capture long-range context based on\\nthe dilation backbone.\\n4\\nChangqian Yu et al.\\nOn the other hand, the encoder-decoder backbone\\nnetwork adds extra top-down and lateral connections to\\nrecover the high-resolution feature maps in the decoder\\npart. FCN and Hypercolumns (Hariharan et al., 2015)\\nadopt the skip connection to integrate the low-level fea-\\nture. Meanwhile, U-net (Ronneberger et al., 2015), Seg-\\nNet with saved pooling indices (Badrinarayanan et al.,\\n2017), ReﬁneNet with multi-path reﬁnement (Lin et al.,\\n2017), LRR with step-wise reconstruction (Ghiasi and\\nFowlkes, 2016), GCN with “large kernel” convolution\\n(Peng et al., 2017) and DFN with channel attention\\nmodule (Yu et al., 2018b) incorporate this backbone\\nnetwork to recover the detailed information. HRNet\\n(Wang et al., 2019) adopts multi-branches to maintain\\nthe high resolution.\\nBoth types of backbone network encode the low-\\nlevel details and high-level semantics simultaneously\\nwith the wide and deep networks. Although both types\\nof backbone network achieve state-of-the-art performance,\\nmost methods run at a slow inference speed. In this\\nstudy, we propose a novel and eﬃcient architecture to\\ntreat the spatial details and categorical semantics sepa-\\nrately to achieve a good trad-oﬀbetween segmentation\\naccuracy and inference speed.\\n2.2 Real-time Semantic Segmentation\\nReal-time semantic segmentation algorithms attract in-\\ncreasing attention when a growing practical applica-\\ntions require fast interaction and response. SegNet (Badri-\\nnarayanan et al., 2017) uses a small network structure\\nand the skip connection to achieve a fast speed. E-Net\\n(Paszke et al., 2016) devises a lightweight network from\\nscratch and delivers extremely high speed. ICNet (Zhao\\net al., 2018a) uses the image cascade to speed up the\\nalgorithm, while DLC (Li et al., 2017) employs a cas-\\ncade network structure to reduce the computation in\\n“easy regions”. ERFNet (Romera et al., 2018) adopts\\nthe residual connection and factorized convolutions to\\nremain eﬃcient and retain accuracy. Meanwhile, ESP-\\nNet (Mehta et al., 2018, 2019) devises an eﬃcient spa-\\ntial pyramid dilated convolution for real-time semantic\\nsegmentation. GUN (Mazzini, 2018) employs a guided\\nupsampling module to fuse the information of the multi-\\nresolution input. DFANet (Li et al., 2019b) reuses the\\nfeature to enhance the feature representation and re-\\nduces the complexity.\\nAlthough these methods can achieve a real-time in-\\nference speed, they dramatically sacriﬁce the accuracy\\nto the eﬃciency with the loss of the low-level details.\\nIn this work, we take both of the low-level details and\\nhigh-level semantics into consideration to achieve high\\naccuracy and high eﬃciency.\\n2.3 Light-weight Architecture\\nFollowing the pioneering work of group/depth-wise con-\\nvolution and separable convolution, light-weight archi-\\ntecture design has achieved rapid development, includ-\\ning Xception (Chollet, 2017), MobileNet (Howard et al.,\\n2017; Sandler et al., 2018), ShuﬄeNet (Zhang et al.,\\n2018b; Ma et al., 2018), to name a few. These methods\\nachieve a valuable trade-oﬀbetween speed and accu-\\nracy for the classiﬁcation task. In this study, we design\\na light-weight network given computation complexity,\\nmemory access cost and real inference speed for the\\nreal-time semantic segmentation.\\n3 Core Concepts of BiSeNetV2\\nOur architecture consists of a Detail Branch (Section 3.1)\\nand a Semantic Branch (Section 3.2), which are merged\\nby an Aggregation Layer (Section 3.3). In this section,\\nwe demonstrate the core concepts of our architecture,\\nas illustrated in Figure 2(c).\\n3.1 Detail Branch\\nThe Detail Branch is responsible for the spatial details,\\nwhich is low-level information. Therefore, this branch\\nrequires a rich channel capacity to encode aﬄuent spa-\\ntial detailed information. Meanwhile, because the De-\\ntail Branch simply focuses on the low-level details, we\\ncan design a shallow structure with a small stride for\\nthis branch. Overall, the key concept of the Detail Branch\\nis to use wide channels and shallow layers for the spa-\\ntial details. Besides, the feature representation in this\\nbranch has a large spatial size and wide channels. There-\\nfore, it is better not to adopt the residual connections,\\nwhich increases the memory access cost and reduce the\\nspeed.\\n3.2 Semantic Branch\\nIn parallel to the Detail Branch, the Semantic Branch\\nis designed to capture high-level semantics. This branch\\nhas low channel capacity, while the spatial details can\\nbe provided by the Detail Branch. In contrast, in our\\nexperiments, the Semantic Branch has a ratio of λ(λ <\\n1) channels of the Detail Branch, which makes this\\nbranch lightweight. Actually, the Semantic Branch can\\nbe any lightweight convolutional model (e.g., (Chol-\\nlet, 2017; Iandola et al., 2016; Howard et al., 2017;\\nSandler et al., 2018; Zhang et al., 2018b; Ma et al.,\\n2018)). Meanwhile, the Semantic Branch adopts the\\nBiSeNet V2: Bilateral Network with Guided Aggregation for Real-time Semantic Segmentation\\n5\\n×\\n×\\n𝐷𝑜𝑤𝑛\\n𝜑\\n𝑈𝑝\\n𝜑\\nSeg Head\\nSeg Head\\nLoss\\nBooster\\nAggregation Layer\\nBackbone\\nContext\\nEmbedding\\n𝐶*\\n𝐶+\\n𝐶,\\n𝛼𝐶*\\n𝛼𝐶+\\n𝛼𝐶,\\n1 2\\n⁄\\n1 4\\n⁄\\n1 8\\n⁄\\n1 2\\n⁄\\n1 4\\n⁄\\n1 8\\n⁄\\n1 16\\n⁄\\n1 32\\n⁄\\n1 32\\n⁄\\nTrain & Test\\nTrain only\\nTest only\\nDetail Branch\\nSemantic Branch\\nSeg Head\\nSeg Head\\nSeg Head\\nFig. 3 Overview of the Bilateral Segmentation Network. There are mainly three components: two-pathway backbone in\\nthe purple dashed box, the aggregation layer in the orange dashed box, and the booster part in the yellow dashed box. The\\ntwo-pathway backbone has a Detail Branch (the blue cubes) and a Semantic Branch (the green cubes). The three stages in Detail\\nBranch have C1, C2, C3 channels respectively. The channels of corresponding stages in Semantic Branch can be made lightweight\\nby the factor λ(λ < 1). The last stage of the Semantic Branch is the output of the Context Embedding Block. Meanwhile, numbers\\nin the cubes are the feature map size ratios to the resolution of the input. In the Aggregation Layer part, we adopt the bilateral\\naggregation layer. Down indicates the downsampling operation, Up represents the upsampling operation, ϕ is the Sigmoid function,\\nand N\\nmeans element-wise product. Besides, in the booster part, we design some auxiliary segmentation heads to improve the\\nsegmentation performance without any extra inference cost.\\nTable 1 Instantiation of the Detail Branch and Semantic Branch. Each stage S contains one or more operations opr\\n(e.g., Conv2d, Stem, GE, CE). Each operation has a kernels size k, stride s and output channels c, repeated r times. The expansion\\nfactor e is applied to expand the channel number of the operation. Here the channel ratio is λ = 1/4. The green colors mark\\nfewer channels of Semantic Branch in the corresponding stage of the Detail Branch. Notation: Conv2d means the convolutional\\nlayer, followed by one batch normalization layer and relu activation function. Stem indicates the stem block. GE represents the\\ngather-and-expansion layer. CE is the context embedding block.\\nStage\\nDetail Branch\\nSemantic Branch\\nOutput Size\\nopr\\nk\\nc\\ns\\nr\\nopr\\nk\\nc\\ne\\ns\\nr\\nInput\\n512×1024\\nS1\\nConv2d\\n3\\n64\\n2\\n1\\nStem\\n3\\n16\\n-\\n4\\n1\\n256×512\\nConv2d\\n3\\n64\\n1\\n1\\n256×512\\nS2\\nConv2d\\n3\\n64\\n2\\n1\\n128×256\\nConv2d\\n3\\n64\\n1\\n2\\n128×256\\nS3\\nConv2d\\n3\\n128\\n2\\n1\\nGE\\n3\\n32\\n6\\n2\\n1\\n64×128\\nConv2d\\n3\\n128\\n1\\n2\\nGE\\n3\\n32\\n6\\n1\\n1\\n64×128\\nS4\\nGE\\n3\\n64\\n6\\n2\\n1\\n32×64\\nGE\\n3\\n64\\n6\\n1\\n1\\n32×64\\nS5\\nGE\\n3\\n128\\n6\\n2\\n1\\n16×32\\nGE\\n3\\n128\\n6\\n1\\n3\\n16×32\\nCE\\n3\\n128\\n-\\n1\\n1\\n16×32\\nfast-downsampling strategy to promote the level of the\\nfeature representation and enlarge the receptive ﬁeld\\nquickly. High-level semantics require large receptive ﬁeld.\\nTherefore, the Semantic Branch employs the global av-\\nerage pooling (Liu et al., 2016) to embed the global\\ncontextual response.\\n3.3 Aggregation Layer\\nThe feature representation of the Detail Branch and\\nthe Semantic Branch is complementary, one of which\\nis unaware of the information of the other one. Thus,\\nan Aggregation Layer is designed to merge both types\\nof feature representation. Due to the fast-downsampling\\nstrategy, the spatial dimensions of the Semantic Branch’s\\noutput are smaller than the Detail Branch. We need\\n6\\nChangqian Yu et al.\\nto upsample the output feature map of the Semantic\\nBranch to match the output of the Detail Branch.\\nThere are a few manners to fuse information, e.g., sim-\\nple summation, concatenation and some well-designed\\noperations. We have experimented diﬀerent fusion meth-\\nods with consideration of accuracy and eﬃciency. At\\nlast, we adopt the bidirectional aggregation method, as\\nshown in Figure 3.\\n4 Bilateral Segmentation Network\\nThe concept of our BiSeNet is generic, which can be\\nimplemented with diﬀerent convolutional models (He\\net al., 2016; Huang et al., 2017; Chollet, 2017; Iandola\\net al., 2016; Howard et al., 2017; Sandler et al., 2018;\\nZhang et al., 2018b; Ma et al., 2018) and any speciﬁc\\ndesigns. There are mainly three key concepts: (i) The\\nDetail Branch has high channel capacity and shallow\\nlayers with small receptive ﬁeld for the spatial details;\\n(ii)The Semantic Branch has low channel capacity and\\ndeep layers with large receptive ﬁeld for the categori-\\ncal semantics. (iii)An eﬃcient Aggregation Layer is de-\\nsigned to fuse both types of representation.\\nIn this subsection, according to the proposed con-\\nceptual design, we demonstrate our instantiations of the\\noverall architecture and some other speciﬁc designs, as\\nillustrated in Figure 3.\\n4.1 Detail Branch\\nThe instantiation of the Detail Branch in Table 1 con-\\ntains three stages, each layer of which is a convolu-\\ntion layer followed by batch normalization (Ioﬀe and\\nSzegedy, 2015) and activation function (Glorot et al.,\\n2011). The ﬁrst layer of each stage has a stride s = 2,\\nwhile the other layers in the same stage have the same\\nnumber of ﬁlters and output feature map size. There-\\nfore, this branch extracts the output feature maps that\\nare 1/8 of the original input. This Detail Branch en-\\ncodes rich spatial details due to the high channel capac-\\nity. Because of the high channel capacity and the large\\nspatial dimension, the residual structure (He et al., 2016)\\nwill increases the memory access cost (Ma et al., 2018).\\nTherefore, this branch mainly obeys the philosophy of\\nVGG nets (Simonyan and Zisserman, 2015) to stack the\\nlayers.\\n4.2 Semantic Branch\\nIn consideration of the large receptive ﬁeld and eﬃcient\\ncomputation simultaneously, we design the Semantic\\n Conv, \\n3 × 3\\n(𝑠𝑡𝑟𝑖𝑑𝑒= 2)\\n MPooling\\n3 × 3\\n(𝑠𝑡𝑟𝑖𝑑𝑒= 2)\\nC\\n Conv\\n1 × 1\\n Conv\\n3 × 3\\n(𝑠𝑡𝑟𝑖𝑑𝑒= 2)\\n Conv\\n3 × 3\\nBN  ReLu\\n(𝐻/2 × 𝑊/2 × 𝐶)\\nBN  ReLu\\n(𝐻/2 × 𝑊/2 × 𝐶/2)\\nBN  ReLu\\n(𝐻/4 × 𝑊/4 × 𝐶)\\nInput\\n(𝐻× 𝑊× 3)\\nBN  ReLu\\n(𝐻/4 × 𝑊/4 × 𝐶)\\n(𝐻/4 × 𝑊/4 × 𝐶)\\nBN\\n(1 × 1 × 𝐶)\\nBN  ReLu Broadcast\\n(1 × 1 × 𝐶)\\n(𝐻× 𝑊× 𝐶)\\n GAPooling\\n3 × 3\\n Conv\\n1 × 1\\n(𝐻× 𝑊× 𝐶)\\n+\\n Conv\\n3 × 3\\n(a) Stem Block\\n(b) Context Embedding Block\\nFig. 4 Illustration of Stem Block and Context Em-\\nbedding Block. (a) is the Stem Block, which adopts a fast-\\ndownsampling strategy. This block has two branches with dif-\\nferent manners to downsample the feature representation. Then\\nboth feature response of two branches is concatenated as the\\noutput. (b) is the Context Embedding Block. As demonstrated\\nin Section 3.2, the Semantic Branch requires large receptive\\nﬁeld. Therefore, we design a Context Embedding Block with\\nthe global average pooling to embed the global contextual in-\\nformation. Notation: Conv is convolutional operation. BN is\\nthe batch normalization. ReLu is the ReLu activation function.\\nMpooling is the max pooling. GPooling is the global average\\npooling. and C means concatenation. Meanwhile, 1 × 1, 3 × 3\\ndenote the kernel size, H × W × C means the tensor shape\\n(height, width, depth).\\nBranch, inspired by the philosophy of the lightweight\\nrecognition model, e.g., Xception (Chollet, 2017), Mo-\\nbileNet (Howard et al., 2017; Sandler et al., 2018; Howard\\net al., 2019), ShuﬄeNet (Zhang et al., 2018b; Ma et al.,\\n2018). Some of the key features of the Semantic Branch\\nare as follows.\\nStem Block\\nInspired from (Szegedy et al., 2017; Shen\\net al., 2017; Wang et al., 2018b), we adopt the Stem\\nBlock as the ﬁrst stage of the Semantic Branch, as illus-\\ntrated in Figure 4. It uses two diﬀerent downsampling\\nmanners to shrink the feature representation. And then\\nthe output feature of both branches are concatenated\\nas the output. This structure has eﬃcient computation\\ncost and eﬀective feature expression ability.\\nContext Embedding Block\\nAs discussed in Sec-\\ntion 3.2, the Semantic Branch requires large receptive\\nﬁeld to capture high-level semantics. Inspired from (Yu\\net al., 2018b; Liu et al., 2016; Zhao et al., 2017; Chen\\net al., 2017), we design the Context Embedding Block.\\nThis block uses the global average pooling and residual\\nconnection (He et al., 2016) to embed the global con-\\ntextual information eﬃciently, as showed in Figure 4.\\nBiSeNet V2: Bilateral Network with Guided Aggregation for Real-time Semantic Segmentation\\n7\\nBN  ReLu6\\n(𝐻× 𝑊× 6𝐶)\\nBN  ReLu6\\n(𝐻× 𝑊× 6𝐶)\\nBN\\n(𝐻× 𝑊× 𝐶)\\n Conv\\n1 × 1\\n DWConv\\n3 × 3\\n Conv\\n1 × 1\\n+\\n(𝐻× 𝑊× 𝐶)\\n DWConv\\n3 × 3\\n Conv\\n1 × 1\\n+\\n Conv\\n3 × 3\\n(𝐻× 𝑊× 𝐶)\\nBN  ReLu\\n(𝐻× 𝑊× 𝐶)\\nBN \\n(𝐻× 𝑊× 6𝐶)\\nBN \\n(𝐻× 𝑊× 𝐶)\\nReLu\\n DWConv\\n3 × 3\\n(𝑠𝑡𝑟𝑖𝑑𝑒= 2)\\n DWConv\\n3 × 3\\n Conv\\n1 × 1\\n+\\n Conv\\n3 × 3\\n(𝐻× 𝑊×\\n)\\n𝐶𝑖\\nBN  ReLu\\n(𝐻× 𝑊×\\n)\\n𝐶𝑖\\nBN \\n(𝐻/2 × 𝑊/2 × 6\\n)\\n𝐶𝑖\\nBN \\n(𝐻/2 × 𝑊/2 ×\\n)\\n𝐶𝑜\\nReLu\\n DWConv\\n3 × 3\\n(𝑠𝑡𝑟𝑖𝑑𝑒= 2)\\n Conv\\n1 × 1\\nBN \\nBN \\nBN \\n(𝐻/2 × 𝑊/2 × 6\\n)\\n𝐶𝑖\\n(a)\\n(b)\\n(c)\\nFig. 5 Illustration of Inverted Bottleneck and Gather-\\nand-Expansion Layer. (a) is the mobile inverted bottleneck\\nConv proposed in MobileNetv2. The dashed shortcut path and\\nsummation circle do not exist with the stride = 2. (b)(c) are the\\nproposed Gather-and-Expansion Layer. The bottleneck struc-\\nture adopts: (i) a 3 × 3 convolution to gather local feature\\nresponse and expand to higher-dimensional space;(ii) a 3 × 3\\ndepth-wise convolution performed independently over each in-\\ndividual output channel of the expansion layer; (iii) a 1×1 con-\\nvolution as the projection layer to project the output of depth-\\nwise convolution into a low channel capacity space. When the\\nstride = 2, we adopt two kernel size = 3 depth-wise convolu-\\ntions on the main path and a 3 × 3 separable convolution as\\nthe shortcut. Notation: Conv is convolutional operation. BN is\\nthe batch normalization. ReLu is the ReLu activation function.\\nMeanwhile, 1×1, 3×3 denote the kernel size, H ×W ×C means\\nthe tensor shape (height, width, depth).\\nGather-and-Expansion Layer\\nTaking advantage of\\nthe beneﬁt of depth-wise convolution, we propose the\\nGather-and-Expansion Layer, as illustrated in Figure 5.\\nThe Gather-and-Expansion Layer consists of: (i) a 3×3\\nconvolution to eﬃciently aggregate feature responses\\nand expand to a higher-dimensional space; (iii) a 3 × 3\\ndepth-wise convolution performed independently over\\neach individual output channel of the expansion layer;\\n(iv) a 1×1 convolution as the projection layer to project\\nthe output of depth-wise convolution into a low chan-\\nnel capacity space. When stide = 2, we adopt two 3×3\\ndepth-wise convolution, which further enlarges the re-\\nceptive ﬁeld, and one 3×3 separable convolution as the\\nshortcut. Recent works (Tan et al., 2019; Howard et al.,\\n2019) adopt 5 × 5 separable convolution heavily to en-\\nlarge the receptive ﬁeld, which has fewer FLOPS than\\ntwo 3 × 3 separable convolution in some conditions. In\\nthis layer, we replace the 5 × 5 depth-wise convolution\\nin the separable convolution with two 3 × 3 depth-wise\\nconvolution, which has fewer FLOPS and the same re-\\nceptive ﬁeld.\\nIn contrast to the inverted bottleneck in MobileNetv2,\\nthe GE Layer has one more 3×3 convolution. However,\\nthis layer is also friendly to the computation cost and\\n DWConv\\n3 × 3\\n Conv\\n3 × 3\\n(𝑠𝑡𝑟𝑖𝑑𝑒= 2)\\n APooling\\n3 × 3\\n(𝑠𝑡𝑟𝑖𝑑𝑒= 2)\\n DWConv\\n3 × 3\\n Conv\\n3 × 3\\n Upsample\\n4 × 4\\n×\\n×\\n Conv\\n3 × 3\\nDetail Branch\\nSemantic Branch\\n(𝐻/4 × 𝑊/4 × 𝐶)\\n(𝐻× 𝑊× 𝐶)\\nBN\\n(𝐻/4 × 𝑊/4 × 𝐶)\\nBN\\n(𝐻/4 × 𝑊/4 × 𝐶)\\nBN\\n(𝐻/2 × 𝑊/2 × 𝐶)\\n(𝐻/4 × 𝑊/4 × 𝐶)\\nSigmoid\\n(𝐻× 𝑊× 𝐶)\\nBN\\n(𝐻× 𝑊× 𝐶)\\nBN\\n(𝐻× 𝑊× 𝐶)\\nSum\\n Conv\\n1 × 1\\n Conv\\n1 × 1\\nSigmoid\\n(𝐻/4 × 𝑊/4 × 𝐶)\\nFig. 6 Detailed design of Bilateral Guided Aggrega-\\ntion Layer. Notation: Conv is convolutional operation. DW-\\nConv is depth-wise convolution. APooling is average pooling.\\nBN denotes the batch normalization. Upsample means bilinear\\ninterpolation. Sigmoid is the Sigmoid activation function. Sum\\nmeans summation. Meanwhile, 1×1, 3×3 denote the kernel size,\\nH × W × C means the tensor shape (height, width, depth), N\\nrepresents element-wise product.\\nmemory access cost (Ma et al., 2018; Sandler et al.,\\n2018), because the 3 × 3 convolution is specially op-\\ntimized in the CUDNN library (Chetlur et al., 2014;\\nMa et al., 2018). Meanwhile, because of this layer, the\\nGE Layer has higher feature expression ability than the\\ninverted bottleneck.\\n4.3 Bilateral Guided Aggregation\\nThere are some diﬀerent manners to merge two types of\\nfeature response, i.e., element-wise summation and con-\\ncatenation. However, the outputs of both branches have\\ndiﬀerent levels of feature representation. The Detail\\nBranch is for the low-level, while the Semantic Branch\\nis for the high-level. Therefore, simple combination ig-\\nnores the diversity of both types of information, leading\\nto worse performance and hard optimization.\\nBased on the observation, we propose the Bilateral\\nGuided Aggregation Layer to fuse the complementary\\ninformation from both branches, as illustrated in Fig-\\nure 6. This layer employs the contextual information of\\nSemantic Branch to guide the feature response of De-\\ntail Branch. With diﬀerent scale guidance, we can cap-\\nture diﬀerent scale feature representation, which inher-\\nently encodes the multi-scale information. Meanwhile,\\nthis guidance manner enables eﬃcient communication\\nbetween both branches compared to the simple combi-\\nnation.\\n8\\nChangqian Yu et al.\\nConv\\n3 × 3\\nConv\\n1 × 1\\nBN  ReLu \\n(𝐻× 𝑊×\\n)\\n𝐶𝑡\\nUpsample\\n(𝑆𝐻× 𝑆𝑊× 𝑁)\\n(𝐻× 𝑊×\\n)\\n𝐶𝑖\\nFig.\\n7 Detailed\\ndesign\\nof\\nSegmentation\\nHead\\nin\\nBooster. Notation: Conv is convolutional operation. BN de-\\nnotes the batch normalization. Upsample means bilinear in-\\nterpolation. Meanwhile, 1 × 1, 3 × 3 denote the kernel size,\\nH × W × C means the tensor shape (height, width, depth),\\nC represents the channel dimension, S denotes the scale ratio\\nof upsampling, and N is the ﬁnal output dimension.\\n4.4 Booster Training Strategy\\nTo further improve the segmentation accuracy, we pro-\\npose a booster training strategy. As the name implies,\\nit is similar to the rocket booster: it can enhance the\\nfeature representation in the training phase and can be\\ndiscarded in the inference phase. Therefore, it increases\\nlittle computation complexity in the inference phase. As\\nillustrated in Figure 3, we can insert the auxiliary seg-\\nmentation head to diﬀerent positions of the Semantic\\nBranch. In Section 5.1, we analyze the eﬀect of diﬀer-\\nent positions to insert. Figure 7 illustrates the details\\nof the segmentation head. We can adjust the computa-\\ntional complexity of auxiliary segmentation head and\\nmain segmentation head by controlling the channel di-\\nmension Ct.\\n5 Experimental Results\\nIn this section, we ﬁrst introduce the datasets and the\\nimplementation details. Next, we investigate the eﬀects\\nof each component of our proposed approach on City-\\nscapes validation set. Finally, we report our ﬁnal accu-\\nracy and speed results on diﬀerent benchmarks com-\\npared with other algorithms.\\nDatasets.\\nCityscapes (Cordts et al., 2016) focuses\\non semantic understanding of urban street scenes from\\na car perspective. The dataset is split into training,\\nvalidation and test sets, with 2, 975, 500 and 1, 525\\nimages respectively. In our experiments, we only use\\nthe ﬁne annotated images to validate the eﬀectiveness\\nof our proposed method. The annotation includes 30\\nclasses, 19 of which are used for semantic segmentation\\ntask. This dataset is challenging for the real-time se-\\nmantic segmentation because of its high resolution of\\n2, 048 × 1, 024.\\nCambridge-driving Labeled Video Database (Cam-\\nVid) (Brostow et al., 2008a) is a road scene dataset from\\nthe perspective of a driving automobile. It contains 701\\nimages with 960 × 720 resolution extracted from the\\nvideo sequences. Following the pioneering work (Bros-\\ntow et al., 2008b; Sturgess et al., 2009; Badrinarayanan\\net al., 2017), the images are split into 367 for training,\\n101 for validation and 233 for testing. We use the sub-\\nset of 11 classes of the provided 32 candidate categories\\nfor the fair comparison with other methods. The pixels\\ndo not belong to one of these classes are ignored.\\nCOCO-Stuﬀ(Caesar et al., 2018) augments 10K\\ncomplex images of the popular COCO (Lin et al., 2014)\\ndataset with dense stuﬀannotations. This is also a chal-\\nlenging dataset for the real-time semantic segmentation\\nbecause it has more complex categories, including 91\\nthing and 91 stuﬀclasses for evaluation. For a fair com-\\nparison, we follow the split in (Caesar et al., 2018): 9K\\nimages for training and 1K images for testing.\\nTraining.\\nOur models are trained from scratch with\\nthe “kaiming normal” initialization manner (He et al.,\\n2015). We use the stochastic gradient descent (SGD)\\nalgorithm with 0.9 momentum to train our model. For\\nall datasets, we adopt 16 batch size. For the Cityscapes\\nand CamVid datasets, the weight decay is 0.0005 weight\\ndecay while the weight decay is 0.0001 for the COCO-\\nStuﬀdataset. We note that the weight decay regulariza-\\ntion is only employed on the parameters of the convolu-\\ntion layers. The initial rate is set to 5e−2 with a “poly”\\nlearning rate strategy in which the initial rate is mul-\\ntiplied by (1 −\\niter\\nitersmax )power each iteration with power\\n0.9. Besides, we train the model for 150K, 10K, 20K\\niterations for the Cityscapes dataset, CamVid dataset,\\nand COCO-Stuﬀdatasets respectively.\\nFor the augmentation, we randomly horizontally ﬂip,\\nrandomly scale, and randomly crop the input images\\nto a ﬁxed size for training. The random scales contain\\n{ 0.75, 1, 1.25, 1.5, 1.75, 2.0}. And the cropped reso-\\nlutions are 2048 × 1024 for Cityscapes, 960 × 720 for\\nCamVid, 640 × 640 for COCO-Stuﬀrespectively. Be-\\nsides, the augmented input of Cityscapes will be resized\\ninto 1024 × 512 resolution to train our model.\\nInference.\\nWe do not adopt any evaluation tricks,\\ne.g., sliding-window evaluation and multi-scale testing,\\nwhich can improve accuracy but are time-consuming.\\nWith the input of 2048 × 1024 resolution, we ﬁrst re-\\nsize it to 1024 × 512 resolution to inference and then\\nresize the prediction to the original size of the input.\\nWe measure the inference time with only one GPU\\ncard and repeat 5000 iterations to eliminate the er-\\nror ﬂuctuation. We note that the time of resizing is\\nincluded in the inference time measurement. In other\\nwords, when measuring the inference time, the practical\\ninput size is 2048×1024. Meanwhile, we adopt the stan-\\ndard metric of the mean intersection of union (mIoU)\\nBiSeNet V2: Bilateral Network with Guided Aggregation for Real-time Semantic Segmentation\\n9\\nTable 2 Ablations on Cityscapes. We validate the eﬀectiveness of each component step by step. We show segmentation\\naccuracy (mIoU%), and computational complexity measured in GFLOPs with the input of spatial size 2048 × 1024. Notation:\\nDetail is the Detail Branch. Semantic is the Semantic Branch. BGA represents the Bilateral Guided Aggregation Layer. Booster\\nmeans the booster training strategy. OHEM is the online hard example mining.\\nDetail\\nSemantic\\nAggregation\\nBooster\\nOHEM\\nmIoU(%)\\nGFLOPs\\nSum\\nConcate\\nBGA\\n\\x13\\n62.35\\n15.26\\n\\x13\\n64.68\\n7.63\\n\\x13\\n\\x13\\n\\x13\\n68.60\\n20.77\\n\\x13\\n\\x13\\n\\x13\\n68.93\\n21.98\\n\\x13\\n\\x13\\n\\x13\\n69.67\\n21.15\\n\\x13\\n\\x13\\n\\x13\\n\\x13\\n73.19\\n21.15\\n\\x13\\n\\x13\\n\\x13\\n\\x13\\n\\x13\\n73.36\\n21.15\\nTable 3 Ablations on the Semantic Branch design on Cityscapes. We conduct experiments about the channel capacity,\\nthe block design, and the expansion ratio of the Semantic Branch. Notation: GLayer indicates the Gather Layer, the ﬁrst 3 × 3\\nconvolution in GE Layer. DDWConv is double depth-wise convolution layer.\\nmIoU(%) GFLOPs\\nDetail-only\\n62.35\\n15.26\\nλ = 1/2\\n69.66\\n25.84\\n1/4\\n69.67\\n21.15\\n1/8\\n69.26\\n19.93\\n1/16\\n68.27\\n19.61\\n(a)\\nChannel\\ncapacity\\nratio:\\nVarying values of λ can control the\\nchannel capacity of the ﬁrst two\\nstages in the Semantic Branch. The\\nchannel dimensions of the last two\\nstages are still 64 and 128. Here, we\\nchoose λ = 1/4.\\nGLayer DDWConv Context mIoU(%) GFLOPs\\n\\x13\\n\\x13\\n\\x13\\n69.67\\n21.15\\n\\x13\\n\\x13\\n69.01\\n21.07\\n\\x13\\n\\x13\\n68.98\\n21.15\\n\\x13\\n\\x13\\n66.62\\n15.78\\n(b) Block Analysis: We speciﬁcally design the\\nGE Layer and adopt double depth-wise convolutions\\nwhen stride = 2. The second row means we use one\\n5 × 5 depth-wise convolution instead of two 3 × 3\\ndepth-wise convolution. The third row represents we\\nreplace the ﬁrst 3 × 3 convolution layer of GE Layer\\nwith the 1 × 1 convolution.\\nmIoU(%) GFLOPs\\nDetail-only\\n62.35\\n15.26\\nϵ = 1\\n67.48\\n17.78\\n2\\n68.41\\n18.45\\n4\\n68.78\\n19.8\\n6\\n69.67\\n21.15\\n8\\n68.99\\n22.49\\n(c) Expansion ratio: Varying val-\\nues of ϵ can aﬀect the representa-\\ntive ability of the Semantic Branch.\\nWe choose the ϵ\\n=\\n6 to make\\nthe trade-oﬀbetween accuracy and\\ncomputation complexity.\\nfor the Cityscapes dataset and CamVid dataset, while\\nthe mIoU and pixel accuracy (pixAcc) for the COCO-\\nStuﬀdataset.\\nSetup.\\nWe conduct experiments based on PyTorch\\n1.0. The measurement of inference time is executed on\\none NVIDIA GeForce GTX 1080Ti with the CUDA 9.0,\\nCUDNN 7.0 and TensorRT v5.1.51.\\n5.1 Ablative Evaluation on Cityscapes\\nThis section introduces the ablation experiments to val-\\nidate the eﬀectiveness of each component in our method.\\nIn the following experiments, we train our models on\\nCityscapes (Cordts et al., 2016) training set and evalu-\\nate on the Cityscapes validation set.\\nIndividual pathways.\\nWe ﬁrst explore the eﬀect of\\nindividual pathways speciﬁcally. The ﬁrst two rows in\\nTable 2 illustrates the segmentation accuracy and com-\\nputational complexity of using only one pathway alone.\\nThe Detail Branch lacks suﬃcient high-level semantics,\\n1 We use FP32 data precision.\\nwhile the Semantic Branch suﬀers from a lack of low-\\nlevel spatial details, which leads to unsatisfactory re-\\nsults. Figure 8 illustrates the gradual attention on the\\nspatial details of Detail Branch. The second group in\\nTable 2 shows that the diﬀerent combinations of both\\nbranches are all better than the only one pathway mod-\\nels. Both branches can provide a complementary repre-\\nsentation to achieve better segmentation performance.\\nThe Semantic Branch and Detail Branch alone only\\nachieve 64.68% and 62.35% mean IoU. However, with\\nthe simple summation, the Semantic Branch can bring\\nin over 6% improvement to the Detail Branch, while\\nthe Detail Branch can acquire 4% gain for the Seman-\\ntic Branch. This observation shows that the represen-\\ntations of both branches are complementary.\\nAggregation methods.\\nWe also investigate the ag-\\ngregation methods of two branches, as illustrated in\\nTable 2. For an eﬀective and eﬃcient aggregation, we\\ndesign the Bilateral Guided Aggregation Layer, which\\nadopts the high-level semantics as the guidance to ag-\\ngregate the multi-scale low-level details. We also show\\ntwo variants without Bilateral Guided Aggregation Layer\\nas the naive aggregation baseline: summation and con-\\ncatenation of the outputs of both branches. For a fair\\n10\\nChangqian Yu et al.\\n(a) Stage1\\n(b) Stage2\\n(c) Stage3\\nFig. 8 Examples showing visual explanations for the diﬀerent stages of the Detail Branch. Following the Grad-\\nCAM (Selvaraju et al., 2017), we visualize the Grad-CAMs of Detail Branch. The visualization shows that Detail Branch can focus\\non the spatial details, e.g., boundary, gradually.\\ncomparison, the inputs of the summation and concate-\\nnation are through one separable layer respectively. Fig-\\nure 9 demonstrates the visualization outputs of Detail\\nBranch, Semantic Branch and the aggregation of both\\nbranches. This illustrates that Detail Branch can pro-\\nvide suﬃcient spatial details, while Semantic Branch\\ncaptures the semantic context.\\nTable 3 illustrates a series of analysis experiments\\non the Semantic Branch design.\\nChannel capacity of Semantic Branch.\\nAs dis-\\ncussed in Section 3.2 and Section 4.2, the Semantic\\nBranch is responsible for the high-level semantics, with-\\nout caring the spatial details. Therefore, the Semantic\\nBranch can be made very lightweight with low chan-\\nnel capacity, which is adapted by the channel capacity\\nratio of λ. Table 4a illustrates the detailed comparison\\nexperiments of varying λ.\\nDiﬀerent λ brings in diﬀerent extents of improve-\\nment to the Detail-only baseline. Even when λ = 1/16,\\nthe ﬁrst layer of the Semantic Branch has only 4 channel\\ndimension, which also brings in 6% (62.35% →68.27%)\\nTable 4 Booster position. We can add the auxiliary segmen-\\ntation head to diﬀerent positions as the booster of the Seman-\\ntic Branch. Here, stages represents the auxiliary segmentation\\nhead can be added after s stage. stage5 4 and stage5 5 means\\nthe position before or after the Context Embedding Block re-\\nspectively. OHEM represents the online bootstrapping strategy.\\nstage2 stage3 stage4 stage5 4 stage5 5 OHEM mIoU(%)\\n69.67\\n\\x13\\n\\x13\\n\\x13\\n\\x13\\n\\x13\\n73.04\\n\\x13\\n\\x13\\n\\x13\\n\\x13\\n73.19\\n\\x13\\n\\x13\\n\\x13\\n71.62\\n\\x13\\n\\x13\\n\\x13\\n\\x13\\n72.84\\n\\x13\\n\\x13\\n\\x13\\n72.68\\n\\x13\\n\\x13\\n72.03\\n\\x13\\n\\x13\\n\\x13\\n\\x13\\n\\x13\\n73.36\\nimprovement to the baseline. Here, we employ λ = 1/4\\nas our default.\\nBlock design of of Semantic Branch.\\nFollowing\\nthe pioneer work (Sandler et al., 2018; Howard et al.,\\n2019), we design a Gather-and-Expansion Layer, as dis-\\ncussed in Section 4.2 and illustrated in Figure 5. The\\nmain improvements consist of two-fold: (i) we adopt one\\n3 × 3 convolution as the Gather Layer instead of one\\nBiSeNet V2: Bilateral Network with Guided Aggregation for Real-time Semantic Segmentation\\n11\\n(a) Input\\n(b) Detail Branch\\n(c) Semantic Branch\\n(d) Prediction\\n(e) Groundtruth\\nFig. 9 Visual improvement of the Bilateral Guided Aggregation layer on the Cityscapes val set.\\nTable 5 Generalization to large models. We enlarge our\\nmodels from two aspects: (i) wider models; (ii) deeper models.\\nWider mIoU(%) GFLOPs\\nα = 1.0\\n73.36\\n21.15\\n1.25\\n73.61\\n34.98\\n1.50\\n74.67\\n49.46\\n1.75\\n74.04\\n66.45\\n2.0\\n75.11\\n85.94\\n(a) Wider models: Varying\\nvalues of α can control the\\nchannel capacity of our archi-\\ntecture.\\nDeeper mIoU(%) GFLOPs\\nd = 1.0\\n73.36\\n21.15\\n2.0\\n74.10\\n25.26\\n3.0\\n74.28\\n29.38\\n4.0\\n74.02\\n33.5\\n(b) Deeper models: Varying\\nvalues of d represents the layer\\nnumber of the model.\\nTable 6 Compatibility with other models. We empoly\\ndiﬀerent light-weight models as the Semantic Branch to explore\\nthe compatibility of our architecture.\\nSemantic Branch\\nPretrained mIoU(%) GFLOPs\\nShuﬄeNetV2 1.5×\\nImageNet\\n74.07\\n128.71\\nMobileNetV2\\nImageNet\\n72.95\\n129.45\\nResNet-18\\nImageNet\\n75.22\\n143.34\\nOurs(α = 1.0, d = 1.0)\\nno\\n73.36\\n21.15\\nOurs(α = 2.0, d = 3.0)\\nno\\n75.80\\n118.51\\npoint-wise convolution in the inverted bottleneck of Mo-\\nbileNetV2 (Sandler et al., 2018); (ii) when stride = 2,\\nwe employs two 3 × 3 depth-wise convolution to substi-\\ntute a 5 × 5 depth-wise convolution.\\nTable 4b shows the improvement of our block de-\\nsign. The Gather-and-Expansion Layer can enlarge the\\nreceptive ﬁeld to capture high-level semantics eﬃciently.\\nExpansion ratio of GE layer.\\nThe ﬁrst 3 × 3 con-\\nvolution layer in GE Layer is also an expansion layer,\\nwhich can project the input to a high-dimensional space.\\nIt has an advantage in memory access cost (Sandler\\net al., 2018; Howard et al., 2019). The expansion ratio\\nof ϵ can control the output dimension of this layer. Ta-\\nble 4c investigates the eﬀect of varying ϵ. It is surprising\\nto see that even with ϵ = 1, the Semantic Branch can\\nalso improve the baseline by 4% (62.35% →67.48%)\\nmean IoU, which validates the lightweight Semantic\\nBranch is eﬃcient and eﬀective.\\nBooster training strategy.\\nWe propose a booster\\ntraining strategy to further improve segmentation ac-\\ncuracy, as discussed in Section 4.4. We insert the seg-\\nmentation head illustrated in Figure 7 to diﬀerent po-\\nsitions of Semantic Branch in the training phase, which\\nare discarded in the inference phase. Therefore, they\\nincrease little computation complexity in the inference\\nphase, which is similar to the booster of the rocket. Ta-\\nble 4 shows the eﬀect of diﬀerent positions to insert\\nsegmentation head. As we can see, the booster training\\nstrategy can obviously improve segmentation accuracy.\\nWe choose the conﬁguration of the third row of Ta-\\nble 4, which further improves the mean IoU by over 3%\\n(69.67% →73.19%), without sacriﬁcing the inference\\nspeed. Based on this conﬁguration, we adopt the online\\nbootstrapping strategy (Wu et al., 2016) to improve the\\nperformance further.\\n5.2 Generalization Capability\\nIn this section, we mainly explore the generalization\\ncapability of our proposed architecture. First, we in-\\nvestigate the performance of a wider model and deeper\\n12\\nChangqian Yu et al.\\nTable\\n7 Comparison\\nwith\\nstate-of-the-art\\non\\nCityscapes.\\nWe\\ntrain\\nand\\nevaluate\\nour\\nmodels\\nwith\\n2048 × 1024 resolution input, which is resized into 1024 × 512\\nin the model. The inference time is measured on one NVIDIA\\nGeForce 1080Ti card. Notation: γ is the downsampling ratio\\ncorresponding to the original 2048 × 1024 resolution. backbone\\nindicates the backbone models pre-trained on the ImageNet\\ndataset. “-” represents that the methods do not report the\\ncorresponding result. The DFANet A and DFANet B adopt\\nthe 1024 × 1024 input size and use the optimized depth-wise\\nconvolutions to accelerate speed.\\nmethod\\nref.\\nγ\\nbackbone\\nmIoU(%)\\nFPS\\nval\\ntest\\nlarge models\\nCRF-RNN∗\\nICCV2015\\n0.5\\nVGG16\\n-\\n62.5\\n1.4\\nDeepLab∗\\nICLR2015\\n0.5\\nVGG16\\n-\\n63.1\\n0.25\\nFCN-8S∗\\nCVPR2015\\n1.0\\nVGG16\\n-\\n65.3\\n2\\nDilation10\\nICLR2016\\n1.0\\nVGG16\\n68.7 67.1\\n0.25\\nLRR\\nECCV2016\\n1.0\\nVGG16\\n70.0 69.7\\n-\\nDeeplabv2\\nICLR2016\\n1.0\\nResNet101 71.4 70.4\\n-\\nFRRN\\nCVPR2017\\n0.5\\nno\\n-\\n71.8\\n2.1\\nReﬁneNet\\nCVPR2017\\n1.0\\nResNet101\\n-\\n73.6\\n-\\nDUC\\nWACV2018\\n1.0\\nResNet101 76.7 76.1\\n-\\nPSPNet\\nCVPR2017\\n1.0\\nResNet101\\n-\\n78.4 0.78\\nsmall models\\nENet\\narXiv2016\\n0.5\\nno\\n-\\n58.3\\n76.9\\nSQ\\nNIPSW2016 1.0 SqueezeNet\\n-\\n59.8\\n16.7\\nESPNet\\nECCV2018\\n0.5\\nESPNet\\n-\\n60.3 112.9\\nESPNetV2\\nCVPR2019\\n0.5\\nESPNetV2 66.4 66.2\\n-\\nERFNet\\nTITS2018\\n0.5\\nno\\n70.0 68.0\\n41.7\\nFast-SCNN\\nBMVC2019\\n1.0\\nno\\n68.6 68.0 123.5\\nICNet\\nECCV2018\\n1.0\\nPSPNet50\\n-\\n69.5\\n30.3\\nDABNet\\nBMVC2019\\n1.0\\nno\\n-\\n70.1\\n27.7\\nDFANet B\\nCVPR2019 0.5∗Xception B\\n-\\n67.1\\n120\\nDFANet A′\\nCVPR2019\\n0.5 Xception A\\n-\\n70.3\\n160\\nDFANet A\\nCVPR2019 0.5∗Xception A\\n-\\n71.3\\n100\\nGUN\\nBMVC2018\\n0.5\\nDRN-D-22 69.6 70.4\\n33.3\\nSwiftNet\\nCVPR2019\\n1.0\\nResNet18\\n75.4 75.5\\n39.9\\nBiSeNetV1\\nECCV2018 0.75 Xception39 69.0 68.4 105.8\\nBiSeNetV1\\nECCV2018 0.75\\nResNet18\\n74.8 74.7\\n65.5\\nBiSeNetV2\\n—\\n0.5\\nno\\n73.4 72.6\\n156\\nBiSeNetV2-L\\n—\\n0.5\\nno\\n75.8 75.3\\n47.3\\nmodel in Table 5. Next, we replace the Semantic Branch\\nwith some other general light-weight models to explore\\nthe compatibility ability in Table 6.\\nGeneralization to large models.\\nAlthough our ar-\\nchitecture is designed mainly for the light-weight task,\\ne.g., real-time semantic segmentation, BiSeNet V2 can\\nbe also generalized to large models. We mainly enlarge\\nthe architecture from two aspects: (i) wider models,\\ncontrolled by the width multiplier α; (ii) deeper mod-\\nels, controlled by the depth multiplier d. Table 5 shows\\nthe segmentation accuracy and computational complex-\\nity of wider models with the diﬀerent width multiplier\\nTable 8 Comparison with state-of-the-art on CamVid.\\nWith 960 × 720 input, we evaluate the segmentation accuracy\\nand corresponding inference speed. Notation: backbone means\\nthe backbone models pre-trained on extra datasets, e.g., Im-\\nageNet dataset and Cityscapes dataset. ∗indicates the mod-\\nels are pre-trained on Cityscapes. † represents the models are\\ntrained from scratch.\\nmethod\\nref.\\nbackbone\\nmIoU(%)\\nFPS\\nlarge models\\nSegNet\\nTPAMI2017\\nVGG16\\n60.1\\n4.6\\nDPN\\nICCV2015\\nVGG16\\n60.1\\n1.2\\nDeeplab\\nICLR2015\\nVGG16\\n61.6\\n4.9\\nRTA\\nECCV2018\\nVGG16\\n62.5\\n0.2\\nDilation8\\nICLR2016\\nVGG16\\n65.3\\n4.4\\nPSPNet\\nCVPR2017\\nResNet50\\n69.1\\n5.4\\nDenseDecoder\\nCVPR2018 ResNeXt101\\n70.9\\n-\\nVideoGCRF∗\\nCVPR2018\\nResNet101\\n75.2\\n-\\nsmall models\\nENet\\narXiv2016\\nno\\n51.3\\n61.2\\nDFANet B\\nCVPR2019\\nXception B\\n59.3\\n160\\nDFANet A\\nCVPR2019\\nXception A\\n64.7\\n120\\nICNet\\nECCV2018\\nPSPNet50\\n67.1\\n27.8\\nSwiftNet\\nCVPR2019\\nResNet18†\\n63.33\\n-\\nSwiftNet\\nCVPR2019\\nResNet18\\n72.58\\n-\\nBiSeNetV1\\nECCV2018 Xception 39\\n65.6\\n175\\nBiSeNetV1\\nECCV2018\\nResNet18\\n68.7\\n116.25\\nBiSeNetV2\\n—\\nno\\n72.4\\n124.5\\nBiSeNetV2-L\\n—\\nno\\n73.2\\n32.7\\nBiSeNetV2∗\\n—\\nno\\n76.7\\n124.5\\nBiSeNetV2-L∗\\n—\\nno\\n78.5\\n32.7\\nα and diﬀerent depth multiplier d. According to the\\nexperiments, we choose α = 2.0 and d = 3.0 to build\\nour large architecture, termed BiSeNetV2-Large, which\\nachieves 75.8% mIoU and GFLOPs.\\nCompatibility with other models.\\nThe BiSeNetV2\\nis a generic architecture with two branches. In this\\nwork, we design some speciﬁc blocks for the Seman-\\ntic Branch. The Semantic Branch can be any light-\\nweight convolutional models (He et al., 2016; Howard\\net al., 2017). Therefore, to explore the compatibility\\nability of our architecture, we conduct a series of ex-\\nperiments with diﬀerent general light-weight models.\\nTable 6 shows the results of the combination with dif-\\nferent models.\\n5.3 Performance Evaluation\\nIn this section, we compare our best model (BiSeNetV2\\nand BiSeNetV2-Large) with other state-of-the-art meth-\\nods on three benchmark datasets: Cityscapes, CamVid\\nand COCO-Stuﬀ.\\nBiSeNet V2: Bilateral Network with Guided Aggregation for Real-time Semantic Segmentation\\n13\\nCityscapes.\\nWe present the segmentation accuracy\\nand inference speed of the proposed BiSeNetV2 on City-\\nscapes test set. We use the training set and validation\\nset with 2048×1024 input to train our models, which is\\nresized into 1024×512 resolution at ﬁrst in the models.\\nThen we evaluate the segmentation accuracy on the test\\nset. The measurement of inference time is conducted\\non one NVIDIA GeForce 1080Ti card. Table 7 reports\\nthe comparison results of our method and state-of-the-\\nart methods. The ﬁrst group is non-real-time methods,\\ncontaining CRF-RNN (Zheng et al., 2015), Deeplab-\\nCRF (Chen et al., 2015), FCN-8S (Long et al., 2015),\\nDilation10 (Yu and Koltun, 2016), LRR (Ghiasi and\\nFowlkes, 2016), Deeplabv2-CRF (Chen et al., 2016),\\nFRRN (Pohlen et al., 2017), ReﬁneNet (Lin et al., 2017),\\nDUC (Wang et al., 2018a), PSPNet (Zhao et al., 2017).\\nThe real-time semantic segmentation algorithms are li-\\nsted in the second group, including ENet (Paszke et al.,\\n2016), SQ (Treml et al., 2016), ESPNet (Mehta et al.,\\n2018), ESPNetV2 (Mehta et al., 2019), ERFNet (Romera\\net al., 2018), Fast-SCNN (Poudel et al., 2019), ICNet\\n(Zhao et al., 2018a), DABNet (Li et al., 2019a), DFANet\\n(Li et al., 2019b), GUN (Mazzini, 2018), SwiftNet (Or-\\nsic et al., 2019), BiSeNetV1 (Yu et al., 2018a). The\\nthird group is our methods with diﬀerent levels of com-\\nplexities. As shown in Table 7, our method achieves\\n72.6% mean IoU with 156 FPS and yields 75.3% mean\\nIoU with 47.3 FPS, which are state-of-the-art trade-\\noﬀbetween accuracy and speed. These results are even\\nbetter than several non-real-time algorithms in the sec-\\nond group of Table 7. We note that many non-real-time\\nmethods may adopt some evaluation tricks, e.g., multi-\\nscale testing and multi-crop evaluation, which can im-\\nprove the accuracy but are time-consuming. Therefore,\\nwe do not adopt this strategy with the consideration\\nof speed. To better view, we illustrate the trade-oﬀbe-\\ntween performance and speed in Figure 1. To highlight\\nthe eﬀectiveness of our method, we also present some\\nvisual examples of BiSeNetV2 on Cityscapes in Fig-\\nure 10.\\nCamVid.\\nTable 8 shows the statistic accuracy and\\nspeed results on the CamVid dataset. In the inference\\nphase, we use the training dataset and validation dataset\\nto train our model with 960 × 720 resolution input.\\nOur models are compared to some non-real-time algo-\\nrithms, including SegNet (Badrinarayanan et al., 2017),\\nDeeplab (Chen et al., 2015), RTA (Huang et al., 2018),\\nDilate8 (Yu and Koltun, 2016), PSPNet (Zhao et al.,\\n2017), VideoGCRF (Chandra et al., 2018), and DenseDe-\\ncoder (Bilinski and Prisacariu, 2018), and real-time al-\\ngorithms, containing ENet (Paszke et al., 2016), IC-\\nNet (Zhao et al., 2018a), DABNet (Li et al., 2019a),\\nDFANet (Li et al., 2019b), SwiftNet (Orsic et al., 2019),\\nTable 9 Comparison with state-of-the-art on COCO-\\nStuﬀ. Our models are trained and evaluated with the input of\\n640×640 resolution. Notation: backbone is the backbone models\\npre-trained on ImageNet dataset.\\nmethod\\nref.\\nbackbone mIoU(%) pixAcc(%) FPS\\nlarge models\\nFCN-16s\\nCVPR2017\\nVGG16\\n22.7\\n52.0\\n5.9\\nDeeplab\\nICLR2015\\nVGG16\\n26.9\\n57.8\\n8.1\\nFCN-8S\\nCVPR2015\\nVGG16\\n27.2\\n60.4\\n-\\nPSPNet50\\nCVPR2017 ResNet50\\n32.6\\n-\\n6.6\\nsmall models\\nICNet\\nECCV2018 PSPNet50\\n29.1\\n-\\n35.7\\nBiSeNetV2\\n—\\nno\\n25.2\\n60.5\\n87.9\\nBiSeNetV2-L\\n—\\nno\\n28.7\\n63.5\\n42.5\\nBiSeNetV1 (Yu et al., 2018a). BiSeNetV2 achieves much\\nfaster inference speed than other methods. Apart from\\nthe eﬃciency, our accuracy results also outperform these\\nwork. Besides, we investigate the eﬀect of the pre-training\\ndatasets on CamVid. The last two rows of Table 8 show\\nthat pre-training on Cityscapes can greatly improve the\\nmean IoU over 6% on the CamVid test set.\\nCOCO-Stuﬀ.\\nWe also report our accuracy and speed\\nresults on COCO-Stuﬀvalidation dataset in Table 9.\\nIn the inference phase, we pad the input into 640 × 640\\nresolution. For a fair comparison (Long et al., 2015;\\nChen et al., 2016; Zhao et al., 2017, 2018a), we do not\\nadopt any time-consuming testing tricks, such as multi-\\nscale and ﬂipping testing. Even With more complex\\ncategories in this dataset, compared to pioneer work,\\nour BiSeNetV2 still performs more eﬃcient and achieve\\ncomparable accuracy.\\n6 Concluding Remarks\\nWe observe that the semantic segmentation task re-\\nquires both low-level details and high-level semantics.\\nWe propose a new architecture to treat the spatial de-\\ntails and categorical semantics separately, termed Bilat-\\neral Segmentation Network (BiSeNetV2). The BiSeNetV2\\nframework is a generic architecture, which can be im-\\nplemented by most convolutional models. Our instanti-\\nations of BiSeNetV2 achieve a good trade-oﬀbetween\\nsegmentation accuracy and inference speed. We hope\\nthat this generic architecture BiSeNetV2 will foster fur-\\nther research in semantic segmentation.\\nAcknowledgment\\nThis work is supported by the National Natural Science\\nFoundation of China (No. 61433007 and 61876210).\\n14\\nChangqian Yu et al.\\n(a) Input\\n(b) BiSeNetV2\\n(c) BiSeNetV2-Large\\n(d) Groundtruth\\nFig. 10 Visualization examples on the Cityscapes val set produced from BiSeNetV2 and BiSeNetV2-Large. The\\nﬁrst row shows that our architecture can focus on the details, e.g., fence. The bus in the third row demonstrates the architecture\\ncan capture the large object. The bus in the last row illustrates the architecture can encode the spatial context to reason it.\\nReferences\\nAchanta R, Shaji A, Smith K, Lucchi A, Fua P, S¨usstrunk S\\n(2012) Slic superpixels compared to state-of-the-art super-\\npixel methods. IEEE Transactions on Pattern Analysis and\\nMachine Intelligence (TPAMI) 34(11):2274–2282 3\\nBadrinarayanan V, Kendall A, Cipolla R (2017) SegNet: A\\ndeep convolutional encoder-decoder architecture for image\\nsegmentation. IEEE Transactions on Pattern Analysis and\\nMachine Intelligence (TPAMI) 39(12):2481–2495 2, 4, 8, 13\\nVan den Bergh M, Boix X, Roig G, de Capitani B, Van Gool L\\n(2012) Seeds: Superpixels extracted via energy-driven sam-\\npling. In: Proc. European Conference on Computer Vision\\n(ECCV), pp 13–26 3\\nBilinski P, Prisacariu V (2018) Dense decoder shortcut connec-\\ntions for single-pass semantic segmentation. In: Proc. IEEE\\nConference on Computer Vision and Pattern Recognition\\n(CVPR), pp 6596–6605 13\\nBoykov YY, Jolly MP (2001) Interactive graph cuts for optimal\\nboundary & region segmentation of objects in nd images. In:\\nProc. IEEE International Conference on Computer Vision\\n(ICCV), vol 1, pp 105–112 3\\nBrostow GJ, Shotton J, Fauqueur J, Cipolla R (2008a) Seg-\\nmentation and recognition using structure from motion point\\nclouds. In: Proc. European Conference on Computer Vision\\n(ECCV), pp 44–57 2, 8\\nBrostow GJ, Shotton J, Fauqueur J, Cipolla R (2008b) Seg-\\nmentation and recognition using structure from motion point\\nclouds. In: Proc. European Conference on Computer Vision\\n(ECCV) 8\\nCaesar H, Uijlings J, Ferrari V (2018) Coco-stuﬀ: Thing and\\nstuﬀclasses in context. In: Proc. IEEE Conference on Com-\\nputer Vision and Pattern Recognition (CVPR) 2, 8\\nChandra S, Couprie C, Kokkinos I (2018) Deep spatio-temporal\\nrandom ﬁelds for eﬃcient video segmentation. In: Proc. IEEE\\nConference on Computer Vision and Pattern Recognition\\n(CVPR), pp 8915–8924 13\\nChen LC, Papandreou G, Kokkinos I, Murphy K, Yuille AL\\n(2015) Semantic image segmentation with deep convolutional\\nnets and fully connected crfs. In: Proc. International Confer-\\nence on Learning Representations (ICLR) 2, 3, 13\\nChen LC, Papandreou G, Kokkinos I, Murphy K, Yuille AL\\n(2016) Deeplab: Semantic image segmentation with deep\\nconvolutional nets, atrous convolution, and fully connected\\ncrfs. arXiv 13\\nChen LC, Papandreou G, SchroﬀF, Adam H (2017) Rethinking\\natrous convolution for semantic image segmentation. arXiv\\n2, 3, 6\\nChen LC, Zhu Y, Papandreou G, SchroﬀF, Adam H (2018)\\nEncoder-decoder with atrous separable convolution for se-\\nmantic image segmentation. In: Proc. European Conference\\non Computer Vision (ECCV), pp 801–818 2, 3\\nChetlur S, Woolley C, Vandermersch P, Cohen J, Tran J,\\nCatanzaro B, Shelhamer E (2014) cudnn: Eﬃcient primitives\\nBiSeNet V2: Bilateral Network with Guided Aggregation for Real-time Semantic Segmentation\\n15\\nfor deep learning. arXiv 7\\nChollet F (2017) Xception: Deep learning with depthwise sepa-\\nrable convolutions. In: Proc. IEEE Conference on Computer\\nVision and Pattern Recognition (CVPR) 2, 4, 6\\nCordts M, Omran M, Ramos S, Rehfeld T, Enzweiler M, Be-\\nnenson R, Franke U, Roth S, Schiele B (2016) The cityscapes\\ndataset for semantic urban scene understanding. In: Proc.\\nIEEE Conference on Computer Vision and Pattern Recogni-\\ntion (CVPR) 1, 2, 8, 9\\nFu J, Liu J, Tian H, Fang Z, Lu H (2019) Dual attention net-\\nwork for scene segmentation. In: Proc. IEEE Conference on\\nComputer Vision and Pattern Recognition (CVPR) 2, 3\\nGeiger A, Lenz P, Urtasun R (2012) Are we ready for au-\\ntonomous driving? the kitti vision benchmark suite. In: Proc.\\nIEEE Conference on Computer Vision and Pattern Recogni-\\ntion (CVPR), pp 3354–3361 1\\nGhiasi G, Fowlkes CC (2016) Laplacian pyramid reconstruction\\nand reﬁnement for semantic segmentation. In: Proc. Euro-\\npean Conference on Computer Vision (ECCV) 4, 13\\nGlorot X, Bordes A, Bengio Y (2011) Deep sparse rectiﬁer neu-\\nral networks. In: Proc. International conference on artiﬁcial\\nintelligence and statistics, pp 315–323 6\\nHariharan B, Arbel´aez P, Girshick R, Malik J (2015) Hyper-\\ncolumns for object segmentation and ﬁne-grained localiza-\\ntion. In: Proc. IEEE Conference on Computer Vision and\\nPattern Recognition (CVPR), pp 447–456 4\\nHe K, Zhang X, Ren S, Sun J (2015) Delving deep into rectiﬁers:\\nSurpassing human-level performance on imagenet classiﬁca-\\ntion. In: Proc. IEEE International Conference on Computer\\nVision (ICCV), pp 1026–1034 8\\nHe K, Zhang X, Ren S, Sun J (2016) Deep residual learning for\\nimage recognition. In: Proc. IEEE Conference on Computer\\nVision and Pattern Recognition (CVPR) 6, 12\\nHoward A, Sandler M, Chu G, Chen LC, Chen B, Tan M, Wang\\nW, Zhu Y, Pang R, Vasudevan V, et al. (2019) Searching\\nfor mobilenetv3. In: Proc. IEEE International Conference on\\nComputer Vision (ICCV) 6, 7, 10, 11\\nHoward AG, Zhu M, Chen B, Kalenichenko D, Wang W,\\nWeyand T, Andreetto M, Adam H (2017) Mobilenets: Ef-\\nﬁcient convolutional neural networks for mobile vision appli-\\ncations. arXiv 4, 6, 12\\nHuang G, Liu Z, van der Maaten L, Weinberger KQ (2017)\\nDensely connected convolutional networks. In: Proc. IEEE\\nConference on Computer Vision and Pattern Recognition\\n(CVPR), pp 2261–2269 6\\nHuang PY, Hsu WT, Chiu CY, Wu TF, Sun M (2018) Ef-\\nﬁcient uncertainty estimation for semantic segmentation in\\nvideos. In: Proc. European Conference on Computer Vision\\n(ECCV), pp 520–535 13\\nIandola\\nFN,\\nMoskewicz\\nMW,\\nAshraf\\nK,\\nHan\\nS,\\nDally\\nWJ, Keutzer K (2016) Squeezenet: Alexnet-level accuracy\\nwith 50x fewer parameters and ¡1mb model size. arXiv\\nabs/1602.07360 4, 6\\nIoﬀe S, Szegedy C (2015) Batch normalization: Accelerating\\ndeep network training by reducing internal covariate shift.\\nIn: Proc. International Conference on Machine Learning\\n(ICML), pp 448–456 6\\nKrizhevsky A, Sutskever I, Hinton GE (2012) Imagenet classi-\\nﬁcation with deep convolutional neural networks. In: Proc.\\nNeural Information Processing Systems (NeurIPS) 2\\nLi G, Yun I, Kim J, Kim J (2019a) Dabnet: Depth-wise asym-\\nmetric bottleneck for real-time semantic segmentation. In:\\nProc. British Machine Vision Conference (BMVC) 13\\nLi H, Xiong P, Fan H, Sun J (2019b) Dfanet: Deep feature\\naggregation for real-time semantic segmentation. In: Proc.\\nIEEE Conference on Computer Vision and Pattern Recogni-\\ntion (CVPR) 4, 13\\nLi X, Liu Z, Luo P, Loy CC, Tang X (2017) Not all pixels are\\nequal: diﬃculty-aware semantic segmentation via deep layer\\ncascade. In: Proc. IEEE Conference on Computer Vision and\\nPattern Recognition (CVPR) 4\\nLin G, Milan A, Shen C, Reid I (2017) Reﬁnenet: Multi-path re-\\nﬁnement networks with identity mappings for high-resolution\\nsemantic segmentation. In: Proc. IEEE Conference on Com-\\nputer Vision and Pattern Recognition (CVPR) 2, 4, 13\\nLin TY, Maire M, Belongie S, Hays J, Perona P, Ramanan D,\\nDoll´ar P, Zitnick CL (2014) Microsoft coco: Common objects\\nin context. In: Proc. European Conference on Computer Vi-\\nsion (ECCV) 8\\nLiu W, Rabinovich A, Berg AC (2016) Parsenet: Looking wider\\nto see better. arXiv 5, 6\\nLong J, Shelhamer E, Darrell T (2015) Fully convolutional net-\\nworks for semantic segmentation. In: Proc. IEEE Conference\\non Computer Vision and Pattern Recognition (CVPR) 2, 3,\\n13\\nMa N, Zhang X, Zheng HT, Sun J (2018) Shuﬄenet v2: Practi-\\ncal guidelines for eﬃcient cnn architecture design. In: Proc.\\nEuropean Conference on Computer Vision (ECCV), pp 116–\\n131 2, 4, 6, 7\\nMazzini D (2018) Guided upsampling network for real-time se-\\nmantic segmentation. In: Proc. British Machine Vision Con-\\nference (BMVC) 2, 4, 13\\nMehta S, Rastegari M, Caspi A, Shapiro L, Hajishirzi H (2018)\\nEspnet: Eﬃcient spatial pyramid of dilated convolutions for\\nsemantic segmentation. In: Proc. European Conference on\\nComputer Vision (ECCV), pp 552–568 4, 13\\nMehta S, Rastegari M, Shapiro LG, Hajishirzi H (2019) Esp-\\nnetv2: A light-weight, power eﬃcient, and general purpose\\nconvolutional neural network. In: Proc. IEEE Conference on\\nComputer Vision and Pattern Recognition (CVPR) 4, 13\\nOrsic M, Kreso I, Bevandic P, Segvic S (2019) In defense of pre-\\ntrained imagenet architectures for real-time semantic seg-\\nmentation of road-driving images. In: Proc. IEEE Confer-\\nence on Computer Vision and Pattern Recognition (CVPR),\\npp 12607–12616 13\\nOtsu N (1979) A threshold selection method from gray-level\\nhistograms. IEEE transactions on systems, man, and cyber-\\nnetics 9(1):62–66 3\\nPaszke A, Chaurasia A, Kim S, Culurciello E (2016) Enet: A\\ndeep neural network architecture for real-time semantic seg-\\nmentation. arXiv 2, 4, 13\\nPeng C, Zhang X, Yu G, Luo G, Sun J (2017) Large kernel\\nmatters–improve semantic segmentation by global convolu-\\ntional network. In: Proc. IEEE Conference on Computer Vi-\\nsion and Pattern Recognition (CVPR) 2, 4\\nPohlen T, Hermans A, Mathias M, Leibe B (2017) Full-\\nresolution residual networks for semantic segmentation in\\nstreet scenes. In: Proc. IEEE Conference on Computer Vision\\nand Pattern Recognition (CVPR), pp 4151–4160 13\\nPoudel RP, Liwicki S, Cipolla R (2019) Fast-scnn: fast semantic\\nsegmentation network. arXiv 13\\nRen X, Malik J (2003) Learning a classiﬁcation model for seg-\\nmentation. In: Proc. IEEE International Conference on Com-\\nputer Vision (ICCV), p 10 3\\nRomera E, Alvarez JM, Bergasa LM, Arroyo R (2018) Erfnet:\\nEﬃcient residual factorized convnet for real-time semantic\\nsegmentation. IEEE Transactions on Intelligent Transporta-\\ntion Systems 19(1):263–272 2, 4, 13\\nRonneberger O, Fischer P, Brox T (2015) U-net: Convolutional\\nnetworks for biomedical image segmentation. In: Proc. In-\\nternational Conference on Medical Image Computing and\\nComputer-Assisted Intervention (MICCAI) 4\\n16\\nChangqian Yu et al.\\nRother C, Kolmogorov V, Blake A (2004) Grabcut: Interactive\\nforeground extraction using iterated graph cuts. In: ACM\\nTransactions on Graphics, vol 23, pp 309–314 3\\nSandler M, Howard A, Zhu M, Zhmoginov A, Chen LC (2018)\\nMobilenetv2: Inverted residuals and linear bottlenecks. In:\\nProc. IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR), pp 4510–4520 4, 6, 7, 10, 11\\nSelvaraju RR, Cogswell M, Das A, Vedantam R, Parikh D, Ba-\\ntra D (2017) Grad-cam: Visual explanations from deep net-\\nworks via gradient-based localization. In: Proc. IEEE Inter-\\nnational Conference on Computer Vision (ICCV), pp 618–\\n626 10\\nShelhamer E, Long J, Darrell T (2017) Fully convolutional net-\\nworks for semantic segmentation. IEEE Transactions on Pat-\\ntern Analysis and Machine Intelligence (TPAMI) (4):640–651\\n3\\nShen Z, Liu Z, Li J, Jiang YG, Chen Y, Xue X (2017) Dsod:\\nLearning deeply supervised object detectors from scratch. In:\\nProc. IEEE International Conference on Computer Vision\\n(ICCV) 6\\nSimonyan K, Zisserman A (2015) Very deep convolutional net-\\nworks for large-scale image recognition. In: Proc. Interna-\\ntional Conference on Learning Representations (ICLR) 6\\nSturgess P, Alahari K, Ladicky L, Torr PHS (2009) Combining\\nAppearance and Structure from Motion Features for Road\\nScene Understanding. In: Proc. British Machine Vision Con-\\nference (BMVC) 8\\nSzegedy C, Ioﬀe S, Vanhoucke V, Alemi AA (2017) Inception-\\nv4, inception-resnet and the impact of residual connections\\non learning. In: Proc. Thirty-ﬁrst AAAI conference on arti-\\nﬁcial intelligence 6\\nTan M, Chen B, Pang R, Vasudevan V, Sandler M, Howard A,\\nLe QV (2019) Mnasnet: Platform-aware neural architecture\\nsearch for mobile. In: Proc. IEEE Conference on Computer\\nVision and Pattern Recognition (CVPR), pp 2820–2828 7\\nTreml M, Arjona-Medina J, Unterthiner T, Durgesh R, Fried-\\nmann F, Schuberth P, Mayr A, Heusel M, Hofmarcher M,\\nWidrich M, et al. (2016) Speeding up semantic segmenta-\\ntion for autonomous driving. In: Proc. Neural Information\\nProcessing Systems Workshops 13\\nVincent L, Soille P (1991) Watersheds in digital spaces: an\\neﬃcient algorithm based on immersion simulations. IEEE\\nTransactions on Pattern Analysis and Machine Intelligence\\n(TPAMI) (6):583–598 3\\nWang J, Sun K, Cheng T, Jiang B, Deng C, Zhao Y, Liu\\nD, Mu Y, Tan M, Wang X, Liu W, Xiao B (2019) Deep\\nhigh-resolution representation learning for visual recognition.\\nIEEE Transactions on Pattern Analysis and Machine Intel-\\nligence (TPAMI) 4\\nWang P, Chen P, Yuan Y, Liu D, Huang Z, Hou X, Cottrell G\\n(2018a) Understanding convolution for semantic segmenta-\\ntion. In: Proc. IEEE Winter Conference on Applications of\\nComputer Vision (WACV) 3, 13\\nWang RJ, Li X, Ling CX (2018b) Pelee: A real-time object\\ndetection system on mobile devices. In: Proc. Neural Infor-\\nmation Processing Systems (NeurIPS) 6\\nWu Z, Shen C, Hengel Avd (2016) High-performance semantic\\nsegmentation using very deep fully convolutional networks.\\narXiv 11\\nYu C, Wang J, Peng C, Gao C, Yu G, Sang N (2018a) Bisenet:\\nBilateral segmentation network for real-time semantic seg-\\nmentation. In: Proc. European Conference on Computer Vi-\\nsion (ECCV), pp 325–341 3, 13\\nYu C, Wang J, Peng C, Gao C, Yu G, Sang N (2018b) Learning\\na discriminative feature network for semantic segmentation.\\nIn: Proc. IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR) 2, 4, 6\\nYu C, Wang J, Gao C, Yu G, Shen C, Sang N (2020) Context\\nprior for scene segmentation. In: Proc. IEEE Conference on\\nComputer Vision and Pattern Recognition (CVPR) 2, 3\\nYu F, Koltun V (2016) Multi-scale context aggregation by di-\\nlated convolutions. In: Proc. International Conference on\\nLearning Representations (ICLR) 13\\nYuan Y, Wang J (2018) Ocnet: Object context network for scene\\nparsing. arXiv 3\\nZhang H, Dana K, Shi J, Zhang Z, Wang X, Tyagi A, Agrawal\\nA (2018a) Context encoding for semantic segmentation. In:\\nProc. IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR), pp 7151–7160 2, 3\\nZhang X, Zhou X, Lin M, Sun J (2018b) Shuﬄenet: An ex-\\ntremely eﬃcient convolutional neural network for mobile de-\\nvices. In: Proc. IEEE Conference on Computer Vision and\\nPattern Recognition (CVPR), pp 6848–6856 4, 6\\nZhao H, Shi J, Qi X, Wang X, Jia J (2017) Pyramid scene\\nparsing network. In: Proc. IEEE Conference on Computer\\nVision and Pattern Recognition (CVPR) 2, 3, 6, 13\\nZhao H, Qi X, Shen X, Shi J, Jia J (2018a) Icnet for real-time\\nsemantic segmentation on high-resolution images. In: Proc.\\nEuropean Conference on Computer Vision (ECCV), pp 405–\\n420 2, 4, 13\\nZhao H, Zhang Y, Liu S, Shi J, Loy CC, Lin D, Jia J (2018b)\\nPSANet: Point-wise spatial attention network for scene pars-\\ning. In: Proc. European Conference on Computer Vision\\n(ECCV) 2, 3\\nZheng S, Jayasumana S, Romera-Paredes B, Vineet V, Su Z,\\nDu D, Huang C, Torr PH (2015) Conditional random ﬁelds\\nas recurrent neural networks. In: Proc. IEEE International\\nConference on Computer Vision (ICCV) 13\\nZhou B, Zhao H, Puig X, Xiao T, Fidler S, Barriuso A, Tor-\\nralba A (2019) Semantic understanding of scenes through\\nthe ade20k dataset. International Journal of Computer Vi-\\nsion (IJCV) 127(3):302–321 1\\n')])\n"
     ]
    }
   ],
   "source": [
    "for topic in research_topics:\n",
    "    print(topic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = research_assistant._check_relevance(papers, research_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[0, 3, 4, 13, 44, 21]',\n",
       " '[11, 13, 42, 12, 14, 10]',\n",
       " '[21, 3, 0, 44, 20, 22]',\n",
       " '[30, 31, 32, 33, 34, 41]',\n",
       " '[42, 41, 19, 39, 24, 30]']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResearchTopics(topic='Deep learning architectures for semantic segmentation using fully convolutional networks without attention mechanisms', priority=1, query='fcn semantic segmentation', timestamp='2024-11-23 17:10:29', research_papers=None)\n",
      "Image Segmentation Keras : Implementation of Segnet, FCN, UNet, PSPNet and other models in Keras\n",
      "Graph-FCN for image semantic segmentation\n",
      "CNN-based Semantic Segmentation using Level Set Loss\n",
      "Object Boundary Guided Semantic Segmentation\n",
      "Semantic Segmentation with Boundary Neural Fields\n",
      "A Brief Survey and an Application of Semantic Image Segmentation for Autonomous Driving\n"
     ]
    }
   ],
   "source": [
    "print(research_topics[1])\n",
    "for num in json.loads(responses[1]):\n",
    "    print(papers[int(num)].title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "condensed_papers = [papers[i] for i in json.loads(responses[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[arxiv.Result(entry_id='http://arxiv.org/abs/2307.13215v1', updated=datetime.datetime(2023, 7, 25, 2, 56, 20, tzinfo=datetime.timezone.utc), published=datetime.datetime(2023, 7, 25, 2, 56, 20, tzinfo=datetime.timezone.utc), title='Image Segmentation Keras : Implementation of Segnet, FCN, UNet, PSPNet and other models in Keras', authors=[arxiv.Result.Author('Divam Gupta')], summary='Semantic segmentation plays a vital role in computer vision tasks, enabling\\nprecise pixel-level understanding of images. In this paper, we present a\\ncomprehensive library for semantic segmentation, which contains implementations\\nof popular segmentation models like SegNet, FCN, UNet, and PSPNet. We also\\nevaluate and compare these models on several datasets, offering researchers and\\npractitioners a powerful toolset for tackling diverse segmentation challenges.', comment=None, journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/2307.13215v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2307.13215v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2001.00335v1', updated=datetime.datetime(2020, 1, 2, 6, 5, 29, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 1, 2, 6, 5, 29, tzinfo=datetime.timezone.utc), title='Graph-FCN for image semantic segmentation', authors=[arxiv.Result.Author('Yi Lu'), arxiv.Result.Author('Yaran Chen'), arxiv.Result.Author('Dongbin Zhao'), arxiv.Result.Author('Jianxin Chen')], summary='Semantic segmentation with deep learning has achieved great progress in\\nclassifying the pixels in the image. However, the local location information is\\nusually ignored in the high-level feature extraction by the deep learning,\\nwhich is important for image semantic segmentation. To avoid this problem, we\\npropose a graph model initialized by a fully convolutional network (FCN) named\\nGraph-FCN for image semantic segmentation. Firstly, the image grid data is\\nextended to graph structure data by a convolutional network, which transforms\\nthe semantic segmentation problem into a graph node classification problem.\\nThen we apply graph convolutional network to solve this graph node\\nclassification problem. As far as we know, it is the first time that we apply\\nthe graph convolutional network in image semantic segmentation. Our method\\nachieves competitive performance in mean intersection over union (mIOU) on the\\nVOC dataset(about 1.34% improvement), compared to the original FCN model.', comment=None, journal_ref='Advances in Neural Networks, ISNN 2019. Lecture Notes in Computer\\n  Science, vol 11554, pp. 97-105, Springer, Cham', doi=None, primary_category='cs.CV', categories=['cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/2001.00335v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00335v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1910.00950v1', updated=datetime.datetime(2019, 10, 2, 13, 45, 33, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 10, 2, 13, 45, 33, tzinfo=datetime.timezone.utc), title='CNN-based Semantic Segmentation using Level Set Loss', authors=[arxiv.Result.Author('Youngeun Kim'), arxiv.Result.Author('Seunghyeon Kim'), arxiv.Result.Author('Taekyung Kim'), arxiv.Result.Author('Changick Kim')], summary='Thesedays, Convolutional Neural Networks are widely used in semantic\\nsegmentation. However, since CNN-based segmentation networks produce\\nlow-resolution outputs with rich semantic information, it is inevitable that\\nspatial details (e.g., small bjects and fine boundary information) of\\nsegmentation results will be lost. To address this problem, motivated by a\\nvariational approach to image segmentation (i.e., level set theory), we propose\\na novel loss function called the level set loss which is designed to refine\\nspatial details of segmentation results. To deal with multiple classes in an\\nimage, we first decompose the ground truth into binary images. Note that each\\nbinary image consists of background and regions belonging to a class. Then we\\nconvert level set functions into class probability maps and calculate the\\nenergy for each class. The network is trained to minimize the weighted sum of\\nthe level set loss and the cross-entropy loss. The proposed level set loss\\nimproves the spatial details of segmentation results in a time and memory\\nefficient way. Furthermore, our experimental results show that the proposed\\nloss function achieves better performance than previous approaches.', comment='2019 IEEE Winter Conference on Applications of Computer Vision\\n  (WACV). IEEE, 2019', journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/1910.00950v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1910.00950v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1603.09742v4', updated=datetime.datetime(2016, 7, 6, 23, 51, 40, tzinfo=datetime.timezone.utc), published=datetime.datetime(2016, 3, 31, 19, 51, 5, tzinfo=datetime.timezone.utc), title='Object Boundary Guided Semantic Segmentation', authors=[arxiv.Result.Author('Qin Huang'), arxiv.Result.Author('Chunyang Xia'), arxiv.Result.Author('Wenchao Zheng'), arxiv.Result.Author('Yuhang Song'), arxiv.Result.Author('Hao Xu'), arxiv.Result.Author('C. -C. Jay Kuo')], summary='Semantic segmentation is critical to image content understanding and object\\nlocalization. Recent development in fully-convolutional neural network (FCN)\\nhas enabled accurate pixel-level labeling. One issue in previous works is that\\nthe FCN based method does not exploit the object boundary information to\\ndelineate segmentation details since the object boundary label is ignored in\\nthe network training. To tackle this problem, we introduce a double branch\\nfully convolutional neural network, which separates the learning of the\\ndesirable semantic class labeling with mask-level object proposals guided by\\nrelabeled boundaries. This network, called object boundary guided FCN\\n(OBG-FCN), is able to integrate the distinct properties of object shape and\\nclass features elegantly in a fully convolutional way with a designed masking\\narchitecture. We conduct experiments on the PASCAL VOC segmentation benchmark,\\nand show that the end-to-end trainable OBG-FCN system offers great improvement\\nin optimizing the target semantic segmentation quality.', comment='The results in the first version of this paper are mistaken due to\\n  overlapping validation data and incorrect benchmark methods', journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/1603.09742v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1603.09742v4', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1511.02674v2', updated=datetime.datetime(2016, 5, 24, 23, 32, 10, tzinfo=datetime.timezone.utc), published=datetime.datetime(2015, 11, 9, 13, 27, 30, tzinfo=datetime.timezone.utc), title='Semantic Segmentation with Boundary Neural Fields', authors=[arxiv.Result.Author('Gedas Bertasius'), arxiv.Result.Author('Jianbo Shi'), arxiv.Result.Author('Lorenzo Torresani')], summary='The state-of-the-art in semantic segmentation is currently represented by\\nfully convolutional networks (FCNs). However, FCNs use large receptive fields\\nand many pooling layers, both of which cause blurring and low spatial\\nresolution in the deep layers. As a result FCNs tend to produce segmentations\\nthat are poorly localized around object boundaries. Prior work has attempted to\\naddress this issue in post-processing steps, for example using a color-based\\nCRF on top of the FCN predictions. However, these approaches require additional\\nparameters and low-level features that are difficult to tune and integrate into\\nthe original network architecture. Additionally, most CRFs use color-based\\npixel affinities, which are not well suited for semantic segmentation and lead\\nto spatially disjoint predictions.\\n  To overcome these problems, we introduce a Boundary Neural Field (BNF), which\\nis a global energy model integrating FCN predictions with boundary cues. The\\nboundary information is used to enhance semantic segment coherence and to\\nimprove object localization. Specifically, we first show that the convolutional\\nfilters of semantic FCNs provide good features for boundary detection. We then\\nemploy the predicted boundaries to define pairwise potentials in our energy.\\nFinally, we show that our energy decomposes semantic segmentation into multiple\\nbinary problems, which can be relaxed for efficient global optimization. We\\nreport extensive experiments demonstrating that minimization of our global\\nboundary-based energy yields results superior to prior globalization methods,\\nboth quantitatively as well as qualitatively.', comment=None, journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/1511.02674v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1511.02674v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1808.08413v1', updated=datetime.datetime(2018, 8, 25, 11, 56, 45, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 8, 25, 11, 56, 45, tzinfo=datetime.timezone.utc), title='A Brief Survey and an Application of Semantic Image Segmentation for Autonomous Driving', authors=[arxiv.Result.Author('Çağrı Kaymak'), arxiv.Result.Author('Ayşegül Uçar')], summary='Deep learning is a fast-growing machine learning approach to perceive and\\nunderstand large amounts of data. In this paper, general information about the\\ndeep learning approach which is attracted much attention in the field of\\nmachine learning is given in recent years and an application about semantic\\nimage segmentation is carried out in order to help autonomous driving of\\nautonomous vehicles. This application is implemented with Fully Convolutional\\nNetwork (FCN) architectures obtained by modifying the Convolutional Neural\\nNetwork (CNN) architectures based on deep learning. Experimental studies for\\nthe application are utilized 4 different FCN architectures named\\nFCN-AlexNet,FCN-8s, FCN-16s and FCN-32s. For the experimental studies, FCNs are\\nfirst trained separately and validation accuracies of these trained network\\nmodels on the used dataset is compared. In addition, image segmentation\\ninferences are visualized to take account of how precisely FCN architectures\\ncan segment objects.', comment='A chapter for Springer Book: Handbook of Deep Learning Applications,\\n  2018,[ Pijush Samui, Editor]. (be published)', journal_ref=None, doi=None, primary_category='eess.IV', categories=['eess.IV'], links=[arxiv.Result.Link('http://arxiv.org/abs/1808.08413v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1808.08413v1', title='pdf', rel='related', content_type=None)])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condensed_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "futures",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
