{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicko\\anaconda3\\envs\\futures\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from manager import ResearchManager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm_name = \"CLAUDE\"\n",
    "\n",
    "research_assistant = ResearchManager(llm_name)\n",
    "\n",
    "research_topics = research_assistant.analyze_research(\"What are some new and novel ways to segment images that don't use tranformer architecture?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ResearchTopics(topic='Graph based image segmentation methods using superpixels and region growing algorithms', priority=3, query='superpixel graph segmentation', timestamp='2024-11-23 17:10:29', research_papers=None), ResearchTopics(topic='Deep learning architectures for semantic segmentation using fully convolutional networks without attention mechanisms', priority=1, query='fcn semantic segmentation', timestamp='2024-11-23 17:10:29', research_papers=None), ResearchTopics(topic='Unsupervised image segmentation using clustering algorithms and traditional computer vision techniques', priority=4, query='unsupervised image clustering', timestamp='2024-11-23 17:10:29', research_papers=None), ResearchTopics(topic='Instance segmentation using mask RCNN variants and region proposal networks', priority=2, query='mask rcnn instance segmentation', timestamp='2024-11-23 17:10:29', research_papers=None), ResearchTopics(topic='Active contour and level set methods for medical image segmentation', priority=5, query='level set segmentation', timestamp='2024-11-23 17:10:29', research_papers=None)]\n",
      "[arxiv.Result(entry_id='http://arxiv.org/abs/2106.03755v2', updated=datetime.datetime(2021, 11, 18, 18, 13, 45, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 6, 7, 16, 20, 4, tzinfo=datetime.timezone.utc), title='HERS Superpixels: Deep Affinity Learning for Hierarchical Entropy Rate Segmentation', authors=[arxiv.Result.Author('Hankui Peng'), arxiv.Result.Author('Angelica I. Aviles-Rivero'), arxiv.Result.Author('Carola-Bibiane Sch√∂nlieb')], summary='Superpixels serve as a powerful preprocessing tool in numerous computer\\nvision tasks. By using superpixel representation, the number of image\\nprimitives can be largely reduced by orders of magnitudes. With the rise of\\ndeep learning in recent years, a few works have attempted to feed deeply\\nlearned features / graphs into existing classical superpixel techniques.\\nHowever, none of them are able to produce superpixels in near real-time, which\\nis crucial to the applicability of superpixels in practice. In this work, we\\npropose a two-stage graph-based framework for superpixel segmentation. In the\\nfirst stage, we introduce an efficient Deep Affinity Learning (DAL) network\\nthat learns pairwise pixel affinities by aggregating multi-scale information.\\nIn the second stage, we propose a highly efficient superpixel method called\\nHierarchical Entropy Rate Segmentation (HERS). Using the learned affinities\\nfrom the first stage, HERS builds a hierarchical tree structure that can\\nproduce any number of highly adaptive superpixels instantaneously. We\\ndemonstrate, through visual and numerical experiments, the effectiveness and\\nefficiency of our method compared to various state-of-the-art superpixel\\nmethods.', comment=None, journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV', 'stat.AP', 'stat.ML', 'I.4; I.5'], links=[arxiv.Result.Link('http://arxiv.org/abs/2106.03755v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2106.03755v2', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2103.03435v2', updated=datetime.datetime(2023, 5, 8, 7, 40, 3, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 3, 5, 2, 20, 26, tzinfo=datetime.timezone.utc), title='Implicit Integration of Superpixel Segmentation into Fully Convolutional Networks', authors=[arxiv.Result.Author('Teppei Suzuki')], summary='Superpixels are a useful representation to reduce the complexity of image\\ndata. However, to combine superpixels with convolutional neural networks (CNNs)\\nin an end-to-end fashion, one requires extra models to generate superpixels and\\nspecial operations such as graph convolution. In this paper, we propose a way\\nto implicitly integrate a superpixel scheme into CNNs, which makes it easy to\\nuse superpixels with CNNs in an end-to-end fashion. Our proposed method\\nhierarchically groups pixels at downsampling layers and generates superpixels.\\nOur method can be plugged into many existing architectures without a change in\\ntheir feed-forward path because our method does not use superpixels in the\\nfeed-forward path but use them to recover the lost resolution instead of\\nbilinear upsampling. As a result, our method preserves detailed information\\nsuch as object boundaries in the form of superpixels even when the model\\ncontains downsampling layers. We evaluate our method on several tasks such as\\nsemantic segmentation, superpixel segmentation, and monocular depth estimation,\\nand confirm that it speeds up modern architectures and/or improves their\\nprediction accuracy in these tasks.', comment='code: https://github.com/DensoITLab/HCFormer', journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/2103.03435v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2103.03435v2', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/1605.03718v2', updated=datetime.datetime(2016, 11, 23, 10, 25, 47, tzinfo=datetime.timezone.utc), published=datetime.datetime(2016, 5, 12, 8, 14, tzinfo=datetime.timezone.utc), title='Improved Image Boundaries for Better Video Segmentation', authors=[arxiv.Result.Author('Anna Khoreva'), arxiv.Result.Author('Rodrigo Benenson'), arxiv.Result.Author('Fabio Galasso'), arxiv.Result.Author('Matthias Hein'), arxiv.Result.Author('Bernt Schiele')], summary='Graph-based video segmentation methods rely on superpixels as starting point.\\nWhile most previous work has focused on the construction of the graph edges and\\nweights as well as solving the graph partitioning problem, this paper focuses\\non better superpixels for video segmentation. We demonstrate by a comparative\\nanalysis that superpixels extracted from boundaries perform best, and show that\\nboundary estimation can be significantly improved via image and time domain\\ncues. With superpixels generated from our better boundaries we observe\\nconsistent improvement for two video segmentation methods in two different\\ndatasets.', comment=None, journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/1605.03718v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1605.03718v2', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2007.04257v1', updated=datetime.datetime(2020, 7, 8, 16, 46, 58, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 7, 8, 16, 46, 58, tzinfo=datetime.timezone.utc), title='Superpixel Segmentation using Dynamic and Iterative Spanning Forest', authors=[arxiv.Result.Author('F. C. Belem'), arxiv.Result.Author('S. J. F. Guimaraes'), arxiv.Result.Author('A. X. Falcao')], summary='As constituent parts of image objects, superpixels can improve several\\nhigher-level operations. However, image segmentation methods might have their\\naccuracy seriously compromised for reduced numbers of superpixels. We have\\ninvestigated a solution based on the Iterative Spanning Forest (ISF) framework.\\nIn this work, we present Dynamic ISF (DISF) -- a method based on the following\\nsteps. (a) It starts from an image graph and a seed set with considerably more\\npixels than the desired number of superpixels. (b) The seeds compete among\\nthemselves, and each seed conquers its most closely connected pixels, resulting\\nin an image partition (spanning forest) with connected superpixels. In step\\n(c), DISF assigns relevance values to seeds based on superpixel analysis and\\nremoves the most irrelevant ones. Steps (b) and (c) are repeated until the\\ndesired number of superpixels is reached. DISF has the chance to reconstruct\\nrelevant edges after each iteration, when compared to region merging\\nalgorithms. As compared to other seed-based superpixel methods, DISF is more\\nlikely to find relevant seeds. It also introduces dynamic arc-weight estimation\\nin the ISF framework for more effective superpixel delineation, and we\\ndemonstrate all results on three datasets with distinct object properties.', comment=None, journal_ref=None, doi='10.1109/LSP.2020.3015433', primary_category='cs.CV', categories=['cs.CV'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/LSP.2020.3015433', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2007.04257v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2007.04257v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2007.14033v2', updated=datetime.datetime(2020, 9, 12, 13, 12, 49, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 7, 28, 7, 30, 50, tzinfo=datetime.timezone.utc), title='Superpixel Based Graph Laplacian Regularization for Sparse Hyperspectral Unmixing', authors=[arxiv.Result.Author('Taner Ince')], summary='An efficient spatial regularization method using superpixel segmentation and\\ngraph Laplacian regularization is proposed for sparse hyperspectral unmixing\\nmethod. Since it is likely to find spectrally similar pixels in a homogeneous\\nregion, we use a superpixel segmentation algorithm to extract the homogeneous\\nregions by considering the image boundaries. We first extract the homogeneous\\nregions, which are called superpixels, then a weighted graph in each superpixel\\nis constructed by selecting $K$-nearest pixels in each superpixel. Each node in\\nthe graph represents the spectrum of a pixel and edges connect the similar\\npixels inside the superpixel. The spatial similarity is investigated using\\ngraph Laplacian regularization. Sparsity regularization for abundance matrix is\\nprovided using a weighted sparsity promoting norm. Experimental results on\\nsimulated and real data sets show the superiority of the proposed algorithm\\nover the well-known algorithms in the literature.', comment='5 pages', journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV', 'eess.IV'], links=[arxiv.Result.Link('http://arxiv.org/abs/2007.14033v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2007.14033v2', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2411.14429v1', updated=datetime.datetime(2024, 11, 21, 18, 59, 8, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 21, 18, 59, 8, tzinfo=datetime.timezone.utc), title='Revisiting the Integration of Convolution and Attention for Vision Backbone', authors=[arxiv.Result.Author('Lei Zhu'), arxiv.Result.Author('Xinjiang Wang'), arxiv.Result.Author('Wayne Zhang'), arxiv.Result.Author('Rynson W. H. Lau')], summary='Convolutions (Convs) and multi-head self-attentions (MHSAs) are typically\\nconsidered alternatives to each other for building vision backbones. Although\\nsome works try to integrate both, they apply the two operators simultaneously\\nat the finest pixel granularity. With Convs responsible for per-pixel feature\\nextraction already, the question is whether we still need to include the heavy\\nMHSAs at such a fine-grained level. In fact, this is the root cause of the\\nscalability issue w.r.t. the input resolution for vision transformers. To\\naddress this important problem, we propose in this work to use MSHAs and Convs\\nin parallel \\\\textbf{at different granularity levels} instead. Specifically, in\\neach layer, we use two different ways to represent an image: a fine-grained\\nregular grid and a coarse-grained set of semantic slots. We apply different\\noperations to these two representations: Convs to the grid for local features,\\nand MHSAs to the slots for global features. A pair of fully differentiable soft\\nclustering and dispatching modules is introduced to bridge the grid and set\\nrepresentations, thus enabling local-global fusion. Through extensive\\nexperiments on various vision tasks, we empirically verify the potential of the\\nproposed integration scheme, named \\\\textit{GLMix}: by offloading the burden of\\nfine-grained features to light-weight Convs, it is sufficient to use MHSAs in a\\nfew (e.g., 64) semantic slots to match the performance of recent\\nstate-of-the-art backbones, while being more efficient. Our visualization\\nresults also demonstrate that the soft clustering module produces a meaningful\\nsemantic grouping effect with only IN1k classification supervision, which may\\ninduce better interpretability and inspire new weakly-supervised semantic\\nsegmentation approaches. Code will be available at\\n\\\\url{https://github.com/rayleizhu/GLMix}.', comment='NeurIPS 2024', journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2411.14429v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.14429v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2411.14418v1', updated=datetime.datetime(2024, 11, 21, 18, 52, 2, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 21, 18, 52, 2, tzinfo=datetime.timezone.utc), title='Multimodal 3D Brain Tumor Segmentation with Adversarial Training and Conditional Random Field', authors=[arxiv.Result.Author('Lan Jiang'), arxiv.Result.Author('Yuchao Zheng'), arxiv.Result.Author('Miao Yu'), arxiv.Result.Author('Haiqing Zhang'), arxiv.Result.Author('Fatemah Aladwani'), arxiv.Result.Author('Alessandro Perelli')], summary='Accurate brain tumor segmentation remains a challenging task due to\\nstructural complexity and great individual differences of gliomas. Leveraging\\nthe pre-eminent detail resilience of CRF and spatial feature extraction\\ncapacity of V-net, we propose a multimodal 3D Volume Generative Adversarial\\nNetwork (3D-vGAN) for precise segmentation. The model utilizes Pseudo-3D for\\nV-net improvement, adds conditional random field after generator and use\\noriginal image as supplemental guidance. Results, using the BraTS-2018 dataset,\\nshow that 3D-vGAN outperforms classical segmentation models, including U-net,\\nGan, FCN and 3D V-net, reaching specificity over 99.8%.', comment='13 pages, 7 figures, Annual Conference on Medical Image Understanding\\n  and Analysis (MIUA) 2024', journal_ref='Medical Image Understanding and Analysis (MIUA), Lecture Notes in\\n  Computer Science, Springer, vol. 14859, 2024', doi='10.1007/978-3-031-66955-2_5', primary_category='eess.IV', categories=['eess.IV', 'cs.CV', '15-11', 'I.4.6; I.5.4'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/978-3-031-66955-2_5', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2411.14418v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.14418v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2411.14397v1', updated=datetime.datetime(2024, 11, 21, 18, 27, 18, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 21, 18, 27, 18, tzinfo=datetime.timezone.utc), title='Discrete Schrodinger equation on graphs: An effective model for branched quantum lattice', authors=[arxiv.Result.Author('M. Akramov'), arxiv.Result.Author('C. Trunk'), arxiv.Result.Author('J. Yusupov'), arxiv.Result.Author('D. Matrasulov')], summary='We propose an approach to quantize discrete networks (graphs with discrete\\nedges). We introduce a new exact solution of discrete Schrodinger equation that\\nis used to write the solution for quantum graphs. Formulation of the problem\\nand derivation of secular equation for arbitrary quantum graphs is presented.\\nApplication of the approach for the star graph is demonstrated by obtaining\\neigenfunctions and eigenvalues explicitely. Practical application of the model\\nin conducting polymers and branched molecular chains is discussed.', comment=None, journal_ref='EPL 147, 62001 (2024)', doi='10.1209/0295-5075/ad752e', primary_category='quant-ph', categories=['quant-ph', 'cond-mat.mes-hall'], links=[arxiv.Result.Link('http://dx.doi.org/10.1209/0295-5075/ad752e', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2411.14397v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.14397v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2411.14385v1', updated=datetime.datetime(2024, 11, 21, 18, 21, 42, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 21, 18, 21, 42, tzinfo=datetime.timezone.utc), title='Enhancing Diagnostic Precision in Gastric Bleeding through Automated Lesion Segmentation: A Deep DuS-KFCM Approach', authors=[arxiv.Result.Author('Xian-Xian Liu'), arxiv.Result.Author('Mingkun Xu'), arxiv.Result.Author('Yuanyuan Wei'), arxiv.Result.Author('Huafeng Qin'), arxiv.Result.Author('Qun Song'), arxiv.Result.Author('Simon Fong'), arxiv.Result.Author('Feng Tien'), arxiv.Result.Author('Wei Luo'), arxiv.Result.Author('Juntao Gao'), arxiv.Result.Author('Zhihua Zhang'), arxiv.Result.Author('Shirley Siu')], summary=\"Timely and precise classification and segmentation of gastric bleeding in\\nendoscopic imagery are pivotal for the rapid diagnosis and intervention of\\ngastric complications, which is critical in life-saving medical procedures.\\nTraditional methods grapple with the challenge posed by the indistinguishable\\nintensity values of bleeding tissues adjacent to other gastric structures. Our\\nstudy seeks to revolutionize this domain by introducing a novel deep learning\\nmodel, the Dual Spatial Kernelized Constrained Fuzzy C-Means (Deep DuS-KFCM)\\nclustering algorithm. This Hybrid Neuro-Fuzzy system synergizes Neural Networks\\nwith Fuzzy Logic to offer a highly precise and efficient identification of\\nbleeding regions. Implementing a two-fold coarse-to-fine strategy for\\nsegmentation, this model initially employs the Spatial Kernelized Fuzzy C-Means\\n(SKFCM) algorithm enhanced with spatial intensity profiles and subsequently\\nharnesses the state-of-the-art DeepLabv3+ with ResNet50 architecture to refine\\nthe segmentation output. Through extensive experiments across mainstream\\ngastric bleeding and red spots datasets, our Deep DuS-KFCM model demonstrated\\nunprecedented accuracy rates of 87.95%, coupled with a specificity of 96.33%,\\noutperforming contemporary segmentation methods. The findings underscore the\\nmodel's robustness against noise and its outstanding segmentation capabilities,\\nparticularly for identifying subtle bleeding symptoms, thereby presenting a\\nsignificant leap forward in medical image processing.\", comment=None, journal_ref=None, doi=None, primary_category='eess.IV', categories=['eess.IV', 'cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/2411.14385v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.14385v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2411.14376v1', updated=datetime.datetime(2024, 11, 21, 18, 10, 37, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 21, 18, 10, 37, tzinfo=datetime.timezone.utc), title='Solutions to the minimal surface system with large singular sets', authors=[arxiv.Result.Author('Connor Mooney'), arxiv.Result.Author('Ovidiu Savin')], summary='Lawson and Osserman proved that the Dirichlet problem for the minimal surface\\nsystem is not always solvable in the class of Lipschitz maps. However, it is\\nknown that minimizing sequences (for area) of Lipschitz graphs converge to\\nobjects called Cartesian currents. Essentially nothing is known about these\\nlimits. We show that such limits can have surprisingly large interior vertical\\nand non-minimal portions. This demonstrates a striking discrepancy between the\\nparametric and non-parametric area minimization problems in higher codimension.\\nMoreover, our construction has the smallest possible dimension ($n = 3$) and\\ncodimension $(m = 2)$.', comment='18 pages', journal_ref=None, doi=None, primary_category='math.AP', categories=['math.AP', 'math.DG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2411.14376v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.14376v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/1808.08413v1', updated=datetime.datetime(2018, 8, 25, 11, 56, 45, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 8, 25, 11, 56, 45, tzinfo=datetime.timezone.utc), title='A Brief Survey and an Application of Semantic Image Segmentation for Autonomous Driving', authors=[arxiv.Result.Author('√áaƒürƒ± Kaymak'), arxiv.Result.Author('Ay≈üeg√ºl U√ßar')], summary='Deep learning is a fast-growing machine learning approach to perceive and\\nunderstand large amounts of data. In this paper, general information about the\\ndeep learning approach which is attracted much attention in the field of\\nmachine learning is given in recent years and an application about semantic\\nimage segmentation is carried out in order to help autonomous driving of\\nautonomous vehicles. This application is implemented with Fully Convolutional\\nNetwork (FCN) architectures obtained by modifying the Convolutional Neural\\nNetwork (CNN) architectures based on deep learning. Experimental studies for\\nthe application are utilized 4 different FCN architectures named\\nFCN-AlexNet,FCN-8s, FCN-16s and FCN-32s. For the experimental studies, FCNs are\\nfirst trained separately and validation accuracies of these trained network\\nmodels on the used dataset is compared. In addition, image segmentation\\ninferences are visualized to take account of how precisely FCN architectures\\ncan segment objects.', comment='A chapter for Springer Book: Handbook of Deep Learning Applications,\\n  2018,[ Pijush Samui, Editor]. (be published)', journal_ref=None, doi=None, primary_category='eess.IV', categories=['eess.IV'], links=[arxiv.Result.Link('http://arxiv.org/abs/1808.08413v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1808.08413v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2307.13215v1', updated=datetime.datetime(2023, 7, 25, 2, 56, 20, tzinfo=datetime.timezone.utc), published=datetime.datetime(2023, 7, 25, 2, 56, 20, tzinfo=datetime.timezone.utc), title='Image Segmentation Keras : Implementation of Segnet, FCN, UNet, PSPNet and other models in Keras', authors=[arxiv.Result.Author('Divam Gupta')], summary='Semantic segmentation plays a vital role in computer vision tasks, enabling\\nprecise pixel-level understanding of images. In this paper, we present a\\ncomprehensive library for semantic segmentation, which contains implementations\\nof popular segmentation models like SegNet, FCN, UNet, and PSPNet. We also\\nevaluate and compare these models on several datasets, offering researchers and\\npractitioners a powerful toolset for tackling diverse segmentation challenges.', comment=None, journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/2307.13215v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2307.13215v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/1603.09742v4', updated=datetime.datetime(2016, 7, 6, 23, 51, 40, tzinfo=datetime.timezone.utc), published=datetime.datetime(2016, 3, 31, 19, 51, 5, tzinfo=datetime.timezone.utc), title='Object Boundary Guided Semantic Segmentation', authors=[arxiv.Result.Author('Qin Huang'), arxiv.Result.Author('Chunyang Xia'), arxiv.Result.Author('Wenchao Zheng'), arxiv.Result.Author('Yuhang Song'), arxiv.Result.Author('Hao Xu'), arxiv.Result.Author('C. -C. Jay Kuo')], summary='Semantic segmentation is critical to image content understanding and object\\nlocalization. Recent development in fully-convolutional neural network (FCN)\\nhas enabled accurate pixel-level labeling. One issue in previous works is that\\nthe FCN based method does not exploit the object boundary information to\\ndelineate segmentation details since the object boundary label is ignored in\\nthe network training. To tackle this problem, we introduce a double branch\\nfully convolutional neural network, which separates the learning of the\\ndesirable semantic class labeling with mask-level object proposals guided by\\nrelabeled boundaries. This network, called object boundary guided FCN\\n(OBG-FCN), is able to integrate the distinct properties of object shape and\\nclass features elegantly in a fully convolutional way with a designed masking\\narchitecture. We conduct experiments on the PASCAL VOC segmentation benchmark,\\nand show that the end-to-end trainable OBG-FCN system offers great improvement\\nin optimizing the target semantic segmentation quality.', comment='The results in the first version of this paper are mistaken due to\\n  overlapping validation data and incorrect benchmark methods', journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/1603.09742v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1603.09742v4', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2001.00335v1', updated=datetime.datetime(2020, 1, 2, 6, 5, 29, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 1, 2, 6, 5, 29, tzinfo=datetime.timezone.utc), title='Graph-FCN for image semantic segmentation', authors=[arxiv.Result.Author('Yi Lu'), arxiv.Result.Author('Yaran Chen'), arxiv.Result.Author('Dongbin Zhao'), arxiv.Result.Author('Jianxin Chen')], summary='Semantic segmentation with deep learning has achieved great progress in\\nclassifying the pixels in the image. However, the local location information is\\nusually ignored in the high-level feature extraction by the deep learning,\\nwhich is important for image semantic segmentation. To avoid this problem, we\\npropose a graph model initialized by a fully convolutional network (FCN) named\\nGraph-FCN for image semantic segmentation. Firstly, the image grid data is\\nextended to graph structure data by a convolutional network, which transforms\\nthe semantic segmentation problem into a graph node classification problem.\\nThen we apply graph convolutional network to solve this graph node\\nclassification problem. As far as we know, it is the first time that we apply\\nthe graph convolutional network in image semantic segmentation. Our method\\nachieves competitive performance in mean intersection over union (mIOU) on the\\nVOC dataset(about 1.34% improvement), compared to the original FCN model.', comment=None, journal_ref='Advances in Neural Networks, ISNN 2019. Lecture Notes in Computer\\n  Science, vol 11554, pp. 97-105, Springer, Cham', doi=None, primary_category='cs.CV', categories=['cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/2001.00335v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00335v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/1511.02674v2', updated=datetime.datetime(2016, 5, 24, 23, 32, 10, tzinfo=datetime.timezone.utc), published=datetime.datetime(2015, 11, 9, 13, 27, 30, tzinfo=datetime.timezone.utc), title='Semantic Segmentation with Boundary Neural Fields', authors=[arxiv.Result.Author('Gedas Bertasius'), arxiv.Result.Author('Jianbo Shi'), arxiv.Result.Author('Lorenzo Torresani')], summary='The state-of-the-art in semantic segmentation is currently represented by\\nfully convolutional networks (FCNs). However, FCNs use large receptive fields\\nand many pooling layers, both of which cause blurring and low spatial\\nresolution in the deep layers. As a result FCNs tend to produce segmentations\\nthat are poorly localized around object boundaries. Prior work has attempted to\\naddress this issue in post-processing steps, for example using a color-based\\nCRF on top of the FCN predictions. However, these approaches require additional\\nparameters and low-level features that are difficult to tune and integrate into\\nthe original network architecture. Additionally, most CRFs use color-based\\npixel affinities, which are not well suited for semantic segmentation and lead\\nto spatially disjoint predictions.\\n  To overcome these problems, we introduce a Boundary Neural Field (BNF), which\\nis a global energy model integrating FCN predictions with boundary cues. The\\nboundary information is used to enhance semantic segment coherence and to\\nimprove object localization. Specifically, we first show that the convolutional\\nfilters of semantic FCNs provide good features for boundary detection. We then\\nemploy the predicted boundaries to define pairwise potentials in our energy.\\nFinally, we show that our energy decomposes semantic segmentation into multiple\\nbinary problems, which can be relaxed for efficient global optimization. We\\nreport extensive experiments demonstrating that minimization of our global\\nboundary-based energy yields results superior to prior globalization methods,\\nboth quantitatively as well as qualitatively.', comment=None, journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/1511.02674v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1511.02674v2', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2411.14429v1', updated=datetime.datetime(2024, 11, 21, 18, 59, 8, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 21, 18, 59, 8, tzinfo=datetime.timezone.utc), title='Revisiting the Integration of Convolution and Attention for Vision Backbone', authors=[arxiv.Result.Author('Lei Zhu'), arxiv.Result.Author('Xinjiang Wang'), arxiv.Result.Author('Wayne Zhang'), arxiv.Result.Author('Rynson W. H. Lau')], summary='Convolutions (Convs) and multi-head self-attentions (MHSAs) are typically\\nconsidered alternatives to each other for building vision backbones. Although\\nsome works try to integrate both, they apply the two operators simultaneously\\nat the finest pixel granularity. With Convs responsible for per-pixel feature\\nextraction already, the question is whether we still need to include the heavy\\nMHSAs at such a fine-grained level. In fact, this is the root cause of the\\nscalability issue w.r.t. the input resolution for vision transformers. To\\naddress this important problem, we propose in this work to use MSHAs and Convs\\nin parallel \\\\textbf{at different granularity levels} instead. Specifically, in\\neach layer, we use two different ways to represent an image: a fine-grained\\nregular grid and a coarse-grained set of semantic slots. We apply different\\noperations to these two representations: Convs to the grid for local features,\\nand MHSAs to the slots for global features. A pair of fully differentiable soft\\nclustering and dispatching modules is introduced to bridge the grid and set\\nrepresentations, thus enabling local-global fusion. Through extensive\\nexperiments on various vision tasks, we empirically verify the potential of the\\nproposed integration scheme, named \\\\textit{GLMix}: by offloading the burden of\\nfine-grained features to light-weight Convs, it is sufficient to use MHSAs in a\\nfew (e.g., 64) semantic slots to match the performance of recent\\nstate-of-the-art backbones, while being more efficient. Our visualization\\nresults also demonstrate that the soft clustering module produces a meaningful\\nsemantic grouping effect with only IN1k classification supervision, which may\\ninduce better interpretability and inspire new weakly-supervised semantic\\nsegmentation approaches. Code will be available at\\n\\\\url{https://github.com/rayleizhu/GLMix}.', comment='NeurIPS 2024', journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2411.14429v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.14429v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2411.14418v1', updated=datetime.datetime(2024, 11, 21, 18, 52, 2, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 21, 18, 52, 2, tzinfo=datetime.timezone.utc), title='Multimodal 3D Brain Tumor Segmentation with Adversarial Training and Conditional Random Field', authors=[arxiv.Result.Author('Lan Jiang'), arxiv.Result.Author('Yuchao Zheng'), arxiv.Result.Author('Miao Yu'), arxiv.Result.Author('Haiqing Zhang'), arxiv.Result.Author('Fatemah Aladwani'), arxiv.Result.Author('Alessandro Perelli')], summary='Accurate brain tumor segmentation remains a challenging task due to\\nstructural complexity and great individual differences of gliomas. Leveraging\\nthe pre-eminent detail resilience of CRF and spatial feature extraction\\ncapacity of V-net, we propose a multimodal 3D Volume Generative Adversarial\\nNetwork (3D-vGAN) for precise segmentation. The model utilizes Pseudo-3D for\\nV-net improvement, adds conditional random field after generator and use\\noriginal image as supplemental guidance. Results, using the BraTS-2018 dataset,\\nshow that 3D-vGAN outperforms classical segmentation models, including U-net,\\nGan, FCN and 3D V-net, reaching specificity over 99.8%.', comment='13 pages, 7 figures, Annual Conference on Medical Image Understanding\\n  and Analysis (MIUA) 2024', journal_ref='Medical Image Understanding and Analysis (MIUA), Lecture Notes in\\n  Computer Science, Springer, vol. 14859, 2024', doi='10.1007/978-3-031-66955-2_5', primary_category='eess.IV', categories=['eess.IV', 'cs.CV', '15-11', 'I.4.6; I.5.4'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/978-3-031-66955-2_5', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2411.14418v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.14418v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2411.14401v1', updated=datetime.datetime(2024, 11, 21, 18, 30, 11, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 21, 18, 30, 11, tzinfo=datetime.timezone.utc), title='Beyond Training: Dynamic Token Merging for Zero-Shot Video Understanding', authors=[arxiv.Result.Author('Yiming Zhang'), arxiv.Result.Author('Zhuokai Zhao'), arxiv.Result.Author('Zhaorun Chen'), arxiv.Result.Author('Zenghui Ding'), arxiv.Result.Author('Xianjun Yang'), arxiv.Result.Author('Yining Sun')], summary='Recent advancements in multimodal large language models (MLLMs) have opened\\nnew avenues for video understanding. However, achieving high fidelity in\\nzero-shot video tasks remains challenging. Traditional video processing methods\\nrely heavily on fine-tuning to capture nuanced spatial-temporal details, which\\nincurs significant data and computation costs. In contrast, training-free\\napproaches, though efficient, often lack robustness in preserving context-rich\\nfeatures across complex video content. To this end, we propose DYTO, a novel\\ndynamic token merging framework for zero-shot video understanding that\\nadaptively optimizes token efficiency while preserving crucial scene details.\\nDYTO integrates a hierarchical frame selection and a bipartite token merging\\nstrategy to dynamically cluster key frames and selectively compress token\\nsequences, striking a balance between computational efficiency with semantic\\nrichness. Extensive experiments across multiple benchmarks demonstrate the\\neffectiveness of DYTO, achieving superior performance compared to both\\nfine-tuned and training-free methods and setting a new state-of-the-art for\\nzero-shot video understanding.', comment=None, journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2411.14401v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.14401v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2411.14385v1', updated=datetime.datetime(2024, 11, 21, 18, 21, 42, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 21, 18, 21, 42, tzinfo=datetime.timezone.utc), title='Enhancing Diagnostic Precision in Gastric Bleeding through Automated Lesion Segmentation: A Deep DuS-KFCM Approach', authors=[arxiv.Result.Author('Xian-Xian Liu'), arxiv.Result.Author('Mingkun Xu'), arxiv.Result.Author('Yuanyuan Wei'), arxiv.Result.Author('Huafeng Qin'), arxiv.Result.Author('Qun Song'), arxiv.Result.Author('Simon Fong'), arxiv.Result.Author('Feng Tien'), arxiv.Result.Author('Wei Luo'), arxiv.Result.Author('Juntao Gao'), arxiv.Result.Author('Zhihua Zhang'), arxiv.Result.Author('Shirley Siu')], summary=\"Timely and precise classification and segmentation of gastric bleeding in\\nendoscopic imagery are pivotal for the rapid diagnosis and intervention of\\ngastric complications, which is critical in life-saving medical procedures.\\nTraditional methods grapple with the challenge posed by the indistinguishable\\nintensity values of bleeding tissues adjacent to other gastric structures. Our\\nstudy seeks to revolutionize this domain by introducing a novel deep learning\\nmodel, the Dual Spatial Kernelized Constrained Fuzzy C-Means (Deep DuS-KFCM)\\nclustering algorithm. This Hybrid Neuro-Fuzzy system synergizes Neural Networks\\nwith Fuzzy Logic to offer a highly precise and efficient identification of\\nbleeding regions. Implementing a two-fold coarse-to-fine strategy for\\nsegmentation, this model initially employs the Spatial Kernelized Fuzzy C-Means\\n(SKFCM) algorithm enhanced with spatial intensity profiles and subsequently\\nharnesses the state-of-the-art DeepLabv3+ with ResNet50 architecture to refine\\nthe segmentation output. Through extensive experiments across mainstream\\ngastric bleeding and red spots datasets, our Deep DuS-KFCM model demonstrated\\nunprecedented accuracy rates of 87.95%, coupled with a specificity of 96.33%,\\noutperforming contemporary segmentation methods. The findings underscore the\\nmodel's robustness against noise and its outstanding segmentation capabilities,\\nparticularly for identifying subtle bleeding symptoms, thereby presenting a\\nsignificant leap forward in medical image processing.\", comment=None, journal_ref=None, doi=None, primary_category='eess.IV', categories=['eess.IV', 'cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/2411.14385v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.14385v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2411.14353v1', updated=datetime.datetime(2024, 11, 21, 17, 49, 15, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 21, 17, 49, 15, tzinfo=datetime.timezone.utc), title='Enhancing Medical Image Segmentation with Deep Learning and Diffusion Models', authors=[arxiv.Result.Author('Houze Liu'), arxiv.Result.Author('Tong Zhou'), arxiv.Result.Author('Yanlin Xiang'), arxiv.Result.Author('Aoran Shen'), arxiv.Result.Author('Jiacheng Hu'), arxiv.Result.Author('Junliang Du')], summary='Medical image segmentation is crucial for accurate clinical diagnoses, yet it\\nfaces challenges such as low contrast between lesions and normal tissues,\\nunclear boundaries, and high variability across patients. Deep learning has\\nimproved segmentation accuracy and efficiency, but it still relies heavily on\\nexpert annotations and struggles with the complexities of medical images. The\\nsmall size of medical image datasets and the high cost of data acquisition\\nfurther limit the performance of segmentation networks. Diffusion models, with\\ntheir iterative denoising process, offer a promising alternative for better\\ndetail capture in segmentation. However, they face difficulties in accurately\\nsegmenting small targets and maintaining the precision of boundary details.\\nThis article discusses the importance of medical image segmentation, the\\nlimitations of current deep learning approaches, and the potential of diffusion\\nmodels to address these challenges.', comment=None, journal_ref=None, doi=None, primary_category='eess.IV', categories=['eess.IV', 'cs.CV', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2411.14353v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.14353v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/1808.04068v2', updated=datetime.datetime(2018, 8, 14, 6, 35, 15, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 8, 13, 5, 39, 28, tzinfo=datetime.timezone.utc), title='A Transfer Learning based Feature-Weak-Relevant Method for Image Clustering', authors=[arxiv.Result.Author('Bo Dong'), arxiv.Result.Author('Xinnian Wang')], summary=\"Image clustering is to group a set of images into disjoint clusters in a way\\nthat images in the same cluster are more similar to each other than to those in\\nother clusters, which is an unsupervised or semi-supervised learning process.\\nIt is a crucial and challenging task in machine learning and computer vision.\\nThe performances of existing image clustering methods have close relations with\\nfeatures used for clustering, even if unsupervised coding based methods have\\nimproved the performances a lot. To reduce the effect of clustering features,\\nwe propose a feature-weak-relevant method for image clustering. The proposed\\nmethod converts an unsupervised clustering process into an alternative\\niterative process of unsupervised learning and transfer learning. The\\nclustering process firstly starts up from handcrafted features based image\\nclustering to estimate an initial label for every image, and secondly use a\\nproposed sampling strategy to choose images with reliable labels to feed a\\ntransfer-learning model to learn representative features that can be used for\\nnext round of unsupervised learning. In this manner, image clustering is\\niteratively optimized. What's more, the handcrafted features are used to boot\\nup the clustering process, and just have a little effect on the final\\nperformance; therefore, the proposed method is feature-weak-relevant.\\nExperimental results on six kinds of public available datasets show that the\\nproposed method outperforms state of the art methods and depends less on the\\nemployed features at the same time.\", comment='17 pages,3 figures', journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV', '62H30'], links=[arxiv.Result.Link('http://arxiv.org/abs/1808.04068v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1808.04068v2', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/1604.00533v1', updated=datetime.datetime(2016, 4, 2, 17, 27, 24, tzinfo=datetime.timezone.utc), published=datetime.datetime(2016, 4, 2, 17, 27, 24, tzinfo=datetime.timezone.utc), title='Voronoi Region-Based Adaptive Unsupervised Color Image Segmentation', authors=[arxiv.Result.Author('R. Hettiarachchi'), arxiv.Result.Author('J. F. Peters')], summary='Color image segmentation is a crucial step in many computer vision and\\npattern recognition applications. This article introduces an adaptive and\\nunsupervised clustering approach based on Voronoi regions, which can be applied\\nto solve the color image segmentation problem. The proposed method performs\\nregion splitting and merging within Voronoi regions of the Dirichlet\\nTessellated image (also called a Voronoi diagram) , which improves the\\nefficiency and the accuracy of the number of clusters and cluster centroids\\nestimation process. Furthermore, the proposed method uses cluster centroid\\nproximity to merge proximal clusters in order to find the final number of\\nclusters and cluster centroids. In contrast to the existing adaptive\\nunsupervised cluster-based image segmentation algorithms, the proposed method\\nuses K-means clustering algorithm in place of the Fuzzy C-means algorithm to\\nfind the final segmented image. The proposed method was evaluated on three\\ndifferent unsupervised image segmentation evaluation benchmarks and its results\\nwere compared with two other adaptive unsupervised cluster-based image\\nsegmentation algorithms. The experimental results reported in this article\\nconfirm that the proposed method outperforms the existing algorithms in terms\\nof the quality of image segmentation results. Also, the proposed method results\\nin the lowest average execution time per image compared to the existing methods\\nreported in this article.', comment='21 pages, 5 figures', journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV', '05B45, 62H30, 54E05, 68T10'], links=[arxiv.Result.Link('http://arxiv.org/abs/1604.00533v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1604.00533v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2206.04686v1', updated=datetime.datetime(2022, 6, 9, 4, 35, 25, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 6, 9, 4, 35, 25, tzinfo=datetime.timezone.utc), title='Unsupervised Deep Discriminant Analysis Based Clustering', authors=[arxiv.Result.Author('Jinyu Cai'), arxiv.Result.Author('Wenzhong Guo'), arxiv.Result.Author('Jicong Fan')], summary='This work presents an unsupervised deep discriminant analysis for clustering.\\nThe method is based on deep neural networks and aims to minimize the\\nintra-cluster discrepancy and maximize the inter-cluster discrepancy in an\\nunsupervised manner. The method is able to project the data into a nonlinear\\nlow-dimensional latent space with compact and distinct distribution patterns\\nsuch that the data clusters can be effectively identified. We further provide\\nan extension of the method such that available graph information can be\\neffectively exploited to improve the clustering performance. Extensive\\nnumerical results on image and non-image data with or without graph information\\ndemonstrate the effectiveness of the proposed methods.', comment=None, journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2206.04686v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2206.04686v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2106.07846v2', updated=datetime.datetime(2022, 5, 9, 11, 43, 43, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 6, 15, 2, 40, 22, tzinfo=datetime.timezone.utc), title='Cluster-guided Asymmetric Contrastive Learning for Unsupervised Person Re-Identification', authors=[arxiv.Result.Author('Mingkun Li'), arxiv.Result.Author('Chun-Guang Li'), arxiv.Result.Author('Jun Guo')], summary='Unsupervised person re-identification (Re-ID) aims to match pedestrian images\\nfrom different camera views in unsupervised setting. Existing methods for\\nunsupervised person Re-ID are usually built upon the pseudo labels from\\nclustering. However, the quality of clustering depends heavily on the quality\\nof the learned features, which are overwhelmingly dominated by the colors in\\nimages especially in the unsupervised setting. In this paper, we propose a\\nCluster-guided Asymmetric Contrastive Learning (CACL) approach for unsupervised\\nperson Re-ID, in which cluster structure is leveraged to guide the feature\\nlearning in a properly designed asymmetric contrastive learning framework. To\\nbe specific, we propose a novel cluster-level contrastive loss to help the\\nsiamese network effectively mine the invariance in feature learning with\\nrespect to the cluster structure within and between different data augmentation\\nviews, respectively. Extensive experiments conducted on three benchmark\\ndatasets demonstrate superior performance of our proposal.', comment=None, journal_ref=None, doi='10.1109/TIP.2022.3173163', primary_category='cs.CV', categories=['cs.CV'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/TIP.2022.3173163', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2106.07846v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2106.07846v2', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/1804.03830v1', updated=datetime.datetime(2018, 4, 11, 6, 30, 30, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 4, 11, 6, 30, 30, tzinfo=datetime.timezone.utc), title='Unsupervised Segmentation of 3D Medical Images Based on Clustering and Deep Representation Learning', authors=[arxiv.Result.Author('Takayasu Moriya'), arxiv.Result.Author('Holger R. Roth'), arxiv.Result.Author('Shota Nakamura'), arxiv.Result.Author('Hirohisa Oda'), arxiv.Result.Author('Kai Nagara'), arxiv.Result.Author('Masahiro Oda'), arxiv.Result.Author('Kensaku Mori')], summary='This paper presents a novel unsupervised segmentation method for 3D medical\\nimages. Convolutional neural networks (CNNs) have brought significant advances\\nin image segmentation. However, most of the recent methods rely on supervised\\nlearning, which requires large amounts of manually annotated data. Thus, it is\\nchallenging for these methods to cope with the growing amount of medical\\nimages. This paper proposes a unified approach to unsupervised deep\\nrepresentation learning and clustering for segmentation. Our proposed method\\nconsists of two phases. In the first phase, we learn deep feature\\nrepresentations of training patches from a target image using joint\\nunsupervised learning (JULE) that alternately clusters representations\\ngenerated by a CNN and updates the CNN parameters using cluster labels as\\nsupervisory signals. We extend JULE to 3D medical images by utilizing 3D\\nconvolutions throughout the CNN architecture. In the second phase, we apply\\nk-means to the deep representations from the trained CNN and then project\\ncluster labels to the target image in order to obtain the fully segmented\\nimage. We evaluated our methods on three images of lung cancer specimens\\nscanned with micro-computed tomography (micro-CT). The automatic segmentation\\nof pathological regions in micro-CT could further contribute to the\\npathological examination process. Hence, we aim to automatically divide each\\nimage into the regions of invasive carcinoma, noninvasive carcinoma, and normal\\ntissue. Our experiments show the potential abilities of unsupervised deep\\nrepresentation learning for medical image segmentation.', comment='This paper was presented at SPIE Medical Imaging 2018, Houston, TX,\\n  USA', journal_ref='Proc. SPIE 10578, Medical Imaging 2018: Biomedical Applications in\\n  Molecular, Structural, and Functional Imaging, 1057820 (12 March 2018)', doi='10.1117/12.2293414', primary_category='cs.CV', categories=['cs.CV'], links=[arxiv.Result.Link('http://dx.doi.org/10.1117/12.2293414', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1804.03830v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1804.03830v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2411.14430v1', updated=datetime.datetime(2024, 11, 21, 18, 59, 51, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 21, 18, 59, 51, tzinfo=datetime.timezone.utc), title='Stable Flow: Vital Layers for Training-Free Image Editing', authors=[arxiv.Result.Author('Omri Avrahami'), arxiv.Result.Author('Or Patashnik'), arxiv.Result.Author('Ohad Fried'), arxiv.Result.Author('Egor Nemchinov'), arxiv.Result.Author('Kfir Aberman'), arxiv.Result.Author('Dani Lischinski'), arxiv.Result.Author('Daniel Cohen-Or')], summary='Diffusion models have revolutionized the field of content synthesis and\\nediting. Recent models have replaced the traditional UNet architecture with the\\nDiffusion Transformer (DiT), and employed flow-matching for improved training\\nand sampling. However, they exhibit limited generation diversity. In this work,\\nwe leverage this limitation to perform consistent image edits via selective\\ninjection of attention features. The main challenge is that, unlike the\\nUNet-based models, DiT lacks a coarse-to-fine synthesis structure, making it\\nunclear in which layers to perform the injection. Therefore, we propose an\\nautomatic method to identify \"vital layers\" within DiT, crucial for image\\nformation, and demonstrate how these layers facilitate a range of controlled\\nstable edits, from non-rigid modifications to object addition, using the same\\nmechanism. Next, to enable real-image editing, we introduce an improved image\\ninversion method for flow models. Finally, we evaluate our approach through\\nqualitative and quantitative comparisons, along with a user study, and\\ndemonstrate its effectiveness across multiple applications. The project page is\\navailable at https://omriavrahami.com/stable-flow', comment='Project page is available at https://omriavrahami.com/stable-flow', journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV', 'cs.GR', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2411.14430v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.14430v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2411.14429v1', updated=datetime.datetime(2024, 11, 21, 18, 59, 8, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 21, 18, 59, 8, tzinfo=datetime.timezone.utc), title='Revisiting the Integration of Convolution and Attention for Vision Backbone', authors=[arxiv.Result.Author('Lei Zhu'), arxiv.Result.Author('Xinjiang Wang'), arxiv.Result.Author('Wayne Zhang'), arxiv.Result.Author('Rynson W. H. Lau')], summary='Convolutions (Convs) and multi-head self-attentions (MHSAs) are typically\\nconsidered alternatives to each other for building vision backbones. Although\\nsome works try to integrate both, they apply the two operators simultaneously\\nat the finest pixel granularity. With Convs responsible for per-pixel feature\\nextraction already, the question is whether we still need to include the heavy\\nMHSAs at such a fine-grained level. In fact, this is the root cause of the\\nscalability issue w.r.t. the input resolution for vision transformers. To\\naddress this important problem, we propose in this work to use MSHAs and Convs\\nin parallel \\\\textbf{at different granularity levels} instead. Specifically, in\\neach layer, we use two different ways to represent an image: a fine-grained\\nregular grid and a coarse-grained set of semantic slots. We apply different\\noperations to these two representations: Convs to the grid for local features,\\nand MHSAs to the slots for global features. A pair of fully differentiable soft\\nclustering and dispatching modules is introduced to bridge the grid and set\\nrepresentations, thus enabling local-global fusion. Through extensive\\nexperiments on various vision tasks, we empirically verify the potential of the\\nproposed integration scheme, named \\\\textit{GLMix}: by offloading the burden of\\nfine-grained features to light-weight Convs, it is sufficient to use MHSAs in a\\nfew (e.g., 64) semantic slots to match the performance of recent\\nstate-of-the-art backbones, while being more efficient. Our visualization\\nresults also demonstrate that the soft clustering module produces a meaningful\\nsemantic grouping effect with only IN1k classification supervision, which may\\ninduce better interpretability and inspire new weakly-supervised semantic\\nsegmentation approaches. Code will be available at\\n\\\\url{https://github.com/rayleizhu/GLMix}.', comment='NeurIPS 2024', journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2411.14429v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.14429v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2411.14426v1', updated=datetime.datetime(2024, 11, 21, 18, 58, 6, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 21, 18, 58, 6, tzinfo=datetime.timezone.utc), title='Quantum States Imaging of Magnetic Field Contours based on Autler-Townes Effect in Yb Atoms', authors=[arxiv.Result.Author('Tanaporn Na Narong'), arxiv.Result.Author('Hongquan Li'), arxiv.Result.Author('Joshua Tong'), arxiv.Result.Author('Mario Due√±as'), arxiv.Result.Author('Leo Hollberg')], summary='An inter-combination transition in Yb enables a novel approach for rapidly\\nimaging magnetic field variations with excellent spatial and temporal\\nresolution and accuracy. This quantum imaging magnetometer reveals \"dark\\nstripes\" that are contours of constant magnetic field visible by eye or\\ncapturable by standard cameras. These dark lines result from a combination of\\nAutler-Townes splitting and the spatial Hanle effect in the $^{1}S_{0} -\\n^{3}P_{1}$ transition of Yb when driven by multiple strong coherent laser\\nfields (carrier and AM/FM modulation sidebands of a single-mode 556 nm laser).\\nWe show good agreement between experimental data and our theoretical model for\\nthe closed, 4-level Zeeman shifted V-system and demonstrate scalar and vector\\nmagnetic fields measurements at video frame rates over spatial dimensions of 5\\ncm (expandable to $>$ 1 m) with 0.1 mm resolution. Additionally, the $^{1}S_{0}\\n- ^{3}P_{1}$ transition allows for $\\\\sim\\\\mu$s response time and a large dynamic\\nrange ($\\\\mu$T to many Ts).', comment=None, journal_ref=None, doi=None, primary_category='physics.atom-ph', categories=['physics.atom-ph', 'quant-ph'], links=[arxiv.Result.Link('http://arxiv.org/abs/2411.14426v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.14426v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2411.14423v1', updated=datetime.datetime(2024, 11, 21, 18, 55, 23, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 21, 18, 55, 23, tzinfo=datetime.timezone.utc), title='Unleashing the Potential of Multi-modal Foundation Models and Video Diffusion for 4D Dynamic Physical Scene Simulation', authors=[arxiv.Result.Author('Zhuoman Liu'), arxiv.Result.Author('Weicai Ye'), arxiv.Result.Author('Yan Luximon'), arxiv.Result.Author('Pengfei Wan'), arxiv.Result.Author('Di Zhang')], summary='Realistic simulation of dynamic scenes requires accurately capturing diverse\\nmaterial properties and modeling complex object interactions grounded in\\nphysical principles. However, existing methods are constrained to basic\\nmaterial types with limited predictable parameters, making them insufficient to\\nrepresent the complexity of real-world materials. We introduce a novel approach\\nthat leverages multi-modal foundation models and video diffusion to achieve\\nenhanced 4D dynamic scene simulation. Our method utilizes multi-modal models to\\nidentify material types and initialize material parameters through image\\nqueries, while simultaneously inferring 3D Gaussian splats for detailed scene\\nrepresentation. We further refine these material parameters using video\\ndiffusion with a differentiable Material Point Method (MPM) and optical flow\\nguidance rather than render loss or Score Distillation Sampling (SDS) loss.\\nThis integrated framework enables accurate prediction and realistic simulation\\nof dynamic interactions in real-world scenarios, advancing both accuracy and\\nflexibility in physics-based simulations.', comment='Homepage: https://zhuomanliu.github.io/PhysFlow/', journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/2411.14423v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.14423v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2411.14420v1', updated=datetime.datetime(2024, 11, 21, 18, 53, 58, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 21, 18, 53, 58, tzinfo=datetime.timezone.utc), title='Aggregating Funnels for Faster Fetch&Add and Queues', authors=[arxiv.Result.Author('Younghun Roh'), arxiv.Result.Author('Yuanhao Wei'), arxiv.Result.Author('Eric Ruppert'), arxiv.Result.Author('Panagiota Fatourou'), arxiv.Result.Author('Siddhartha Jayanti'), arxiv.Result.Author('Julian Shun')], summary=\"Many concurrent algorithms require processes to perform fetch-and-add\\noperations on a single memory location, which can be a hot spot of contention.\\nWe present a novel algorithm called Aggregating Funnels that reduces this\\ncontention by spreading the fetch-and-add operations across multiple memory\\nlocations. It aggregates fetch-and-add operations into batches so that the\\nbatch can be performed by a single hardware fetch-and-add instruction on one\\nlocation and all operations in the batch can efficiently compute their results\\nby performing a fetch-and-add instruction on a different location. We show\\nexperimentally that this approach achieves higher throughput than previous\\ncombining techniques, such as Combining Funnels, and is substantially more\\nscalable than applying hardware fetch-and-add instructions on a single memory\\nlocation. We show that replacing the fetch-and-add instructions in the fastest\\nstate-of-the-art concurrent queue by our Aggregating Funnels eliminates a\\nbottleneck and greatly improves the queue's overall throughput.\", comment='This is the full version of the paper appearing in PPoPP 2025', journal_ref=None, doi=None, primary_category='cs.DC', categories=['cs.DC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2411.14420v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.14420v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/1805.00500v1', updated=datetime.datetime(2018, 5, 1, 18, 11, 38, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 5, 1, 18, 11, 38, tzinfo=datetime.timezone.utc), title='Adapting Mask-RCNN for Automatic Nucleus Segmentation', authors=[arxiv.Result.Author('Jeremiah W. Johnson')], summary='Automatic segmentation of microscopy images is an important task in medical\\nimage processing and analysis. Nucleus detection is an important example of\\nthis task. Mask-RCNN is a recently proposed state-of-the-art algorithm for\\nobject detection, object localization, and object instance segmentation of\\nnatural images. In this paper we demonstrate that Mask-RCNN can be used to\\nperform highly effective and efficient automatic segmentations of a wide range\\nof microscopy images of cell nuclei, for a variety of cells acquired under a\\nvariety of conditions.', comment='7 pages, 3 figures', journal_ref='Proceedings of the 2019 Computer Vision Conference, Vol. 2', doi='10.1007/978-3-030-17798-0', primary_category='cs.CV', categories=['cs.CV', 'cs.LG'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/978-3-030-17798-0', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/1805.00500v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1805.00500v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2007.03593v1', updated=datetime.datetime(2020, 7, 7, 16, 23, 10, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 7, 7, 16, 23, 10, tzinfo=datetime.timezone.utc), title='Instance Segmentation for Whole Slide Imaging: End-to-End or Detect-Then-Segment', authors=[arxiv.Result.Author('Aadarsh Jha'), arxiv.Result.Author('Haichun Yang'), arxiv.Result.Author('Ruining Deng'), arxiv.Result.Author('Meghan E. Kapp'), arxiv.Result.Author('Agnes B. Fogo'), arxiv.Result.Author('Yuankai Huo')], summary='Automatic instance segmentation of glomeruli within kidney Whole Slide\\nImaging (WSI) is essential for clinical research in renal pathology. In\\ncomputer vision, the end-to-end instance segmentation methods (e.g., Mask-RCNN)\\nhave shown their advantages relative to detect-then-segment approaches by\\nperforming complementary detection and segmentation tasks simultaneously. As a\\nresult, the end-to-end Mask-RCNN approach has been the de facto standard method\\nin recent glomerular segmentation studies, where downsampling and patch-based\\ntechniques are used to properly evaluate the high resolution images from WSI\\n(e.g., >10,000x10,000 pixels on 40x). However, in high resolution WSI, a single\\nglomerulus itself can be more than 1,000x1,000 pixels in original resolution\\nwhich yields significant information loss when the corresponding features maps\\nare downsampled via the Mask-RCNN pipeline. In this paper, we assess if the\\nend-to-end instance segmentation framework is optimal for high-resolution WSI\\nobjects by comparing Mask-RCNN with our proposed detect-then-segment framework.\\nBeyond such a comparison, we also comprehensively evaluate the performance of\\nour detect-then-segment pipeline through: 1) two of the most prevalent\\nsegmentation backbones (U-Net and DeepLab_v3); 2) six different image\\nresolutions (from 512x512 to 28x28); and 3) two different color spaces (RGB and\\nLAB). Our detect-then-segment pipeline, with the DeepLab_v3 segmentation\\nframework operating on previously detected glomeruli of 512x512 resolution,\\nachieved a 0.953 dice similarity coefficient (DSC), compared with a 0.902 DSC\\nfrom the end-to-end Mask-RCNN pipeline. Further, we found that neither RGB nor\\nLAB color spaces yield better performance when compared against each other in\\nthe context of a detect-then-segment framework. Detect-then-segment pipeline\\nachieved better segmentation performance compared with End-to-end method.', comment=None, journal_ref=None, doi=None, primary_category='eess.IV', categories=['eess.IV', 'cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/2007.03593v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2007.03593v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/1910.00032v1', updated=datetime.datetime(2019, 9, 30, 18, 3, 9, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 9, 30, 18, 3, 9, tzinfo=datetime.timezone.utc), title='LIP: Learning Instance Propagation for Video Object Segmentation', authors=[arxiv.Result.Author('Ye Lyu'), arxiv.Result.Author('George Vosselman'), arxiv.Result.Author('Gui-Song Xia'), arxiv.Result.Author('Michael Ying Yang')], summary='In recent years, the task of segmenting foreground objects from background in\\na video, i.e. video object segmentation (VOS), has received considerable\\nattention. In this paper, we propose a single end-to-end trainable deep neural\\nnetwork, convolutional gated recurrent Mask-RCNN, for tackling the\\nsemi-supervised VOS task. We take advantage of both the instance segmentation\\nnetwork (Mask-RCNN) and the visual memory module (Conv-GRU) to tackle the VOS\\ntask. The instance segmentation network predicts masks for instances, while the\\nvisual memory module learns to selectively propagate information for multiple\\ninstances simultaneously, which handles the appearance change, the variation of\\nscale and pose and the occlusions between objects. After offline and online\\ntraining under purely instance segmentation losses, our approach is able to\\nachieve satisfactory results without any post-processing or synthetic video\\ndata augmentation. Experimental results on DAVIS 2016 dataset and DAVIS 2017\\ndataset have demonstrated the effectiveness of our method for video object\\nsegmentation task.', comment='ICCVW19', journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/1910.00032v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1910.00032v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2003.05664v4', updated=datetime.datetime(2020, 7, 26, 2, 18, 32, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 3, 12, 8, 42, 36, tzinfo=datetime.timezone.utc), title='Conditional Convolutions for Instance Segmentation', authors=[arxiv.Result.Author('Zhi Tian'), arxiv.Result.Author('Chunhua Shen'), arxiv.Result.Author('Hao Chen')], summary='We propose a simple yet effective instance segmentation framework, termed\\nCondInst (conditional convolutions for instance segmentation). Top-performing\\ninstance segmentation methods such as Mask R-CNN rely on ROI operations\\n(typically ROIPool or ROIAlign) to obtain the final instance masks. In\\ncontrast, we propose to solve instance segmentation from a new perspective.\\nInstead of using instance-wise ROIs as inputs to a network of fixed weights, we\\nemploy dynamic instance-aware networks, conditioned on instances. CondInst\\nenjoys two advantages: 1) Instance segmentation is solved by a fully\\nconvolutional network, eliminating the need for ROI cropping and feature\\nalignment. 2) Due to the much improved capacity of dynamically-generated\\nconditional convolutions, the mask head can be very compact (e.g., 3 conv.\\nlayers, each having only 8 channels), leading to significantly faster\\ninference. We demonstrate a simpler instance segmentation method that can\\nachieve improved performance in both accuracy and inference speed. On the COCO\\ndataset, we outperform a few recent methods including well-tuned Mask RCNN\\nbaselines, without longer training schedules needed.\\n  Code is available: https://github.com/aim-uofa/adet', comment='Accepted to Proc. European Conf. Computer Vision (ECCV) 2020 as oral\\n  presentation', journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/2003.05664v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2003.05664v4', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2302.09569v1', updated=datetime.datetime(2023, 2, 19, 13, 12, 28, tzinfo=datetime.timezone.utc), published=datetime.datetime(2023, 2, 19, 13, 12, 28, tzinfo=datetime.timezone.utc), title='SEMI-PointRend: Improved Semiconductor Wafer Defect Classification and Segmentation as Rendering', authors=[arxiv.Result.Author('MinJin Hwang'), arxiv.Result.Author('Bappaditya Dey'), arxiv.Result.Author('Enrique Dehaerne'), arxiv.Result.Author('Sandip Halder'), arxiv.Result.Author('Young-han Shin')], summary='In this study, we applied the PointRend (Point-based Rendering) method to\\nsemiconductor defect segmentation. PointRend is an iterative segmentation\\nalgorithm inspired by image rendering in computer graphics, a new image\\nsegmentation method that can generate high-resolution segmentation masks. It\\ncan also be flexibly integrated into common instance segmentation\\nmeta-architecture such as Mask-RCNN and semantic meta-architecture such as FCN.\\nWe implemented a model, termed as SEMI-PointRend, to generate precise\\nsegmentation masks by applying the PointRend neural network module. In this\\npaper, we focus on comparing the defect segmentation predictions of\\nSEMI-PointRend and Mask-RCNN for various defect types (line-collapse, single\\nbridge, thin bridge, multi bridge non-horizontal). We show that SEMI-PointRend\\ncan outperforms Mask R-CNN by up to 18.8% in terms of segmentation mean average\\nprecision.', comment='7 pages, 6 figures, 5 tables. To be published by SPIE in the\\n  proceedings of Metrology, Inspection, and Process Control XXXVII', journal_ref='Proc. SPIE 12496, Metrology, Inspection, and Process Control\\n  XXXVII, 1249608 (27 April 2023)', doi='10.1117/12.2657555', primary_category='cs.CV', categories=['cs.CV', 'I.4.9'], links=[arxiv.Result.Link('http://dx.doi.org/10.1117/12.2657555', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2302.09569v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2302.09569v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2411.14429v1', updated=datetime.datetime(2024, 11, 21, 18, 59, 8, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 21, 18, 59, 8, tzinfo=datetime.timezone.utc), title='Revisiting the Integration of Convolution and Attention for Vision Backbone', authors=[arxiv.Result.Author('Lei Zhu'), arxiv.Result.Author('Xinjiang Wang'), arxiv.Result.Author('Wayne Zhang'), arxiv.Result.Author('Rynson W. H. Lau')], summary='Convolutions (Convs) and multi-head self-attentions (MHSAs) are typically\\nconsidered alternatives to each other for building vision backbones. Although\\nsome works try to integrate both, they apply the two operators simultaneously\\nat the finest pixel granularity. With Convs responsible for per-pixel feature\\nextraction already, the question is whether we still need to include the heavy\\nMHSAs at such a fine-grained level. In fact, this is the root cause of the\\nscalability issue w.r.t. the input resolution for vision transformers. To\\naddress this important problem, we propose in this work to use MSHAs and Convs\\nin parallel \\\\textbf{at different granularity levels} instead. Specifically, in\\neach layer, we use two different ways to represent an image: a fine-grained\\nregular grid and a coarse-grained set of semantic slots. We apply different\\noperations to these two representations: Convs to the grid for local features,\\nand MHSAs to the slots for global features. A pair of fully differentiable soft\\nclustering and dispatching modules is introduced to bridge the grid and set\\nrepresentations, thus enabling local-global fusion. Through extensive\\nexperiments on various vision tasks, we empirically verify the potential of the\\nproposed integration scheme, named \\\\textit{GLMix}: by offloading the burden of\\nfine-grained features to light-weight Convs, it is sufficient to use MHSAs in a\\nfew (e.g., 64) semantic slots to match the performance of recent\\nstate-of-the-art backbones, while being more efficient. Our visualization\\nresults also demonstrate that the soft clustering module produces a meaningful\\nsemantic grouping effect with only IN1k classification supervision, which may\\ninduce better interpretability and inspire new weakly-supervised semantic\\nsegmentation approaches. Code will be available at\\n\\\\url{https://github.com/rayleizhu/GLMix}.', comment='NeurIPS 2024', journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2411.14429v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.14429v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2411.14418v1', updated=datetime.datetime(2024, 11, 21, 18, 52, 2, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 21, 18, 52, 2, tzinfo=datetime.timezone.utc), title='Multimodal 3D Brain Tumor Segmentation with Adversarial Training and Conditional Random Field', authors=[arxiv.Result.Author('Lan Jiang'), arxiv.Result.Author('Yuchao Zheng'), arxiv.Result.Author('Miao Yu'), arxiv.Result.Author('Haiqing Zhang'), arxiv.Result.Author('Fatemah Aladwani'), arxiv.Result.Author('Alessandro Perelli')], summary='Accurate brain tumor segmentation remains a challenging task due to\\nstructural complexity and great individual differences of gliomas. Leveraging\\nthe pre-eminent detail resilience of CRF and spatial feature extraction\\ncapacity of V-net, we propose a multimodal 3D Volume Generative Adversarial\\nNetwork (3D-vGAN) for precise segmentation. The model utilizes Pseudo-3D for\\nV-net improvement, adds conditional random field after generator and use\\noriginal image as supplemental guidance. Results, using the BraTS-2018 dataset,\\nshow that 3D-vGAN outperforms classical segmentation models, including U-net,\\nGan, FCN and 3D V-net, reaching specificity over 99.8%.', comment='13 pages, 7 figures, Annual Conference on Medical Image Understanding\\n  and Analysis (MIUA) 2024', journal_ref='Medical Image Understanding and Analysis (MIUA), Lecture Notes in\\n  Computer Science, Springer, vol. 14859, 2024', doi='10.1007/978-3-031-66955-2_5', primary_category='eess.IV', categories=['eess.IV', 'cs.CV', '15-11', 'I.4.6; I.5.4'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/978-3-031-66955-2_5', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2411.14418v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.14418v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2411.14411v1', updated=datetime.datetime(2024, 11, 21, 18, 46, 23, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 21, 18, 46, 23, tzinfo=datetime.timezone.utc), title='Multi-Agent Environments for Vehicle Routing Problems', authors=[arxiv.Result.Author('Ricardo Gama'), arxiv.Result.Author('Daniel Fuertes'), arxiv.Result.Author('Carlos R. del-Blanco'), arxiv.Result.Author('Hugo L. Fernandes')], summary='Research on Reinforcement Learning (RL) approaches for discrete optimization\\nproblems has increased considerably, extending RL to an area classically\\ndominated by Operations Research (OR). Vehicle routing problems are a good\\nexample of discrete optimization problems with high practical relevance where\\nRL techniques have had considerable success. Despite these advances,\\nopen-source development frameworks remain scarce, hampering both the testing of\\nalgorithms and the ability to objectively compare results. This ultimately\\nslows down progress in the field and limits the exchange of ideas between the\\nRL and OR communities.\\n  Here we propose a library composed of multi-agent environments that simulates\\nclassic vehicle routing problems. The library, built on PyTorch, provides a\\nflexible modular architecture design that allows easy customization and\\nincorporation of new routing problems. It follows the Agent Environment Cycle\\n(\"AEC\") games model and has an intuitive API, enabling rapid adoption and easy\\nintegration into existing reinforcement learning frameworks.\\n  The library allows for a straightforward use of classical OR benchmark\\ninstances in order to narrow the gap between the test beds for algorithm\\nbenchmarking used by the RL and OR communities. Additionally, we provide\\nbenchmark instance sets for each environment, as well as baseline RL models and\\ntraining code.', comment=None, journal_ref=None, doi=None, primary_category='cs.LG', categories=['cs.LG', 'cs.MA'], links=[arxiv.Result.Link('http://arxiv.org/abs/2411.14411v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.14411v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2411.14385v1', updated=datetime.datetime(2024, 11, 21, 18, 21, 42, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 21, 18, 21, 42, tzinfo=datetime.timezone.utc), title='Enhancing Diagnostic Precision in Gastric Bleeding through Automated Lesion Segmentation: A Deep DuS-KFCM Approach', authors=[arxiv.Result.Author('Xian-Xian Liu'), arxiv.Result.Author('Mingkun Xu'), arxiv.Result.Author('Yuanyuan Wei'), arxiv.Result.Author('Huafeng Qin'), arxiv.Result.Author('Qun Song'), arxiv.Result.Author('Simon Fong'), arxiv.Result.Author('Feng Tien'), arxiv.Result.Author('Wei Luo'), arxiv.Result.Author('Juntao Gao'), arxiv.Result.Author('Zhihua Zhang'), arxiv.Result.Author('Shirley Siu')], summary=\"Timely and precise classification and segmentation of gastric bleeding in\\nendoscopic imagery are pivotal for the rapid diagnosis and intervention of\\ngastric complications, which is critical in life-saving medical procedures.\\nTraditional methods grapple with the challenge posed by the indistinguishable\\nintensity values of bleeding tissues adjacent to other gastric structures. Our\\nstudy seeks to revolutionize this domain by introducing a novel deep learning\\nmodel, the Dual Spatial Kernelized Constrained Fuzzy C-Means (Deep DuS-KFCM)\\nclustering algorithm. This Hybrid Neuro-Fuzzy system synergizes Neural Networks\\nwith Fuzzy Logic to offer a highly precise and efficient identification of\\nbleeding regions. Implementing a two-fold coarse-to-fine strategy for\\nsegmentation, this model initially employs the Spatial Kernelized Fuzzy C-Means\\n(SKFCM) algorithm enhanced with spatial intensity profiles and subsequently\\nharnesses the state-of-the-art DeepLabv3+ with ResNet50 architecture to refine\\nthe segmentation output. Through extensive experiments across mainstream\\ngastric bleeding and red spots datasets, our Deep DuS-KFCM model demonstrated\\nunprecedented accuracy rates of 87.95%, coupled with a specificity of 96.33%,\\noutperforming contemporary segmentation methods. The findings underscore the\\nmodel's robustness against noise and its outstanding segmentation capabilities,\\nparticularly for identifying subtle bleeding symptoms, thereby presenting a\\nsignificant leap forward in medical image processing.\", comment=None, journal_ref=None, doi=None, primary_category='eess.IV', categories=['eess.IV', 'cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/2411.14385v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.14385v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2411.14353v1', updated=datetime.datetime(2024, 11, 21, 17, 49, 15, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 21, 17, 49, 15, tzinfo=datetime.timezone.utc), title='Enhancing Medical Image Segmentation with Deep Learning and Diffusion Models', authors=[arxiv.Result.Author('Houze Liu'), arxiv.Result.Author('Tong Zhou'), arxiv.Result.Author('Yanlin Xiang'), arxiv.Result.Author('Aoran Shen'), arxiv.Result.Author('Jiacheng Hu'), arxiv.Result.Author('Junliang Du')], summary='Medical image segmentation is crucial for accurate clinical diagnoses, yet it\\nfaces challenges such as low contrast between lesions and normal tissues,\\nunclear boundaries, and high variability across patients. Deep learning has\\nimproved segmentation accuracy and efficiency, but it still relies heavily on\\nexpert annotations and struggles with the complexities of medical images. The\\nsmall size of medical image datasets and the high cost of data acquisition\\nfurther limit the performance of segmentation networks. Diffusion models, with\\ntheir iterative denoising process, offer a promising alternative for better\\ndetail capture in segmentation. However, they face difficulties in accurately\\nsegmenting small targets and maintaining the precision of boundary details.\\nThis article discusses the importance of medical image segmentation, the\\nlimitations of current deep learning approaches, and the potential of diffusion\\nmodels to address these challenges.', comment=None, journal_ref=None, doi=None, primary_category='eess.IV', categories=['eess.IV', 'cs.CV', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2411.14353v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.14353v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2102.00337v2', updated=datetime.datetime(2021, 4, 13, 14, 24, 6, tzinfo=datetime.timezone.utc), published=datetime.datetime(2021, 1, 30, 23, 34, 15, tzinfo=datetime.timezone.utc), title='Using Multiple Generative Adversarial Networks to Build Better-Connected Levels for Mega Man', authors=[arxiv.Result.Author('Benjamin Capps'), arxiv.Result.Author('Jacob Schrum')], summary=\"Generative Adversarial Networks (GANs) can generate levels for a variety of\\ngames. This paper focuses on combining GAN-generated segments in a snaking\\npattern to create levels for Mega Man. Adjacent segments in such levels can be\\northogonally adjacent in any direction, meaning that an otherwise fine segment\\nmight impose a barrier between its neighbor depending on what sorts of segments\\nin the training set are being most closely emulated: horizontal, vertical, or\\ncorner segments. To pick appropriate segments, multiple GANs were trained on\\ndifferent types of segments to ensure better flow between segments. Flow was\\nfurther improved by evolving the latent vectors for the segments being joined\\nin the level to maximize the length of the level's solution path. Using\\nmultiple GANs to represent different types of segments results in significantly\\nlonger solution paths than using one GAN for all segment types, and a human\\nsubject study verifies that these levels are more fun and have more human-like\\ndesign than levels produced by one GAN.\", comment='Accepted to Genetic and Evolutionary Computation Conference 2021', journal_ref=None, doi=None, primary_category='cs.NE', categories=['cs.NE', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2102.00337v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2102.00337v2', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2308.13759v1', updated=datetime.datetime(2023, 8, 26, 4, 46, 10, tzinfo=datetime.timezone.utc), published=datetime.datetime(2023, 8, 26, 4, 46, 10, tzinfo=datetime.timezone.utc), title='SamDSK: Combining Segment Anything Model with Domain-Specific Knowledge for Semi-Supervised Learning in Medical Image Segmentation', authors=[arxiv.Result.Author('Yizhe Zhang'), arxiv.Result.Author('Tao Zhou'), arxiv.Result.Author('Shuo Wang'), arxiv.Result.Author('Ye Wu'), arxiv.Result.Author('Pengfei Gu'), arxiv.Result.Author('Danny Z. Chen')], summary='The Segment Anything Model (SAM) exhibits a capability to segment a wide\\narray of objects in natural images, serving as a versatile perceptual tool for\\nvarious downstream image segmentation tasks. In contrast, medical image\\nsegmentation tasks often rely on domain-specific knowledge (DSK). In this\\npaper, we propose a novel method that combines the segmentation foundation\\nmodel (i.e., SAM) with domain-specific knowledge for reliable utilization of\\nunlabeled images in building a medical image segmentation model. Our new method\\nis iterative and consists of two main stages: (1) segmentation model training;\\n(2) expanding the labeled set by using the trained segmentation model, an\\nunlabeled set, SAM, and domain-specific knowledge. These two stages are\\nrepeated until no more samples are added to the labeled set. A novel\\noptimal-matching-based method is developed for combining the SAM-generated\\nsegmentation proposals and pixel-level and image-level DSK for constructing\\nannotations of unlabeled images in the iterative stage (2). In experiments, we\\ndemonstrate the effectiveness of our proposed method for breast cancer\\nsegmentation in ultrasound images, polyp segmentation in endoscopic images, and\\nskin lesion segmentation in dermoscopic images. Our work initiates a new\\ndirection of semi-supervised learning for medical image segmentation: the\\nsegmentation foundation model can be harnessed as a valuable tool for\\nlabel-efficient segmentation learning in medical image segmentation.', comment='15 pages, 7 figures, Github: https://github.com/yizhezhang2000/SamDSK', journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV', 'cs.AI', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2308.13759v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2308.13759v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/1910.00950v1', updated=datetime.datetime(2019, 10, 2, 13, 45, 33, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 10, 2, 13, 45, 33, tzinfo=datetime.timezone.utc), title='CNN-based Semantic Segmentation using Level Set Loss', authors=[arxiv.Result.Author('Youngeun Kim'), arxiv.Result.Author('Seunghyeon Kim'), arxiv.Result.Author('Taekyung Kim'), arxiv.Result.Author('Changick Kim')], summary='Thesedays, Convolutional Neural Networks are widely used in semantic\\nsegmentation. However, since CNN-based segmentation networks produce\\nlow-resolution outputs with rich semantic information, it is inevitable that\\nspatial details (e.g., small bjects and fine boundary information) of\\nsegmentation results will be lost. To address this problem, motivated by a\\nvariational approach to image segmentation (i.e., level set theory), we propose\\na novel loss function called the level set loss which is designed to refine\\nspatial details of segmentation results. To deal with multiple classes in an\\nimage, we first decompose the ground truth into binary images. Note that each\\nbinary image consists of background and regions belonging to a class. Then we\\nconvert level set functions into class probability maps and calculate the\\nenergy for each class. The network is trained to minimize the weighted sum of\\nthe level set loss and the cross-entropy loss. The proposed level set loss\\nimproves the spatial details of segmentation results in a time and memory\\nefficient way. Furthermore, our experimental results show that the proposed\\nloss function achieves better performance than previous approaches.', comment='2019 IEEE Winter Conference on Applications of Computer Vision\\n  (WACV). IEEE, 2019', journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/1910.00950v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1910.00950v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2207.04297v1', updated=datetime.datetime(2022, 7, 9, 16, 38, 33, tzinfo=datetime.timezone.utc), published=datetime.datetime(2022, 7, 9, 16, 38, 33, tzinfo=datetime.timezone.utc), title='SHDM-NET: Heat Map Detail Guidance with Image Matting for Industrial Weld Semantic Segmentation Network', authors=[arxiv.Result.Author('Qi Wang'), arxiv.Result.Author('Jingwu Mei')], summary='In actual industrial production, the assessment of the steel plate welding\\neffect is an important task, and the segmentation of the weld section is the\\nbasis of the assessment. This paper proposes an industrial weld segmentation\\nnetwork based on a deep learning semantic segmentation algorithm fused with\\nheatmap detail guidance and Image Matting to solve the automatic segmentation\\nproblem of weld regions. In the existing semantic segmentation networks, the\\nboundary information can be preserved by fusing the features of both high-level\\nand low-level layers. However, this method can lead to insufficient expression\\nof the spatial information in the low-level layer, resulting in inaccurate\\nsegmentation boundary positioning. We propose a detailed guidance module based\\non heatmaps to fully express the segmented region boundary information in the\\nlow-level network to address this problem. Specifically, the expression of\\nboundary information can be enhanced by adding a detailed branch to predict\\nsegmented boundary and then matching it with the boundary heat map generated by\\nmask labels to calculate the mean square error loss. In addition, although deep\\nlearning has achieved great success in the field of semantic segmentation, the\\nprecision of the segmentation boundary region is not high due to the loss of\\ndetailed information caused by the classical segmentation network in the\\nprocess of encoding and decoding process. This paper introduces a matting\\nalgorithm to calibrate the boundary of the segmentation region of the semantic\\nsegmentation network to solve this problem. Through many experiments on\\nindustrial weld data sets, the effectiveness of our method is demonstrated, and\\nthe MIOU reaches 97.93%. It is worth noting that this performance is comparable\\nto human manual segmentation ( MIOU 97.96%).', comment=None, journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/2207.04297v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2207.04297v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/1206.2807v1', updated=datetime.datetime(2012, 6, 13, 13, 49, 23, tzinfo=datetime.timezone.utc), published=datetime.datetime(2012, 6, 13, 13, 49, 23, tzinfo=datetime.timezone.utc), title='An efficient hierarchical graph based image segmentation', authors=[arxiv.Result.Author('Silvio Jamil F. Guimar√£es'), arxiv.Result.Author('Jean Cousty'), arxiv.Result.Author('Yukiko Kenmochi'), arxiv.Result.Author('Laurent Najman')], summary='Hierarchical image segmentation provides region-oriented scalespace, i.e., a\\nset of image segmentations at different detail levels in which the\\nsegmentations at finer levels are nested with respect to those at coarser\\nlevels. Most image segmentation algorithms, such as region merging algorithms,\\nrely on a criterion for merging that does not lead to a hierarchy, and for\\nwhich the tuning of the parameters can be difficult. In this work, we propose a\\nhierarchical graph based image segmentation relying on a criterion popularized\\nby Felzenzwalb and Huttenlocher. We illustrate with both real and synthetic\\nimages, showing efficiency, ease of use, and robustness of our method.', comment=None, journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/1206.2807v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1206.2807v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2411.14432v1', updated=datetime.datetime(2024, 11, 21, 18, 59, 55, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 21, 18, 59, 55, tzinfo=datetime.timezone.utc), title='Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models', authors=[arxiv.Result.Author('Yuhao Dong'), arxiv.Result.Author('Zuyan Liu'), arxiv.Result.Author('Hai-Long Sun'), arxiv.Result.Author('Jingkang Yang'), arxiv.Result.Author('Winston Hu'), arxiv.Result.Author('Yongming Rao'), arxiv.Result.Author('Ziwei Liu')], summary=\"Large Language Models (LLMs) demonstrate enhanced capabilities and\\nreliability by reasoning more, evolving from Chain-of-Thought prompting to\\nproduct-level solutions like OpenAI o1. Despite various efforts to improve LLM\\nreasoning, high-quality long-chain reasoning data and optimized training\\npipelines still remain inadequately explored in vision-language tasks. In this\\npaper, we present Insight-V, an early effort to 1) scalably produce long and\\nrobust reasoning data for complex multi-modal tasks, and 2) an effective\\ntraining pipeline to enhance the reasoning capabilities of multi-modal large\\nlanguage models (MLLMs). Specifically, to create long and structured reasoning\\ndata without human labor, we design a two-step pipeline with a progressive\\nstrategy to generate sufficiently long and diverse reasoning paths and a\\nmulti-granularity assessment method to ensure data quality. We observe that\\ndirectly supervising MLLMs with such long and complex reasoning data will not\\nyield ideal reasoning ability. To tackle this problem, we design a multi-agent\\nsystem consisting of a reasoning agent dedicated to performing long-chain\\nreasoning and a summary agent trained to judge and summarize reasoning results.\\nWe further incorporate an iterative DPO algorithm to enhance the reasoning\\nagent's generation stability and quality. Based on the popular LLaVA-NeXT model\\nand our stronger base MLLM, we demonstrate significant performance gains across\\nchallenging multi-modal benchmarks requiring visual reasoning. Benefiting from\\nour multi-agent system, Insight-V can also easily maintain or improve\\nperformance on perception-focused multi-modal tasks.\", comment=None, journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/2411.14432v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.14432v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2411.14429v1', updated=datetime.datetime(2024, 11, 21, 18, 59, 8, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 21, 18, 59, 8, tzinfo=datetime.timezone.utc), title='Revisiting the Integration of Convolution and Attention for Vision Backbone', authors=[arxiv.Result.Author('Lei Zhu'), arxiv.Result.Author('Xinjiang Wang'), arxiv.Result.Author('Wayne Zhang'), arxiv.Result.Author('Rynson W. H. Lau')], summary='Convolutions (Convs) and multi-head self-attentions (MHSAs) are typically\\nconsidered alternatives to each other for building vision backbones. Although\\nsome works try to integrate both, they apply the two operators simultaneously\\nat the finest pixel granularity. With Convs responsible for per-pixel feature\\nextraction already, the question is whether we still need to include the heavy\\nMHSAs at such a fine-grained level. In fact, this is the root cause of the\\nscalability issue w.r.t. the input resolution for vision transformers. To\\naddress this important problem, we propose in this work to use MSHAs and Convs\\nin parallel \\\\textbf{at different granularity levels} instead. Specifically, in\\neach layer, we use two different ways to represent an image: a fine-grained\\nregular grid and a coarse-grained set of semantic slots. We apply different\\noperations to these two representations: Convs to the grid for local features,\\nand MHSAs to the slots for global features. A pair of fully differentiable soft\\nclustering and dispatching modules is introduced to bridge the grid and set\\nrepresentations, thus enabling local-global fusion. Through extensive\\nexperiments on various vision tasks, we empirically verify the potential of the\\nproposed integration scheme, named \\\\textit{GLMix}: by offloading the burden of\\nfine-grained features to light-weight Convs, it is sufficient to use MHSAs in a\\nfew (e.g., 64) semantic slots to match the performance of recent\\nstate-of-the-art backbones, while being more efficient. Our visualization\\nresults also demonstrate that the soft clustering module produces a meaningful\\nsemantic grouping effect with only IN1k classification supervision, which may\\ninduce better interpretability and inspire new weakly-supervised semantic\\nsegmentation approaches. Code will be available at\\n\\\\url{https://github.com/rayleizhu/GLMix}.', comment='NeurIPS 2024', journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV', 'cs.AI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2411.14429v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.14429v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2411.14419v1', updated=datetime.datetime(2024, 11, 21, 18, 52, 29, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 21, 18, 52, 29, tzinfo=datetime.timezone.utc), title='Combining summary statistics with simulation-based inference for the 21 cm signal from the Epoch of Reionization', authors=[arxiv.Result.Author('Benoit Semelin'), arxiv.Result.Author('Romain M√©riot'), arxiv.Result.Author('Ashutosh Mishra'), arxiv.Result.Author('David Cornu')], summary='The 21 cm signal from the Epoch of Reionization will be observed with the\\nup-coming Square Kilometer Array (SKA). SKA should yield a full tomography of\\nthe signal which opens the possibility to explore its non-Gaussian properties.\\nHow can we extract the maximum information from the tomography and derive the\\ntightest constraint on the signal? In this work, instead of looking for the\\nmost informative summary statistics, we investigate how to combine the\\ninformation from two sets of summary statistics using simulation-based\\ninference. To this purpose, we train Neural Density Estimators (NDE) to fit the\\nimplicit likelihood of our model, the LICORICE code, using the Loreli II\\ndatabase. We train three different NDEs: one to perform Bayesian inference on\\nthe power spectrum, one to do it on the linear moments of the Pixel\\nDistribution Function (PDF) and one to work with the combination of the two. We\\nperform $\\\\sim 900$ inferences at different points in our parameter space and\\nuse them to assess both the validity of our posteriors with Simulation-based\\nCalibration (SBC) and the typical gain obtained by combining summary\\nstatistics. We find that our posteriors are biased by no more than $\\\\sim 20 \\\\%$\\nof their standard deviation and under-confident by no more than $\\\\sim 15 \\\\%$.\\nThen, we establish that combining summary statistics produces a contraction of\\nthe 4-D volume of the posterior (derived from the generalized variance) in 91.5\\n% of our cases, and in 70 to 80 % of the cases for the marginalized 1-D\\nposteriors. The median volume variation is a contraction of a factor of a few\\nfor the 4D posteriors and a contraction of 20 to 30 % in the case of the\\nmarginalized 1D posteriors. This shows that our approach is a possible\\nalternative to looking for sufficient statistics in the theoretical sense.', comment='11 pages, submitted to A&A', journal_ref=None, doi=None, primary_category='astro-ph.CO', categories=['astro-ph.CO'], links=[arxiv.Result.Link('http://arxiv.org/abs/2411.14419v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.14419v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2411.14418v1', updated=datetime.datetime(2024, 11, 21, 18, 52, 2, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 21, 18, 52, 2, tzinfo=datetime.timezone.utc), title='Multimodal 3D Brain Tumor Segmentation with Adversarial Training and Conditional Random Field', authors=[arxiv.Result.Author('Lan Jiang'), arxiv.Result.Author('Yuchao Zheng'), arxiv.Result.Author('Miao Yu'), arxiv.Result.Author('Haiqing Zhang'), arxiv.Result.Author('Fatemah Aladwani'), arxiv.Result.Author('Alessandro Perelli')], summary='Accurate brain tumor segmentation remains a challenging task due to\\nstructural complexity and great individual differences of gliomas. Leveraging\\nthe pre-eminent detail resilience of CRF and spatial feature extraction\\ncapacity of V-net, we propose a multimodal 3D Volume Generative Adversarial\\nNetwork (3D-vGAN) for precise segmentation. The model utilizes Pseudo-3D for\\nV-net improvement, adds conditional random field after generator and use\\noriginal image as supplemental guidance. Results, using the BraTS-2018 dataset,\\nshow that 3D-vGAN outperforms classical segmentation models, including U-net,\\nGan, FCN and 3D V-net, reaching specificity over 99.8%.', comment='13 pages, 7 figures, Annual Conference on Medical Image Understanding\\n  and Analysis (MIUA) 2024', journal_ref='Medical Image Understanding and Analysis (MIUA), Lecture Notes in\\n  Computer Science, Springer, vol. 14859, 2024', doi='10.1007/978-3-031-66955-2_5', primary_category='eess.IV', categories=['eess.IV', 'cs.CV', '15-11', 'I.4.6; I.5.4'], links=[arxiv.Result.Link('http://dx.doi.org/10.1007/978-3-031-66955-2_5', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2411.14418v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.14418v1', title='pdf', rel='related', content_type=None)]), arxiv.Result(entry_id='http://arxiv.org/abs/2411.14412v1', updated=datetime.datetime(2024, 11, 21, 18, 46, 45, tzinfo=datetime.timezone.utc), published=datetime.datetime(2024, 11, 21, 18, 46, 45, tzinfo=datetime.timezone.utc), title='Adversarial Poisoning Attack on Quantum Machine Learning Models', authors=[arxiv.Result.Author('Satwik Kundu'), arxiv.Result.Author('Swaroop Ghosh')], summary=\"With the growing interest in Quantum Machine Learning (QML) and the\\nincreasing availability of quantum computers through cloud providers,\\naddressing the potential security risks associated with QML has become an\\nurgent priority. One key concern in the QML domain is the threat of data\\npoisoning attacks in the current quantum cloud setting. Adversarial access to\\ntraining data could severely compromise the integrity and availability of QML\\nmodels. Classical data poisoning techniques require significant knowledge and\\ntraining to generate poisoned data, and lack noise resilience, making them\\nineffective for QML models in the Noisy Intermediate Scale Quantum (NISQ) era.\\nIn this work, we first propose a simple yet effective technique to measure\\nintra-class encoder state similarity (ESS) by analyzing the outputs of encoding\\ncircuits. Leveraging this approach, we introduce a quantum indiscriminate data\\npoisoning attack, QUID. Through extensive experiments conducted in both\\nnoiseless and noisy environments (e.g., IBM\\\\_Brisbane's noise), across various\\narchitectures and datasets, QUID achieves up to $92\\\\%$ accuracy degradation in\\nmodel performance compared to baseline models and up to $75\\\\%$ accuracy\\ndegradation compared to random label-flipping. We also tested QUID against\\nstate-of-the-art classical defenses, with accuracy degradation still exceeding\\n$50\\\\%$, demonstrating its effectiveness. This work represents the first attempt\\nto reevaluate data poisoning attacks in the context of QML.\", comment=None, journal_ref=None, doi=None, primary_category='quant-ph', categories=['quant-ph', 'cs.CR', 'cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/2411.14412v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2411.14412v1', title='pdf', rel='related', content_type=None)])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(research_topics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = research_assistant._check_relevance(papers, research_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[0, 3, 4, 13, 44, 21]',\n",
       " '[11, 13, 42, 12, 14, 10]',\n",
       " '[21, 3, 0, 44, 20, 22]',\n",
       " '[30, 31, 32, 33, 34, 41]',\n",
       " '[42, 41, 19, 39, 24, 30]']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResearchTopics(topic='Deep learning architectures for semantic segmentation using fully convolutional networks without attention mechanisms', priority=1, query='fcn semantic segmentation', timestamp='2024-11-23 17:10:29', research_papers=None)\n",
      "Image Segmentation Keras : Implementation of Segnet, FCN, UNet, PSPNet and other models in Keras\n",
      "Graph-FCN for image semantic segmentation\n",
      "CNN-based Semantic Segmentation using Level Set Loss\n",
      "Object Boundary Guided Semantic Segmentation\n",
      "Semantic Segmentation with Boundary Neural Fields\n",
      "A Brief Survey and an Application of Semantic Image Segmentation for Autonomous Driving\n"
     ]
    }
   ],
   "source": [
    "print(research_topics[1])\n",
    "for num in json.loads(responses[1]):\n",
    "    print(papers[int(num)].title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "condensed_papers = [papers[i] for i in json.loads(responses[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[arxiv.Result(entry_id='http://arxiv.org/abs/2307.13215v1', updated=datetime.datetime(2023, 7, 25, 2, 56, 20, tzinfo=datetime.timezone.utc), published=datetime.datetime(2023, 7, 25, 2, 56, 20, tzinfo=datetime.timezone.utc), title='Image Segmentation Keras : Implementation of Segnet, FCN, UNet, PSPNet and other models in Keras', authors=[arxiv.Result.Author('Divam Gupta')], summary='Semantic segmentation plays a vital role in computer vision tasks, enabling\\nprecise pixel-level understanding of images. In this paper, we present a\\ncomprehensive library for semantic segmentation, which contains implementations\\nof popular segmentation models like SegNet, FCN, UNet, and PSPNet. We also\\nevaluate and compare these models on several datasets, offering researchers and\\npractitioners a powerful toolset for tackling diverse segmentation challenges.', comment=None, journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/2307.13215v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2307.13215v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2001.00335v1', updated=datetime.datetime(2020, 1, 2, 6, 5, 29, tzinfo=datetime.timezone.utc), published=datetime.datetime(2020, 1, 2, 6, 5, 29, tzinfo=datetime.timezone.utc), title='Graph-FCN for image semantic segmentation', authors=[arxiv.Result.Author('Yi Lu'), arxiv.Result.Author('Yaran Chen'), arxiv.Result.Author('Dongbin Zhao'), arxiv.Result.Author('Jianxin Chen')], summary='Semantic segmentation with deep learning has achieved great progress in\\nclassifying the pixels in the image. However, the local location information is\\nusually ignored in the high-level feature extraction by the deep learning,\\nwhich is important for image semantic segmentation. To avoid this problem, we\\npropose a graph model initialized by a fully convolutional network (FCN) named\\nGraph-FCN for image semantic segmentation. Firstly, the image grid data is\\nextended to graph structure data by a convolutional network, which transforms\\nthe semantic segmentation problem into a graph node classification problem.\\nThen we apply graph convolutional network to solve this graph node\\nclassification problem. As far as we know, it is the first time that we apply\\nthe graph convolutional network in image semantic segmentation. Our method\\nachieves competitive performance in mean intersection over union (mIOU) on the\\nVOC dataset(about 1.34% improvement), compared to the original FCN model.', comment=None, journal_ref='Advances in Neural Networks, ISNN 2019. Lecture Notes in Computer\\n  Science, vol 11554, pp. 97-105, Springer, Cham', doi=None, primary_category='cs.CV', categories=['cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/2001.00335v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2001.00335v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1910.00950v1', updated=datetime.datetime(2019, 10, 2, 13, 45, 33, tzinfo=datetime.timezone.utc), published=datetime.datetime(2019, 10, 2, 13, 45, 33, tzinfo=datetime.timezone.utc), title='CNN-based Semantic Segmentation using Level Set Loss', authors=[arxiv.Result.Author('Youngeun Kim'), arxiv.Result.Author('Seunghyeon Kim'), arxiv.Result.Author('Taekyung Kim'), arxiv.Result.Author('Changick Kim')], summary='Thesedays, Convolutional Neural Networks are widely used in semantic\\nsegmentation. However, since CNN-based segmentation networks produce\\nlow-resolution outputs with rich semantic information, it is inevitable that\\nspatial details (e.g., small bjects and fine boundary information) of\\nsegmentation results will be lost. To address this problem, motivated by a\\nvariational approach to image segmentation (i.e., level set theory), we propose\\na novel loss function called the level set loss which is designed to refine\\nspatial details of segmentation results. To deal with multiple classes in an\\nimage, we first decompose the ground truth into binary images. Note that each\\nbinary image consists of background and regions belonging to a class. Then we\\nconvert level set functions into class probability maps and calculate the\\nenergy for each class. The network is trained to minimize the weighted sum of\\nthe level set loss and the cross-entropy loss. The proposed level set loss\\nimproves the spatial details of segmentation results in a time and memory\\nefficient way. Furthermore, our experimental results show that the proposed\\nloss function achieves better performance than previous approaches.', comment='2019 IEEE Winter Conference on Applications of Computer Vision\\n  (WACV). IEEE, 2019', journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/1910.00950v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1910.00950v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1603.09742v4', updated=datetime.datetime(2016, 7, 6, 23, 51, 40, tzinfo=datetime.timezone.utc), published=datetime.datetime(2016, 3, 31, 19, 51, 5, tzinfo=datetime.timezone.utc), title='Object Boundary Guided Semantic Segmentation', authors=[arxiv.Result.Author('Qin Huang'), arxiv.Result.Author('Chunyang Xia'), arxiv.Result.Author('Wenchao Zheng'), arxiv.Result.Author('Yuhang Song'), arxiv.Result.Author('Hao Xu'), arxiv.Result.Author('C. -C. Jay Kuo')], summary='Semantic segmentation is critical to image content understanding and object\\nlocalization. Recent development in fully-convolutional neural network (FCN)\\nhas enabled accurate pixel-level labeling. One issue in previous works is that\\nthe FCN based method does not exploit the object boundary information to\\ndelineate segmentation details since the object boundary label is ignored in\\nthe network training. To tackle this problem, we introduce a double branch\\nfully convolutional neural network, which separates the learning of the\\ndesirable semantic class labeling with mask-level object proposals guided by\\nrelabeled boundaries. This network, called object boundary guided FCN\\n(OBG-FCN), is able to integrate the distinct properties of object shape and\\nclass features elegantly in a fully convolutional way with a designed masking\\narchitecture. We conduct experiments on the PASCAL VOC segmentation benchmark,\\nand show that the end-to-end trainable OBG-FCN system offers great improvement\\nin optimizing the target semantic segmentation quality.', comment='The results in the first version of this paper are mistaken due to\\n  overlapping validation data and incorrect benchmark methods', journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/1603.09742v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1603.09742v4', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1511.02674v2', updated=datetime.datetime(2016, 5, 24, 23, 32, 10, tzinfo=datetime.timezone.utc), published=datetime.datetime(2015, 11, 9, 13, 27, 30, tzinfo=datetime.timezone.utc), title='Semantic Segmentation with Boundary Neural Fields', authors=[arxiv.Result.Author('Gedas Bertasius'), arxiv.Result.Author('Jianbo Shi'), arxiv.Result.Author('Lorenzo Torresani')], summary='The state-of-the-art in semantic segmentation is currently represented by\\nfully convolutional networks (FCNs). However, FCNs use large receptive fields\\nand many pooling layers, both of which cause blurring and low spatial\\nresolution in the deep layers. As a result FCNs tend to produce segmentations\\nthat are poorly localized around object boundaries. Prior work has attempted to\\naddress this issue in post-processing steps, for example using a color-based\\nCRF on top of the FCN predictions. However, these approaches require additional\\nparameters and low-level features that are difficult to tune and integrate into\\nthe original network architecture. Additionally, most CRFs use color-based\\npixel affinities, which are not well suited for semantic segmentation and lead\\nto spatially disjoint predictions.\\n  To overcome these problems, we introduce a Boundary Neural Field (BNF), which\\nis a global energy model integrating FCN predictions with boundary cues. The\\nboundary information is used to enhance semantic segment coherence and to\\nimprove object localization. Specifically, we first show that the convolutional\\nfilters of semantic FCNs provide good features for boundary detection. We then\\nemploy the predicted boundaries to define pairwise potentials in our energy.\\nFinally, we show that our energy decomposes semantic segmentation into multiple\\nbinary problems, which can be relaxed for efficient global optimization. We\\nreport extensive experiments demonstrating that minimization of our global\\nboundary-based energy yields results superior to prior globalization methods,\\nboth quantitatively as well as qualitatively.', comment=None, journal_ref=None, doi=None, primary_category='cs.CV', categories=['cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/1511.02674v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1511.02674v2', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/1808.08413v1', updated=datetime.datetime(2018, 8, 25, 11, 56, 45, tzinfo=datetime.timezone.utc), published=datetime.datetime(2018, 8, 25, 11, 56, 45, tzinfo=datetime.timezone.utc), title='A Brief Survey and an Application of Semantic Image Segmentation for Autonomous Driving', authors=[arxiv.Result.Author('√áaƒürƒ± Kaymak'), arxiv.Result.Author('Ay≈üeg√ºl U√ßar')], summary='Deep learning is a fast-growing machine learning approach to perceive and\\nunderstand large amounts of data. In this paper, general information about the\\ndeep learning approach which is attracted much attention in the field of\\nmachine learning is given in recent years and an application about semantic\\nimage segmentation is carried out in order to help autonomous driving of\\nautonomous vehicles. This application is implemented with Fully Convolutional\\nNetwork (FCN) architectures obtained by modifying the Convolutional Neural\\nNetwork (CNN) architectures based on deep learning. Experimental studies for\\nthe application are utilized 4 different FCN architectures named\\nFCN-AlexNet,FCN-8s, FCN-16s and FCN-32s. For the experimental studies, FCNs are\\nfirst trained separately and validation accuracies of these trained network\\nmodels on the used dataset is compared. In addition, image segmentation\\ninferences are visualized to take account of how precisely FCN architectures\\ncan segment objects.', comment='A chapter for Springer Book: Handbook of Deep Learning Applications,\\n  2018,[ Pijush Samui, Editor]. (be published)', journal_ref=None, doi=None, primary_category='eess.IV', categories=['eess.IV'], links=[arxiv.Result.Link('http://arxiv.org/abs/1808.08413v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/1808.08413v1', title='pdf', rel='related', content_type=None)])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condensed_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "futures",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
